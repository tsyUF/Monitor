{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "73r42amgm5ufi4hykymy",
   "authorId": "33929106622",
   "authorName": "TSY@UFL.EDU",
   "authorEmail": "tsy@ufl.edu",
   "sessionId": "99efbc03-7c90-4ee6-a2b8-d57b22d8934b",
   "lastEditTime": 1751998588864
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "96f19f42-7d6f-4449-ba45-8c01bef4e691",
   "metadata": {
    "language": "python",
    "name": "dependency_checker"
   },
   "outputs": [],
   "source": "# Cell: dependency_checker\n\"\"\"\nDependencies: None (root cell)\nProvides: Dependency checking functionality\n\"\"\"\nimport logging\nfrom typing import Dict, Set, Any\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass DependencyChecker:\n    def __init__(self):\n        self.loaded_cells = set()\n        self.load_times = {}\n        self.dependency_map = {\n            'dependency_checker': set(),  # No dependencies\n            'notebook_monitor': {'dependency_checker'},\n            'imports': {'dependency_checker', 'notebook_monitor'},\n            'global_constants': {'imports'},\n            'environment_config': {'global_constants', 'imports'},\n            'snowflake_utils': {'environment_config', 'global_constants', 'imports'},\n            'model_scoring': {'snowflake_utils', 'global_constants'},\n            'score_validation': {'model_scoring', 'global_constants'},\n            'scoring_report': {'score_validation', 'global_constants'}\n        }\n\n    def register_cell(self, cell_name: str):\n        \"\"\"Register a cell as loaded\"\"\"\n        self.loaded_cells.add(cell_name)\n        self.load_times[cell_name] = datetime.now()\n        logger.info(f\"Registered cell: {cell_name}\")\n        print(f\"Registered cell: {cell_name}\")\n\n    def check_dependencies(self, cell_name: str) -> bool:\n        \"\"\"Check if all dependencies for a cell are loaded\"\"\"\n        if cell_name not in self.dependency_map:\n            logger.warning(f\"Unknown cell: {cell_name}\")\n            return False\n        \n        missing = self.dependency_map[cell_name] - self.loaded_cells\n        if missing:\n            logger.error(f\"Missing dependencies for {cell_name}: {missing}\")\n            return False\n        return True\n\n    def show_dependency_tree(self):\n        \"\"\"Display the dependency tree in a readable format\"\"\"\n        print(\"\\nDependency Tree:\")\n        print(\"=\" * 50)\n        for cell in self.dependency_map:\n            deps = self.dependency_map[cell]\n            status = \"✓\" if self.check_dependencies(cell) else \"✗\"\n            loaded = \"✓\" if cell in self.loaded_cells else \"✗\"\n            print(f\"{status} {cell} [{loaded}]\")\n            if deps:\n                for dep in deps:\n                    dep_loaded = \"✓\" if dep in self.loaded_cells else \"✗\"\n                    print(f\"  ├── {dep_loaded} {dep}\")\n            print(\"  │\")\n\n# Initialize the dependency checker\ndep_checker = DependencyChecker()\ndep_checker.register_cell('dependency_checker')\n\n# Show initial status\nlogger.info(\"Dependency checker initialized\")\ndep_checker.show_dependency_tree()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78c81bad-9ff7-4b71-ac0d-d52f62092857",
   "metadata": {
    "language": "python",
    "name": "notebook_monitor"
   },
   "outputs": [],
   "source": "# Cell: notebook_monitor\n\"\"\"\nDependencies: dependency_checker\nProvides: Enhanced monitoring with memory usage and operation counts\n\"\"\"\nimport time\nimport pandas as pd\nimport psutil\nimport os\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Any\n\nclass ScoringMonitor:\n    def __init__(self):\n        if not dep_checker.check_dependencies('notebook_monitor'):\n            print(\"Warning: Dependencies not met for notebook monitor\")\n        \n        self.execution_logs = []\n        self.current_cell = None\n        self.start_time = None\n        self.end_time = None\n        self.operation_counts = {}\n        self.scoring_metrics = {}\n        self.context = []\n        pd.set_option('display.max_rows', None)\n        pd.set_option('display.max_columns', None)\n\n    def add_context(self, context: str) -> None:\n        \"\"\"Add context to the current monitoring\"\"\"\n        self.context.append(context)\n        \n    def remove_context(self) -> None:\n        \"\"\"Remove the most recent context\"\"\"\n        if self.context:\n            self.context.pop()\n    \n    def _get_memory_usage(self):\n        \"\"\"Get current memory usage in MB\"\"\"\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss / 1024 / 1024\n    \n    def start(self, operation_name: str) -> None:\n        \"\"\"Start monitoring an operation\"\"\"\n        if self.context:\n            operation_name = f\"{operation_name}_{'_'.join(self.context)}\"\n        \n        self.current_cell = {\n            'operation': operation_name,\n            'start_time': datetime.now(),\n            'status': 'running',\n            'duration_seconds': 0,\n            'start_memory_mb': self._get_memory_usage(),\n            'records_processed': 0,\n            'error': None\n        }\n        self.start_time = time.time()\n    \n    def update_metrics(self, metrics: Dict[str, Any]) -> None:\n        \"\"\"Update scoring metrics\"\"\"\n        if self.current_cell:\n            self.current_cell.update(metrics)\n    \n    def end(self) -> None:\n        \"\"\"End monitoring current operation\"\"\"\n        if self.current_cell:\n            self.end_time = time.time()\n            duration = self.end_time - self.start_time\n            end_memory = self._get_memory_usage()\n            \n            self.current_cell.update({\n                'end_time': datetime.now(),\n                'status': 'completed',\n                'duration_seconds': round(duration, 2),\n                'end_memory_mb': end_memory,\n                'memory_change_mb': round(end_memory - self.current_cell['start_memory_mb'], 2)\n            })\n            \n            self.execution_logs.append(self.current_cell.copy())\n            self.current_cell = None\n    \n    def show_summary(self) -> pd.DataFrame:\n        \"\"\"Show execution summary\"\"\"\n        if not self.execution_logs:\n            return pd.DataFrame()\n        \n        try:\n            # Create DataFrame from logs\n            df = pd.DataFrame(self.execution_logs)\n            \n            # Format timestamps\n            for col in ['start_time', 'end_time']:\n                if col in df.columns:\n                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d %H:%M:%S')\n            \n            # Add scoring specific metrics\n            total_duration = df['duration_seconds'].sum()\n            total_records = df['records_processed'].sum()\n            \n            print(\"\\nScoring Summary:\")\n            print(f\"Total Duration: {total_duration:.2f} seconds\")\n            print(f\"Total Records Processed: {total_records:,}\")\n            if total_duration > 0:\n                print(f\"Average Processing Rate: {total_records/total_duration:.2f} records/second\")\n            \n            return df\n        \n        except Exception as e:\n            print(f\"Error generating summary: {type(e).__name__} - {str(e)}\")\n            print(traceback.format_exc())\n            return pd.DataFrame()\n    \n    def log_error(self, error: Exception) -> None:\n        \"\"\"Log an error for the current operation\"\"\"\n        if self.current_cell:\n            self.current_cell.update({\n                'status': 'error',\n                'error': f\"{type(error).__name__}: {str(error)}\",\n                'end_memory_mb': self._get_memory_usage()\n            })\n            self.end()\n\n# Initialize monitor\nmonitor = ScoringMonitor()\ndep_checker.register_cell('notebook_monitor')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8708664-ee4f-4e07-ae1e-3a971252beb6",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "# Cell: imports\nmonitor.start('imports')\ntry:\n    \"\"\"\n    Dependencies: dependency_checker, notebook_monitor\n    Provides: All required package imports for the scoring notebook\n    \"\"\"\n    # Standard libraries\n    import logging\n    from typing import Dict, Set, Any, List, Optional, Tuple\n    from datetime import datetime\n    import time\n    import traceback\n    import json\n    import gc\n    import pickle\n    import base64\n    import copy\n    import joblib\n    import io\n\n    \n    # Snowpark\n    from snowflake.snowpark.functions import col, when\n    import snowflake.snowpark.functions as F\n    from snowflake.snowpark import Session\n\n    def get_active_session() -> Session:\n        return Session.get_active_session()\n    \n    # Models\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.svm import SVC\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import StandardScaler\n\n    # Scientific computing\n    import pandas as pd\n    import numpy as np\n\n    # Visualization\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # Configure logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('imports')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ce84ad4-3680-4718-8ec9-630fdb82a207",
   "metadata": {
    "language": "python",
    "name": "global_constants"
   },
   "outputs": [],
   "source": "# Cell: global_constants\nmonitor.start('global_constants')\ntry:\n    \"\"\"\n    Dependencies: imports\n    Provides: Global constants for the scoring notebook\n    \"\"\"\n    # Department and Model Configuration\n    DEPARTMENT = 'UF'\n    MODEL_TYPE = 'MAJOR_GIFT'\n    \n    # Database configuration\n    SCHEMA = \"UFF_MODEL\"\n    SOURCE_TABLE = \"UF_MAJOR_GIFT_VIEWS_COMBO\"\n    \n    # Scoring parameters\n    CHUNK_SIZE = 150000   # Size of processing chunks\n    LIMIT = 1500000\n    \n    # Model Selection Weights\n    MODEL_SELECTION_WEIGHTS = {\n        'PRIMARY': {\n            'F1_MEAN': 0.30,\n            'ROC_AUC_MEAN': 0.30,\n            'MCC_MEAN': 0.20,\n            'BALANCED_ACCURACY_MEAN': 0.20\n        },\n        'SECONDARY': {\n            'PERFORMANCE': 0.40,\n            'PRECISION_RECALL_BALANCE': 0.30,\n            'STABILITY': 0.30\n        }\n    }\n\n    OUTPUT_COLUMNS = {\n        'required': [\n            'ID',\n            'AFFINITY_SCORE',\n            'AFFINITY_GRADE',\n            'PROBABILITY',\n            'SCORED_DATE'\n        ],\n        'optional': [\n            'MODEL_NAME',\n            'MODEL_VERSION',\n            'FEATURE_COUNT',\n            'CONFIDENCE'\n        ]\n    }\n    \n    VALIDATION_THRESHOLDS = {\n        'min_score_count': 1000,\n        'max_null_percentage': 0.01,\n        'grade_distribution_bounds': {\n            'A': (0.001, 0.01),\n            'B': (0.01, 0.05),\n            'C': (0.05, 0.15),\n            'D': (0.15, 0.30),\n            'E': (0.40, 0.80)\n        }\n    }\n\n    PROBABILITY_THRESHOLDS = {\n        'A': 0.995,\n        'B': 0.980,\n        'C': 0.950,\n        'D': 0.850,\n        'E': 0.000\n    }\n    \n    GRADE_THRESHOLDS = {\n        'A': 99.5,\n        'B': 98.0,\n        'C': 95.0,\n        'D': 85.0,\n        'E': 0.0\n    }\n\n    # Grade configuration\n    GRADING_METHOD = 'PERCENTILE'  # or PERCENTILE or FIXED\n    \n    # Percentile-based grading configuration\n    PERCENTILE_GRADE_CONFIG = {\n        'A': 99,    # Top 1% (99th percentile and above)\n        'B': 95,    # 95-99th percentile  \n        'C': 90,    # 90-95th percentile\n        'D': 85,    # 80-90th percentile # Changed from 80 to 85\n        'E': 0      # Bottom 80%\n    }\n    \n    \n    FIXED_GRADE_RANGES = [\n        ('A', 99.50, float('inf')),\n        ('B', 98.00, 99.49),\n        ('C', 95.00, 97.99),\n        ('D', 85.00, 94.99),\n        ('E', float('-inf'), 85.00)\n    ]\n        \n    def get_table_name(base_name: str, timestamp: Optional[str] = None) -> str:\n        \"\"\"Generate standardized table names\"\"\"\n        name = f\"{DEPARTMENT}_{MODEL_TYPE}_{base_name}\"\n        if timestamp:\n            name = f\"{name}_{timestamp}\"\n        return name\n\n    # Define standard table names\n    TABLE_NAMES = {\n        'SOURCE': SOURCE_TABLE,\n        'MODEL_RESULTS': get_table_name('MODEL_PERFORMANCE'),\n        'MODELS': get_table_name('MODELS'),\n        'FEATURES': get_table_name('FEATURE_IMPORTANCE'),\n        'SCORES': get_table_name('AFFINITY_SCORES'),\n        'METADATA': get_table_name('SCORING_METADATA')\n    }\n\n    TABLE_SCHEMAS = {\n        'SCORES': \"\"\"(\n            ID VARCHAR,\n            PROBABILITY FLOAT,\n            AFFINITY_SCORE FLOAT,\n            AFFINITY_GRADE VARCHAR,\n            SCORED_DATE TIMESTAMP_NTZ,\n            CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n        )\"\"\",\n        'METADATA': \"\"\"(\n            SCORING_RUN_ID VARCHAR,\n            MODEL_NAME VARCHAR,\n            FEATURE_SAMPLE VARCHAR,\n            N_FEATURES NUMBER,\n            F1_SCORE FLOAT,\n            ROC_AUC FLOAT,\n            RECORDS_SCORED NUMBER,\n            SCORING_START TIMESTAMP_NTZ,\n            WHERE_CLAUSE VARCHAR,\n            GRADE_DISTRIBUTION VARIANT,\n            FEATURES_USED VARIANT\n        )\"\"\"\n    }\n\n    print(\"\\nConfiguration Settings:\")\n    print(\"=\" * 50)\n    \n    print(\"\\nTable References:\")\n    for key, value in TABLE_NAMES.items():\n        print(f\"{key}: {value}\")\n        \n    print(\"\\nModel Selection Weights:\")\n    print(\"\\nPrimary Weights:\")\n    for metric, weight in MODEL_SELECTION_WEIGHTS['PRIMARY'].items():\n        print(f\"{metric}: {weight:.2f}\")\n    print(\"\\nSecondary Weights:\")\n    for aspect, weight in MODEL_SELECTION_WEIGHTS['SECONDARY'].items():\n        print(f\"{aspect}: {weight:.2f}\")\n        \n    print(\"\\nAffinity Grade Thresholds:\")\n    for grade, threshold in GRADE_THRESHOLDS.items():\n        print(f\"Grade {grade}: >= {threshold}\")\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('global_constants')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f141557-95a0-4679-9e43-55e679f476f8",
   "metadata": {
    "language": "python",
    "name": "environment_config"
   },
   "outputs": [],
   "source": "# Cell: environment_config\nmonitor.start('environment_config')\ntry:\n    class EnvironmentConfig:\n        # Static configurations that don't change between environments\n        SCHEMA = SCHEMA\n        TABLES = {\n            'SOURCE': TABLE_NAMES['SOURCE'],\n            'MODEL_RESULTS': TABLE_NAMES['MODEL_RESULTS'],\n            'MODELS': TABLE_NAMES['MODELS'],\n            'FEATURES': TABLE_NAMES['FEATURES'],\n            'SCORES': TABLE_NAMES['SCORES'],\n            'METADATA': TABLE_NAMES['METADATA']\n        }\n        \n        # Environment configurations\n        ENVIRONMENTS = {\n            'test': {\n                'database': 'PRE_PRODUCTION',\n                'schema': SCHEMA,\n                'warehouse': 'ANALYSIS'\n            },\n            'prod': {\n                'database': 'PRODUCTION',\n                'schema': SCHEMA,\n                'warehouse': 'ANALYSIS'\n            }\n        }\n        \n        def __init__(self):\n            self.current_env = 'test'  # default to test environment\n            self.session = None\n            \n            # Validate table configurations\n            required_tables = {'SOURCE', 'MODEL_RESULTS', 'MODELS', 'FEATURES', 'SCORES', 'METADATA'}\n            missing_tables = required_tables - set(self.TABLES.keys())\n            if missing_tables:\n                raise ValueError(f\"Missing required table configurations: {missing_tables}\")\n\n        def set_session(self, session):\n            \"\"\"Set the Snowflake session\"\"\"\n            self.session = session\n            self._apply_environment()\n        \n        def _apply_environment(self):\n            \"\"\"Apply environment settings to Snowflake session\"\"\"\n            if not self.session:\n                raise ValueError(\"Session not initialized\")\n                \n            env = self.ENVIRONMENTS[self.current_env]\n            try:\n                self.session.sql(f\"USE DATABASE {env['database']}\").collect()\n                self.session.sql(f\"USE SCHEMA {env['schema']}\").collect()\n                self.session.sql(f\"USE WAREHOUSE {env['warehouse']}\").collect()\n            except Exception as e:\n                print(f\"Error setting environment: {str(e)}\")\n                raise\n\n        def get_schema_db_name(self) -> str:\n            \"\"\"Get database.schema\"\"\"\n            env = self.ENVIRONMENTS[self.current_env]\n            return f\"{env['database']}.{env['schema']}\"\n            \n        def show_environment_status(self):\n            \"\"\"Display current environment settings\"\"\"\n            if not self.session:\n                raise ValueError(\"Session not initialized\")\n                \n            border = \"=\" * 60\n            print(border)\n            print(f\"{'ENVIRONMENT STATUS':^60}\")\n            print(border)\n            print(f\"ENVIRONMENT: {self.current_env.upper()}\")\n            \n            try:\n                current_settings = self.session.sql(\"\"\"\n                SELECT \n                    CURRENT_DATABASE() as database,\n                    CURRENT_SCHEMA() as schema,\n                    CURRENT_WAREHOUSE() as warehouse\n                \"\"\").collect()\n                \n                print(f\"\"\"\n                DATABASE:  {current_settings[0]['DATABASE']}\n                SCHEMA:    {current_settings[0]['SCHEMA']}\n                WAREHOUSE: {current_settings[0]['WAREHOUSE']}\n                \"\"\")\n                print(border)\n            except Exception as e:\n                print(f\"Error getting current settings: {str(e)}\")\n                raise\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('environment_config')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bf4bd49-01f3-44b6-b25a-ac370f445b41",
   "metadata": {
    "language": "python",
    "name": "snowflake_utils"
   },
   "outputs": [],
   "source": "# Cell: snowflake_utils\nmonitor.start('snowflake_utils')\ntry:\n    class ScoringDatabaseManager:\n        def __init__(self, config: EnvironmentConfig):\n            self.config = config\n            self.session = config.session\n            self.max_retries = 3\n            self.retry_delay = 5  # seconds\n\n        def execute_with_retry(self, operation_name: str, operation, *args, **kwargs):\n            \"\"\"Execute operation with retry logic\"\"\"\n            monitor.start(f'execute_{operation_name}')\n            try:\n                for attempt in range(self.max_retries):\n                    try:\n                        result = operation(*args, **kwargs)\n                        return result\n                    except Exception as e:\n                        if attempt < self.max_retries - 1:\n                            print(f\"Error in {operation_name}, retrying... (Attempt {attempt + 1}/{self.max_retries})\")\n                            time.sleep(self.retry_delay * (attempt + 1))\n                            self.session = get_active_session()\n                            self.config.set_session(self.session)\n                        else:\n                            raise\n            finally:\n                monitor.end()\n\n        def load_data_for_scoring(self, features: List[str],\n                                 where_clause: Optional[str] = None,\n                                 limit: Optional[int] = None) -> pd.DataFrame:\n            \"\"\"Load and preprocess data for scoring, applying same encoding as training\"\"\"\n            monitor.start('load_scoring_data')\n            try:\n                # First, load a sample to identify data types (same as training)\n                sample_query = f\"\"\"\n                SELECT *\n                FROM {self.config.get_schema_db_name()}.{TABLE_NAMES['SOURCE']}\n                LIMIT 1000\n                \"\"\"\n                sample_df = self.execute_with_retry('load_sample', \n                                                  lambda: self.session.sql(sample_query).to_pandas())\n                \n                # Use SAME logic as training to identify categorical columns\n                categorical_columns = sample_df.select_dtypes(include=['object']).columns.tolist()\n                print(f\"Identified categorical columns by dtype: {categorical_columns}\")\n                \n                # Now identify which of our features come from these categorical columns\n                original_features = set(['DONOR_ID'])\n                categorical_prefixes = set()\n                \n                for feature in features:\n                    # Check if this feature could be from a categorical column\n                    feature_is_categorical = False\n                    for cat_col in categorical_columns:\n                        if feature.startswith(cat_col + '_'):\n                            categorical_prefixes.add(cat_col)\n                            feature_is_categorical = True\n                            break\n                    \n                    if not feature_is_categorical:\n                        original_features.add(feature)\n                \n                print(f\"Categorical prefixes needed: {categorical_prefixes}\")\n                print(f\"Original features needed: {original_features}\")\n                \n                # Load original columns (categorical + numeric + DONOR_ID)\n                columns_to_load = list(original_features | categorical_prefixes)\n                feature_list = ', '.join([f'\"{f}\"' for f in columns_to_load])\n                \n                query = f\"\"\"\n                SELECT {feature_list}\n                FROM {self.config.get_schema_db_name()}.{TABLE_NAMES['SOURCE']}\n                \"\"\"\n                if where_clause:\n                    query += f\" WHERE {where_clause}\"\n                if limit:\n                    query += f\" LIMIT {limit}\"\n                    \n                print(f\"Loading original data columns: {columns_to_load}\")\n                df = self.execute_with_retry('load_data', lambda: self.session.sql(query).to_pandas())\n                \n                # Apply same preprocessing as training\n                processed_df = df[['DONOR_ID']].copy()  # Start with DONOR_ID\n                \n                # Add original numeric features\n                for feature in original_features:\n                    if feature != 'DONOR_ID' and feature in df.columns:\n                        processed_df[feature] = df[feature]\n                \n                # Apply one-hot encoding for categorical features\n                for cat_col in categorical_prefixes:\n                    if cat_col in df.columns:\n                        print(f\"Processing categorical column: {cat_col}\")\n                        # Apply same cleaning as training\n                        cleaned_series = df[cat_col].astype(str).apply(\n                            lambda x: x.replace(' ', '').replace(\"'\", \"\")\n                        ).replace(\"-\", \"\")\n                        \n                        # Apply one-hot encoding with same parameters as training\n                        dummies = pd.get_dummies(\n                            cleaned_series,\n                            prefix=cat_col,\n                            dummy_na=False,\n                            drop_first=False\n                        )\n                        \n                        # Apply same column name standardization as training\n                        dummies.columns = [\n                            col_name.upper().replace(' ', '_').replace('-', '_').replace('.', '_')\n                            for col_name in dummies.columns\n                        ]\n                        \n                        # Only keep columns that existed during training\n                        expected_columns = [f for f in features if f.startswith(cat_col + '_')]\n                        training_columns = set(expected_columns)\n                        scoring_columns = set(dummies.columns)\n                        \n                        # Keep only columns that were in training\n                        valid_columns = list(training_columns & scoring_columns)\n                        if valid_columns:\n                            dummies = dummies[valid_columns]\n                            print(f\"Kept {len(valid_columns)} training columns for {cat_col}\")\n                        \n                        # Log unseen categories for monitoring\n                        unseen_columns = scoring_columns - training_columns\n                        if unseen_columns:\n                            print(f\"WARNING: Found unseen categories for {cat_col}: {unseen_columns}\")\n                            print(\"These will be ignored in scoring\")\n                        \n                        # Add to processed dataframe\n                        processed_df = pd.concat([processed_df, dummies], axis=1)\n                \n                # Ensure we have all required features, add missing ones as zeros\n                missing_features = set(features) - set(processed_df.columns)\n                if missing_features:\n                    print(f\"Adding missing features as zeros: {missing_features}\")\n                    for feature in missing_features:\n                        processed_df[feature] = 0.0\n                \n                # Reorder columns to match expected feature order\n                final_columns = ['DONOR_ID'] + [f for f in features if f != 'DONOR_ID']\n                processed_df = processed_df[final_columns]\n                \n                print(f\"Final processed data shape: {processed_df.shape}\")\n                print(f\"Final columns: {processed_df.columns.tolist()}\")\n                \n                return processed_df\n                \n            finally:\n                monitor.end()        \t\t\n  \n\n        def save_scores(self, df: pd.DataFrame, timestamp: str = None) -> None:\n            \"\"\"Save scoring results\"\"\"\n            monitor.start('save_scores')\n            try:\n                if 'SCORES' not in TABLE_NAMES:\n                    raise ValueError(\"SCORES table name not found in TABLE_NAMES\")\n                \n                if 'required' not in OUTPUT_COLUMNS:\n                    raise ValueError(\"Required columns not defined in OUTPUT_COLUMNS\")\n                \n                missing_cols = set(OUTPUT_COLUMNS['required']) - set(df.columns)\n                if missing_cols:\n                    raise ValueError(f\"Missing required columns: {missing_cols}\")\n                \n                table_name = TABLE_NAMES['SCORES']\n                full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                \n                snowpark_df = self.session.create_dataframe(df)\n                snowpark_df.write.save_as_table(full_name, mode='overwrite')\n                \n                print(f\"Overwritten scores to: {full_name}\")\n                \n            finally:\n                monitor.end()\n\n        def save_scoring_metadata(self, metadata: Dict[str, Any]) -> None:\n            \"\"\"Save metadata about scoring run\"\"\"\n            monitor.start('save_metadata')\n            try:\n                metadata_df = pd.DataFrame([metadata])\n                table_name = TABLE_NAMES['METADATA']\n                full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                \n                snowpark_df = self.session.create_dataframe(metadata_df)\n                snowpark_df.write.save_as_table(full_name, mode='append')\n                \n                print(f\"Saved metadata to: {full_name}\")\n                \n            finally:\n                monitor.end()\n\n        def ensure_scoring_tables(self) -> None:\n            \"\"\"Ensure scoring-specific tables exist\"\"\"\n            schemas = {\n                'SCORES': f\"\"\"\n                    CREATE TABLE IF NOT EXISTS {TABLE_NAMES['SCORES']} (\n                        ID VARCHAR,\n                        PROBABILITY FLOAT,\n                        AFFINITY_SCORE FLOAT,\n                        AFFINITY_GRADE VARCHAR,\n                        SCORED_DATE TIMESTAMP_NTZ\n                    )\n                \"\"\",\n                'METADATA': f\"\"\"\n                    CREATE TABLE IF NOT EXISTS {TABLE_NAMES['METADATA']} (\n                        SCORING_RUN_ID VARCHAR,\n                        MODEL_NAME VARCHAR,\n                        FEATURE_SAMPLE VARCHAR,\n                        N_FEATURES NUMBER,\n                        F1_SCORE FLOAT,\n                        ROC_AUC FLOAT,\n                        RECORDS_SCORED NUMBER,\n                        SCORING_START TIMESTAMP_NTZ,\n                        WHERE_CLAUSE VARCHAR,\n                        GRADE_DISTRIBUTION VARIANT,\n                        FEATURES_USED VARIANT\n                    )\n                \"\"\"\n            }\n            \n            for table_type, create_sql in schemas.items():\n                table_name = TABLE_NAMES[table_type]\n                full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                \n                # Check if table exists\n                exists_query = f\"\"\"\n                SELECT 1 \n                FROM INFORMATION_SCHEMA.TABLES \n                WHERE TABLE_SCHEMA = '{self.config.SCHEMA}'\n                AND TABLE_NAME = '{table_name}'\n                \"\"\"\n        \n                result = self.execute_with_retry(\n                    'check_table_exists',\n                    lambda: self.session.sql(exists_query).collect()\n                )\n                \n                if not result:\n                    print(f\"Creating table: {table_name}\")\n                    self.execute_with_retry(\n                        'create_table',\n                        lambda: self.session.sql(create_sql).collect()\n                    )\n\n\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('snowflake_utils')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "model_scoring"
   },
   "source": "# Cell: model_scoring\nmonitor.start('model_scoring')\ntry:\n    class AffinityScorer:\n        def __init__(self, config: EnvironmentConfig):\n            self.config = config\n            self.db_manager = ScoringDatabaseManager(config)\n            self.session = config.session  # Ensure session is assigned\n            self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            self.model = None\n            self.features = None\n            self.scaler = None\n            self.imputers = None\n            self.model_info = None\n            # Ensure required tables exist\n            self.db_manager.ensure_scoring_tables()\n\n        def deserialize_object(self, serialized_str: str) -> Any:\n            \"\"\"Deserialize object using joblib\"\"\"\n            import joblib\n            import io\n            buffer = io.BytesIO(base64.b64decode(serialized_str))\n            return joblib.load(buffer)\n         \n        def get_best_model_info(self, target: str = 'COMMIT_MAJOR') -> Dict[str, Any]:\n            print(\"\\nDEBUG: Looking for best model...\")\n            full_table_name = f\"{self.config.get_schema_db_name()}.{TABLE_NAMES['MODEL_RESULTS']}\"\n            print(f\"Checking table: {full_table_name}\")\n            print(\"\\nDEBUG: Table Information:\")\n            print(f\"Full qualified table name: {self.config.get_schema_db_name()}.{TABLE_NAMES['MODEL_RESULTS']}\")\n            print(\"\\nDEBUG: Checking Snowflake columns\")\n\n            columns_query = f\"\"\"\n            SELECT *\n            FROM {self.config.get_schema_db_name()}.{TABLE_NAMES['MODEL_RESULTS']}\n            LIMIT 1\n            \"\"\"\n            try:\n                columns_result = self.db_manager.execute_with_retry(\n                    'check_columns',\n                    lambda: self.session.sql(columns_query).collect()\n                )\n                if columns_result:\n                    print(\"Available columns:\", list(columns_result[0].asDict().keys()))\n                else:\n                    print(\"No data found in table\")\n            except Exception as e:\n                print(f\"Error checking columns: {str(e)}\")\n\n            debug_query = f\"\"\"\n            WITH MODEL_METRICS AS (\n                SELECT\n                    MODEL,\n                    FEATURE_SAMPLE,\n                    N_FEATURES,\n                    F1_MEAN,\n                    ROC_AUC_MEAN,\n                    ACCURACY_MEAN,\n                    PRECISION_MEAN,\n                    RECALL_MEAN,\n                    (F1_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['F1_MEAN']} +\n                     ROC_AUC_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['ROC_AUC_MEAN']} +\n                     ACCURACY_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['BALANCED_ACCURACY_MEAN']})\n                    AS PRIMARY_SCORE,\n                    ROW_NUMBER() OVER (\n                        PARTITION BY FEATURE_SAMPLE\n                        ORDER BY F1_MEAN DESC\n                    ) as RANK_IN_GROUP\n                FROM {full_table_name}\n                WHERE TARGET = '{target}'\n            )\n            SELECT *\n            FROM MODEL_METRICS\n            ORDER BY FEATURE_SAMPLE, RANK_IN_GROUP;\n            \"\"\"\n            print(\"\\nDEBUG: All Models and Rankings\")\n            print(\"=\" * 80)\n            debug_result = self.db_manager.execute_with_retry(\n                'debug_models',\n                lambda: self.session.sql(debug_query).to_pandas()\n            )\n            if not debug_result.empty:\n                for feature_sample in debug_result['FEATURE_SAMPLE'].unique():\n                    sample_data = debug_result[debug_result['FEATURE_SAMPLE'] == feature_sample]\n                    print(f\"\\nFeature Sample: {feature_sample}\")\n                    print(\"-\" * 40)\n                    for _, row in sample_data.iterrows():\n                        print(f\"Model: {row['MODEL']}\")\n                        print(f\"  Rank: {row['RANK_IN_GROUP']}\")\n                        print(f\"  F1: {row['F1_MEAN']:.4f}\")\n                        print(f\"  ROC-AUC: {row['ROC_AUC_MEAN']:.4f}\")\n                        print(f\"  Primary Score: {row['PRIMARY_SCORE']:.4f}\")\n            else:\n                print(\"No models found in debug query\")\n\n            query = f\"\"\"\n            WITH MODEL_METRICS AS (\n                SELECT\n                    MODEL,\n                    FEATURE_SAMPLE,\n                    N_FEATURES,\n                    F1_MEAN,\n                    ROC_AUC_MEAN,\n                    ACCURACY_MEAN,\n                    PRECISION_MEAN,\n                    RECALL_MEAN,\n                    (F1_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['F1_MEAN']} +\n                     ROC_AUC_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['ROC_AUC_MEAN']} +\n                     ACCURACY_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['BALANCED_ACCURACY_MEAN']})\n                    AS PRIMARY_SCORE,\n                    ROW_NUMBER() OVER (\n                        PARTITION BY FEATURE_SAMPLE\n                        ORDER BY F1_MEAN DESC\n                    ) as rank_in_group\n                FROM {full_table_name}\n                WHERE TARGET = '{target}'\n            )\n            SELECT\n                MODEL,\n                FEATURE_SAMPLE,\n                N_FEATURES,\n                F1_MEAN,\n                ROC_AUC_MEAN,\n                ACCURACY_MEAN as BALANCED_ACCURACY_MEAN,\n                PRECISION_MEAN,\n                RECALL_MEAN,\n                PRIMARY_SCORE\n            FROM MODEL_METRICS\n            WHERE rank_in_group = 1\n            ORDER BY PRIMARY_SCORE DESC\n            LIMIT 1;\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'get_best_model',\n                lambda: self.session.sql(query).to_pandas()\n            )\n            if result.empty:\n                raise ValueError(\"No model results found\")\n            self.model_info = result.iloc[0].to_dict()\n            print(\"\\nDEBUG: Selected Best Model\")\n            print(\"=\" * 80)\n            print(f\"Model Name: {self.model_info['MODEL']}\")\n            print(f\"Feature Sample: {self.model_info['FEATURE_SAMPLE']}\")\n            print(f\"Number of Features: {self.model_info['N_FEATURES']}\")\n            print(\"\\nPerformance Metrics:\")\n            print(f\"F1 Score: {self.model_info['F1_MEAN']:.4f}\")\n            print(f\"ROC-AUC: {self.model_info['ROC_AUC_MEAN']:.4f}\")\n            print(f\"Balanced Accuracy: {self.model_info['BALANCED_ACCURACY_MEAN']:.4f}\")\n            return self.model_info\n\n        def load_model_components(self) -> None:\n            \"\"\"Load model and its components\"\"\"\n            query = f\"\"\"\n            SELECT MODEL_OBJECT, SELECTED_FEATURES, METRICS, SCALER, IMPUTERS\n            FROM {self.config.get_schema_db_name()}.{TABLE_NAMES['MODELS']}\n            WHERE MODEL = '{self.model_info['MODEL']}'\n            AND FEATURE_SAMPLE = '{self.model_info['FEATURE_SAMPLE']}'\n            ORDER BY CREATED_AT DESC\n            LIMIT 1;\n            \"\"\"\n            print(\"\\nLoading best model components...\")\n            result = self.db_manager.execute_with_retry(\n                'load_model',\n                lambda: self.session.sql(query).collect()\n            )\n            if not result:\n                raise ValueError(\"Could not load model components\")\n        \n            row = result[0]\n        \n            # Load base components\n#            self.model = pickle.loads(base64.b64decode(row['MODEL_OBJECT']))\n#            saved_features = pickle.loads(base64.b64decode(row['SELECTED_FEATURES']))\n#            original_scaler = pickle.loads(base64.b64decode(row['SCALER']))\n#            original_imputers = pickle.loads(base64.b64decode(row['IMPUTERS']))\n            # NEW:\n            self.model = self.deserialize_object(row['MODEL_OBJECT'])  \n            saved_features = self.deserialize_object(row['SELECTED_FEATURES'])\n            original_scaler = self.deserialize_object(row['SCALER'])\n            original_imputers = self.deserialize_object(row['IMPUTERS'])            \n        \n            print(f\"Loaded model type: {type(self.model).__name__}\")\n            print(f\"Saved features from database: {saved_features}\")\n            \n            # Determine the correct feature list to use\n            model_features = None\n            if hasattr(self.model, 'feature_names_in_') and self.model.feature_names_in_ is not None:\n                model_features = self.model.feature_names_in_.tolist()\n                print(f\"Model's actual features: {model_features}\")\n            \n            # Use model features if available, otherwise use saved features\n            if model_features is not None:\n                self.features = model_features\n                print(f\"Using model's feature list ({len(self.features)} features)\")\n                \n                # Check for discrepancies with saved features\n                if saved_features is not None:\n                    saved_set = set(saved_features)\n                    model_set = set(self.features)\n                    if saved_set != model_set:\n                        print(f\"WARNING: Feature mismatch detected!\")\n                        print(f\"Features in saved list but not in model: {saved_set - model_set}\")\n                        print(f\"Features in model but not in saved list: {model_set - saved_set}\")\n                else:\n                    print(\"WARNING: Saved features from database are None\")\n                    \n            elif saved_features is not None:\n                self.features = saved_features\n                print(f\"Using saved features from database ({len(self.features)} features)\")\n            else:\n                raise ValueError(\"Both model features and saved features are None - cannot proceed\")\n            \n            print(f\"Final feature list: {self.features}\")\n        \n            # Create dummy data with correct features\n            if self.features is None or len(self.features) == 0:\n                raise ValueError(\"No features available for model\")\n                \n            dummy_data = pd.DataFrame(columns=self.features)\n            if dummy_data.empty:\n                print(\"Dummy data is empty. Initializing with zeros.\")\n                dummy_data = pd.DataFrame(np.zeros((2, len(self.features))), columns=self.features)\n            print(f\"Initialized dummy data with {len(dummy_data.columns)} columns\")\n        \n            if isinstance(self.model, (RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, SVC)):\n                print(\"Handling specifics for RandomForest, LogisticRegression, DecisionTree, and SVC.\")\n                if hasattr(original_scaler, 'feature_names_in_') and original_scaler.feature_names_in_ is not None:\n                    # Only check features that are actually in our model\n                    scaler_features = set(original_scaler.feature_names_in_)\n                    model_features_set = set(self.features)\n                    common_features = model_features_set & scaler_features\n                    print(f\"Common features between model and scaler: {len(common_features)} out of {len(self.features)}\")\n                    \n                    if len(common_features) != len(self.features):\n                        print(\"WARNING: Not all model features are in the scaler\")\n                        \n                print(\"Scaler feature names consistency check passed.\")\n                print(\"\\nProcessing scaler:\")\n                if hasattr(original_scaler, 'mean_') and original_scaler.mean_ is not None:\n                    for i, feature in enumerate(self.features):\n                        if hasattr(original_scaler, 'feature_names_in_') and original_scaler.feature_names_in_ is not None:\n                            if feature in original_scaler.feature_names_in_:\n                                idx = list(original_scaler.feature_names_in_).index(feature)\n                                dummy_data.iloc[:, i] = original_scaler.mean_[idx]\n                                dummy_data.iloc[1, i] += original_scaler.scale_[idx]\n                            else:\n                                print(f\"WARNING: Feature {feature} not found in original scaler, using zero\")\n                                dummy_data.iloc[:, i] = 0.0\n                        else:\n                            print(f\"WARNING: Original scaler has no feature names, using zero for {feature}\")\n                            dummy_data.iloc[:, i] = 0.0\n                else:\n                    print(\"No mean or scale attributes found in the scaler.\")\n                    \n                self.scaler = StandardScaler()\n                self.scaler.fit(dummy_data)\n                print(\"Scaler fitted with dummy data.\")\n        \n            elif isinstance(self.model, MLPClassifier):\n                print(\"Handling specifics for MLPClassifier.\")\n                # For MLPClassifier, we don't need to check dummy_data columns since we're using NoOpScaler\n                class NoOpScaler:\n                    def fit(self, X): return self\n                    def transform(self, X): return X\n                self.scaler = NoOpScaler()\n                print(\"Set NoOpScaler for MLPClassifier.\")\n            else:\n                raise ValueError(f\"Unsupported model type: {type(self.model).__name__}\")\n        \n            # Process imputers\n            self.imputers = {}\n            if original_imputers is not None:\n                for strategy, imputer in original_imputers.items():\n                    if hasattr(imputer, 'feature_names_in_') and imputer.feature_names_in_ is not None:\n                        # Only use features that are both in the imputer AND in our model\n                        valid_features = [f for f in imputer.feature_names_in_ if f in self.features]\n                        if valid_features:\n                            print(f\"Features to keep for imputation ({strategy}): {valid_features}\")\n                            new_imputer = SimpleImputer(strategy=imputer.strategy)\n                            indices = [list(imputer.feature_names_in_).index(f) for f in valid_features]\n                            feature_values = imputer.statistics_[indices]\n                            dummy_data_impute = pd.DataFrame(np.zeros((2, len(valid_features))), columns=valid_features)\n                            dummy_data_impute.iloc[0] = feature_values\n                            new_imputer.fit(dummy_data_impute)\n                            self.imputers[strategy] = new_imputer\n                            print(f\"{strategy} imputer fitted with valid features.\")\n                        else:\n                            print(f\"WARNING: No valid features found for {strategy} imputer\")\n                    else:\n                        print(f\"WARNING: {strategy} imputer has no feature names\")\n            else:\n                print(\"WARNING: No imputers found in saved model\")\n                \n            print(\"Model components loaded successfully.\")\n            print(f\"Final feature count: {len(self.features)}\")\n            print(f\"Final imputer count: {len(self.imputers)}\")\n            \n\n        def calculate_affinity_score(self, probabilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n            \"\"\"Convert probabilities to scores and grades\"\"\"\n            print(\"\\nProbability Analysis:\")\n            print(f\"Min probability: {probabilities.min():.3f}\")\n            print(f\"Max probability: {probabilities.max():.3f}\")\n            print(f\"Mean probability: {probabilities.mean():.3f}\")\n            \n            probabilities = np.round(probabilities, 3)\n            scores = np.round(probabilities * 100, 1)\n            grades = np.full(len(scores), 'E', dtype='object')\n            \n            if GRADING_METHOD == 'PERCENTILE':\n                print(f\"\\nUsing Percentile-Based Grading: {PERCENTILE_GRADE_CONFIG}\")\n                \n                # Calculate percentile thresholds\n                percentiles = {\n                    grade: np.percentile(scores, pct) \n                    for grade, pct in PERCENTILE_GRADE_CONFIG.items() \n                    if grade != 'E'\n                }\n                \n                print(f\"Calculated thresholds: {percentiles}\")\n\n                # **ADD DEBUG: Show actual threshold values**\n                print(f\"Debug - Score thresholds:\")\n                print(f\"A threshold (99th percentile): {percentiles['A']:.3f}\")\n                print(f\"B threshold (95th percentile): {percentiles['B']:.3f}\")\n                print(f\"C threshold (90th percentile): {percentiles['C']:.3f}\")\n                print(f\"D threshold (80th percentile): {percentiles['D']:.3f}\")\n\n                \n                # Assign grades (order matters - start with highest)\n                grades[scores >= percentiles['A']] = 'A'\n                grades[(scores >= percentiles['B']) & (scores < percentiles['A'])] = 'B'\n                grades[(scores >= percentiles['C']) & (scores < percentiles['B'])] = 'C'\n                grades[(scores > percentiles['D']) & (scores < percentiles['C'])] = 'D'\n                # E is default\n\n                # **ADD: Print final distribution (was missing!)**\n                print(\"\\nGrade Distribution:\")\n                for grade in ['A', 'B', 'C', 'D', 'E']:\n                    count = np.sum(grades == grade)\n                    print(f\"Grade {grade}: {count} records ({count/len(grades):.1%})\")\n\n            \n            else:  # FIXED method\n                print(f\"\\nUsing Fixed Thresholds: {FIXED_GRADE_RANGES}\")\n                for grade, lower_bound, upper_bound in FIXED_GRADE_RANGES:\n\n                    if grade == 'E':\n                        mask = (scores < upper_bound)\n                    elif grade == 'A':\n                        mask = (scores >= lower_bound)\n                    else:\n                        mask = (scores >= lower_bound) & (scores < upper_bound)\n                    grades[mask] = grade\n                    count = np.sum(mask)\n                    print(f\"\\nGrade {grade} ({lower_bound:.3f} to {upper_bound:.3f}):\")\n                    print(f\"Found {count} records ({count/len(grades):.2%})\")\n                    if count > 0:\n                        sample_idx = np.where(mask)[0][:5]\n                        print(\"Sample records:\")\n                        for idx in sample_idx:\n                            print(f\"  Score: {scores[idx]:.3f}, Probability: {probabilities[idx]:.3f}\")\n                print(\"\\nValidation Check:\")\n                for grade, lower_bound, upper_bound in FIXED_GRADE_RANGES:\n                    if grade == 'E':\n                        expected_mask = (scores < upper_bound)\n                    elif grade == 'A':\n                        expected_mask = (scores >= lower_bound)\n                    else:\n                        expected_mask = (scores >= lower_bound) & (scores < upper_bound)\n                    actual_mask = (grades == grade)\n                    mismatches = np.sum(expected_mask != actual_mask)\n                    if mismatches > 0:\n                        print(f\"WARNING: Found {mismatches} mismatches for grade {grade}\")\n                        mismatch_idx = np.where(expected_mask != actual_mask)[0][:5]\n                        print(\"Sample mismatches:\")\n                        for idx in mismatch_idx:\n                            print(f\"  Score: {scores[idx]:.1f}, Expected: {grade}, Got: {grades[idx]}\")\n                    else:\n                        print(f\"✓ Grade {grade}: No mismatches\")\n                print(\"\\nValidation - Probability to Score Conversion:\")\n                for i in range(5):\n                    idx = np.random.randint(0, len(probabilities))\n                    expected = probabilities[idx] * 100\n                    actual = scores[idx]\n                    print(f\"Probability: {probabilities[idx]:.3f} → Expected: {expected:.3f} → Actual Score: {actual:.1f}\")\n                \n                        \n                # Print final distribution\n                print(\"\\nGrade Distribution:\")\n                for grade in ['A', 'B', 'C', 'D', 'E']:\n                    count = np.sum(grades == grade)\n                    print(f\"Grade {grade}: {count} records ({count/len(grades):.1%})\")\n            \n            return scores, grades\n        \n            \n        def calculate_affinity_score_OG(self, probabilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n            \"\"\"Convert probabilities to scores and grades\"\"\"\n            print(\"\\nProbability Analysis:\")\n            print(f\"Min probability: {probabilities.min():.3f}\")\n            print(f\"Max probability: {probabilities.max():.3f}\")\n            print(f\"Mean probability: {probabilities.mean():.3f}\")\n\n            probabilities = np.round(probabilities, 3)\n            scores = np.round(probabilities * 100, 1)\n            grades = np.full(len(scores), 'E', dtype='object')\n            grade_ranges = GRADE_RANGES\n\n            print(\"\\nGrade Assignment Process:\")\n            for grade, lower_bound, upper_bound in grade_ranges:\n                if grade == 'E':\n                    mask = (scores < upper_bound)\n                elif grade == 'A':\n                    mask = (scores >= lower_bound)\n                else:\n                    mask = (scores >= lower_bound) & (scores < upper_bound)\n                grades[mask] = grade\n                count = np.sum(mask)\n                print(f\"\\nGrade {grade} ({lower_bound:.3f} to {upper_bound:.3f}):\")\n                print(f\"Found {count} records ({count/len(grades):.2%})\")\n                if count > 0:\n                    sample_idx = np.where(mask)[0][:5]\n                    print(\"Sample records:\")\n                    for idx in sample_idx:\n                        print(f\"  Score: {scores[idx]:.3f}, Probability: {probabilities[idx]:.3f}\")\n\n            print(\"\\nValidation Check:\")\n            for grade, lower_bound, upper_bound in grade_ranges:\n                if grade == 'E':\n                    expected_mask = (scores < upper_bound)\n                elif grade == 'A':\n                    expected_mask = (scores >= lower_bound)\n                else:\n                    expected_mask = (scores >= lower_bound) & (scores < upper_bound)\n                actual_mask = (grades == grade)\n                mismatches = np.sum(expected_mask != actual_mask)\n                if mismatches > 0:\n                    print(f\"WARNING: Found {mismatches} mismatches for grade {grade}\")\n                    mismatch_idx = np.where(expected_mask != actual_mask)[0][:5]\n                    print(\"Sample mismatches:\")\n                    for idx in mismatch_idx:\n                        print(f\"  Score: {scores[idx]:.1f}, Expected: {grade}, Got: {grades[idx]}\")\n                else:\n                    print(f\"✓ Grade {grade}: No mismatches\")\n\n            print(\"\\nValidation - Probability to Score Conversion:\")\n            for i in range(5):\n                idx = np.random.randint(0, len(probabilities))\n                expected = probabilities[idx] * 100\n                actual = scores[idx]\n                print(f\"Probability: {probabilities[idx]:.3f} → Expected: {expected:.3f} → Actual Score: {actual:.1f}\")\n\n            return scores, grades\n\n        def prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:\n            \"\"\"Prepare data for scoring using saved feature order\"\"\"\n            monitor.start('prepare_data')\n            try:\n                if 'DONOR_ID' not in df.columns:\n                    raise ValueError(\"DONOR_ID not found in input data\")\n                donor_ids = df['DONOR_ID'].copy()\n                print(f\"Preserved {len(donor_ids)} DONOR_IDs\")\n\n                feature_df = df[self.features].copy()\n                print(f\"\\nInitial feature columns: {feature_df.columns.tolist()}\")\n\n                print(\"\\nPre-imputation validation:\")\n                null_counts = feature_df.isnull().sum()\n                if null_counts.any():\n                    print(\"Columns with nulls:\")\n                    print(null_counts[null_counts > 0])\n\n                for strategy, imputer in self.imputers.items():\n                    print(f\"\\nProcessing {strategy} imputation:\")\n                    features_to_impute = imputer.feature_names_in_\n                    print(f\"Imputer features: {features_to_impute.tolist()}\")\n                    if len(features_to_impute) > 0:\n                        impute_df = feature_df[features_to_impute]\n                        print(f\"Data shape for imputation: {impute_df.shape}\")\n                        if impute_df.shape[1] != len(features_to_impute):\n                            raise ValueError(f\"Feature count mismatch: got {impute_df.shape[1]}, expected {len(features_to_impute)}\")\n                        feature_df[features_to_impute] = imputer.transform(impute_df)\n\n                post_null_counts = feature_df.isnull().sum()\n                if post_null_counts.any():\n                    print(\"\\nWARNING: Nulls remain after imputation:\")\n                    print(post_null_counts[post_null_counts > 0])\n\n                if self.scaler is not None:\n                    print(f\"\\nApplying scaler to features: {self.features}\")\n                    feature_df = pd.DataFrame(\n                        self.scaler.transform(feature_df),\n                        columns=self.features,\n                        index=feature_df.index\n                    )\n                    print(\"\\nApplied scaler to data.\")\n\n                feature_df['DONOR_ID'] = donor_ids\n                print(\"\\nPrepared feature columns for prediction:\", feature_df.columns.tolist())\n                return feature_df\n            finally:\n                monitor.end()\n\n        def score_dataset(self, where_clause: Optional[str] = None,\n                          save_results: bool = True,\n                          limit: Optional[int] = None) -> pd.DataFrame:\n            \"\"\"Score the dataset\"\"\"\n            monitor.start('score_dataset')\n            try:\n                print(\"Loading model components...\")\n                self.get_best_model_info()\n                self.load_model_components()\n\n                print(\"Loading data...\")\n                df = self.db_manager.load_data_for_scoring(\n                    features=self.features, \n                    where_clause=where_clause,\n                    limit=limit\n                )\n                print(f\"Total records to process: {len(df)}\")\n\n                try:\n                    feature_df = self.prepare_data(df)\n                    donor_ids = feature_df['DONOR_ID']\n                    prediction_features = feature_df.drop('DONOR_ID', axis=1)\n\n                    print(\"\\nFeature Statistics Before Prediction:\")\n                    print(prediction_features.describe())\n                    print(\"\\nFeature columns for prediction:\", prediction_features.columns.tolist())\n\n                    print(\"\\nModel Information:\")\n                    print(f\"Model type: {type(self.model).__name__}\")\n                    if hasattr(self.model, 'feature_names_in_'):\n                        print(f\"Model expected features: {self.model.feature_names_in_.tolist()}\")\n\n                    missing_features = set(self.model.feature_names_in_).difference(prediction_features.columns)\n                    if missing_features:\n                        print(f\"Missing features at prediction time: {missing_features}\")\n                    else:\n                        print(\"All expected features present at prediction time.\")\n\n                    probabilities = self.model.predict_proba(prediction_features)\n                    print(\"\\nRaw Prediction Output:\")\n                    print(f\"Shape: {probabilities.shape}\")\n                    print(f\"Class probabilities min: {probabilities.min():.3f}\")\n                    print(f\"Class probabilities max: {probabilities.max():.3f}\")\n\n                    probabilities = np.round(probabilities[:, 1], 3)\n                    print(\"\\nVerification of probability rounding:\")\n                    print(f\"Sample of raw vs rounded probabilities:\")\n                    sample_indices = np.random.choice(len(probabilities), min(5, len(probabilities)), replace=False)\n                    for idx in sample_indices:\n                        orig = probabilities[idx]\n                        score = orig * 100\n                        print(f\"Probability: {orig:.3f} -> Score: {score:.1f}\")\n\n                    print(\"\\nUnique probability values:\")\n                    unique_probs = np.unique(probabilities)\n                    print(\"First few:\", unique_probs[:10])\n                    print(\"Last few:\", unique_probs[-10:])\n\n                    scores, grades = self.calculate_affinity_score(probabilities)\n\n                    results_df = pd.DataFrame({\n                        'ID': donor_ids,\n                        'PROBABILITY': np.round(probabilities, 3),\n                        'AFFINITY_SCORE': np.round(scores, 1),\n                        'AFFINITY_GRADE': grades,\n                        'SCORED_DATE': datetime.now()\n                    })\n\n                    print(f\"\\nFinal Results Shape: {results_df.shape}\")\n                    print(\"\\nGrade Distribution:\")\n                    print(results_df['AFFINITY_GRADE'].value_counts())\n\n                    if save_results:\n                        print(f\"\\nSaving results...\")\n                        self.db_manager.save_scores(results_df)\n\n                        metadata = {\n                            'SCORING_RUN_ID': self.timestamp,\n                            'MODEL': self.model_info['MODEL'],\n                            'FEATURE_SAMPLE': self.model_info['FEATURE_SAMPLE'],\n                            'N_FEATURES': self.model_info['N_FEATURES'],\n                            'F1_SCORE': self.model_info['F1_MEAN'],\n                            'ROC_AUC': self.model_info['ROC_AUC_MEAN'],\n                            'RECORDS_SCORED': len(results_df),\n                            'SCORING_START': datetime.now(),\n                            'WHERE_CLAUSE': where_clause,\n                            'GRADE_DISTRIBUTION': results_df['AFFINITY_GRADE'].value_counts().to_dict(),\n                            'FEATURES_USED': self.features\n                        }\n                        self.db_manager.save_scoring_metadata(metadata)\n                        print(\"Results and metadata saved successfully\")\n                    else:\n                        print(\"\\nResults not saved (save_results=False)\")\n\n                    return results_df\n\n                except Exception as e:\n                    print(f\"Error during scoring: {str(e)}\")\n                    raise\n\n            finally:\n                monitor.end()\n\n    # Test scoring\n    if __name__ == \"__main__\":\n        try:\n            config = EnvironmentConfig()\n            session = get_active_session()\n            config.set_session(session)\n\n            scorer = AffinityScorer(config)\n            exclusion_where_clause = \"\"\"\n            DONOR_ID NOT IN (\n                SELECT DISTINCT C.\"ucinn_ascendv2__Donor_ID__c\"\n                FROM PRE_PRODUCTION.ASCEND.\"Contact\" C\n                WHERE C.\"IsDeleted\" = FALSE\n                AND (\n                    C.\"uff_Is_Lump_Sum_Donor__c\" = TRUE\n                    OR C.\"ucinn_ascendv2__First_and_Last_Name_Formula__c\" LIKE ANY ('%Cash Donations%', '%Anonymous Donor%', 'DONOR%')\n                    OR C.\"ucinn_ascendv2__Contact_Type__c\" LIKE ANY ('%Estate Rep%', '%External Contact%', 'Student')\n                    OR C.\"ucinn_ascendv2__Primary_Contact_Type__c\" = 'Student'\n                    OR C.\"ucinn_ascendv2__Is_Deceased__c\" = TRUE\n                    OR C.\"uff_UF_Disqualified__c\" = TRUE\n                )\n            )\n            \"\"\"\n            # Score with exclusion criteria  \n            test_where = \"1=1\"\n           # change where clause to random if you want a sample\n            # \"RANDOM() < 0.01\"  # 1% sample for testing\n            results = scorer.score_dataset(\n                where_clause=test_where,\n                save_results=True, # Set to False to test without saving\n#                limit=LIMIT # 750000  # Explicitly set limit\n            )\n            # Show distribution\n            print(\"\\nScore Distribution:\")\n            print(results['AFFINITY_GRADE'].value_counts(normalize=True))\n        except Exception as e:\n            print(f\"Error in scoring: {str(e)}\")\n            traceback.print_exc()\nfinally:\n    monitor.end()\ndep_checker.register_cell('model_scoring')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "896fec52-c5ef-42f2-b1f7-8a37389fe5dd",
   "metadata": {
    "language": "python",
    "name": "scoring_report"
   },
   "outputs": [],
   "source": " # Cell: scoring_report\nmonitor.start('scoring_report')\ntry:\n    class ScoringReporter:\n        def __init__(self, scoring_run_id: str, config: EnvironmentConfig):\n            self.scoring_run_id = scoring_run_id\n            self.db_manager = ScoringDatabaseManager(config)\n            self.metadata = None\n            self.load_metadata()\n\n        def load_metadata(self) -> None:\n            \"\"\"Load metadata for scoring run\"\"\"\n            query = f\"\"\"\n            SELECT *\n            FROM {TABLE_NAMES['METADATA']}\n            WHERE SCORING_RUN_ID = '{self.scoring_run_id}'\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'load_metadata',\n                lambda: self.db_manager.session.sql(query).collect()\n            ).to_pandas()\n            \n            if result.empty:\n                raise ValueError(f\"No metadata found for run {self.scoring_run_id}\")\n            \n            self.metadata = result.iloc[0].to_dict()\n\n        def generate_distribution_analysis(self) -> pd.DataFrame:\n            \"\"\"Analyze score and grade distribution\"\"\"\n            query = f\"\"\"\n            SELECT \n                AFFINITY_GRADE,\n                COUNT(*) as COUNT,\n                COUNT(*) / SUM(COUNT(*)) OVER () as PERCENTAGE,\n                MIN(AFFINITY_SCORE) as MIN_SCORE,\n                MAX(AFFINITY_SCORE) as MAX_SCORE,\n                AVG(AFFINITY_SCORE) as AVG_SCORE,\n                PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY AFFINITY_SCORE) as Q1,\n                PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY AFFINITY_SCORE) as MEDIAN,\n                PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY AFFINITY_SCORE) as Q3\n            FROM {TABLE_NAMES['SCORES']}_{self.scoring_run_id}\n            GROUP BY AFFINITY_GRADE\n            ORDER BY AFFINITY_GRADE\n            \"\"\"\n            return self.db_manager.execute_with_retry(\n                'distribution_analysis',\n                lambda: self.db_manager.session.sql(query).collect()\n            ).to_pandas()\n\n        def generate_feature_importance_summary(self) -> pd.DataFrame:\n            \"\"\"Summarize feature importance for the model used\"\"\"\n            query = f\"\"\"\n            SELECT \n                FEATURE_NAME,\n                IMPORTANCE,\n                RANK() OVER (ORDER BY IMPORTANCE DESC) as IMPORTANCE_RANK\n            FROM {TABLE_NAMES['FEATURES']}\n            WHERE MODEL_NAME = '{self.metadata['MODEL_NAME']}'\n            AND FEATURE_SAMPLE = '{self.metadata['FEATURE_SAMPLE']}'\n            ORDER BY IMPORTANCE DESC\n            LIMIT 20\n            \"\"\"\n            return self.db_manager.execute_with_retry(\n                'feature_importance',\n                lambda: self.db_manager.session.sql(query).collect()\n            ).to_pandas()\n\n        def plot_distribution(self, dist_data: pd.DataFrame) -> None:\n            \"\"\"Plot score distribution\"\"\"\n            plt.figure(figsize=(15, 10))\n            \n            # Create subplots\n            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n            \n            # Plot 1: Grade Distribution\n            sns.barplot(\n                data=dist_data,\n                x='AFFINITY_GRADE',\n                y='PERCENTAGE',\n                ax=ax1\n            )\n            ax1.set_title('Grade Distribution')\n            ax1.set_ylabel('Percentage')\n            \n            # Add percentage labels\n            for i, v in enumerate(dist_data['PERCENTAGE']):\n                ax1.text(i, v, f'{v:.1%}', ha='center', va='bottom')\n            \n            # Plot 2: Box Plot of Scores by Grade\n            sns.boxplot(\n                data=dist_data,\n                x='AFFINITY_GRADE',\n                y='AVG_SCORE',\n                ax=ax2\n            )\n            ax2.set_title('Score Distribution by Grade')\n            ax2.set_ylabel('Score')\n            \n            plt.tight_layout()\n            plt.show()\n\n        def generate_report(self) -> None:\n            \"\"\"Generate comprehensive scoring report\"\"\"\n            monitor.start('generate_report')\n            try:\n                print(\"\\nAffinity Score Report\")\n                print(\"=\" * 50)\n                \n                # Metadata\n                print(\"\\nScoring Run Information:\")\n                print(f\"Run ID: {self.scoring_run_id}\")\n                print(f\"Model: {self.metadata['MODEL_NAME']}\")\n                print(f\"Feature Sample: {self.metadata['FEATURE_SAMPLE']}\")\n                print(f\"Features Used: {self.metadata['N_FEATURES']}\")\n                print(f\"Records Scored: {self.metadata['RECORDS_SCORED']:,}\")\n                print(f\"Model Performance (F1): {self.metadata['F1_SCORE']:.4f}\")\n                print(f\"Model Performance (ROC-AUC): {self.metadata['ROC_AUC']:.4f}\")\n                \n                # Distribution Analysis\n                print(\"\\nScore Distribution Analysis:\")\n                dist_data = self.generate_distribution_analysis()\n                print(\"\\nGrade Distribution:\")\n                print(\"-\" * 80)\n                print(dist_data[['AFFINITY_GRADE', 'COUNT', 'PERCENTAGE', \n                               'MIN_SCORE', 'MAX_SCORE', 'AVG_SCORE']].to_string(index=False))\n                \n                # Plot distributions\n                self.plot_distribution(dist_data)\n                \n                # Feature Importance\n                print(\"\\nTop Feature Importance:\")\n                print(\"-\" * 80)\n                feat_imp = self.generate_feature_importance_summary()\n                print(feat_imp[['FEATURE_NAME', 'IMPORTANCE', \n                              'IMPORTANCE_RANK']].to_string(index=False))\n                \n                # Plot feature importance\n                plt.figure(figsize=(12, 6))\n                sns.barplot(\n                    data=feat_imp.head(10),\n                    x='IMPORTANCE',\n                    y='FEATURE_NAME'\n                )\n                plt.title('Top 10 Feature Importance')\n                plt.tight_layout()\n                plt.show()\n                \n            finally:\n                monitor.end()\n\n    # Test reporting\n#    if __name__ == \"__main__\":\n# \n#        try:\n#            config = EnvironmentConfig()\n#            session = get_active_session()\n#            config.set_session(session)\n#            \n#            # Initialize scorer\n#            scorer = AffinityScorer(config)\n#            \n#            # Score a test sample\n#            test_where = \"RANDOM() < 0.01\"  # 1% sample for testing\n#            results = scorer.score_dataset(\n#                where_clause=test_where,\n#                save_results=False,  # Set to False to test without saving\n#                limit=150000  # Add limit for testing\n#            )\n#            \n#            # Show distribution\n#            print(\"\\nScore Distribution:\")\n#            print(results['AFFINITY_GRADE'].value_counts(normalize=True))\n#            \n#        except Exception as e:\n#            print(f\"Error in scoring: {str(e)}\")\n#            traceback.print_exc()\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('scoring_report')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "663b1021-a90a-41d1-8078-408306425352",
   "metadata": {
    "language": "python",
    "name": "score_validation"
   },
   "outputs": [],
   "source": " # Cell: score_validation\nmonitor.start('score_validation')\ntry:\n    class ScoreValidator:\n        def __init__(self, scoring_run_id: str, config: EnvironmentConfig):\n            self.scoring_run_id = scoring_run_id\n            self.db_manager = ScoringDatabaseManager(config)\n            self.validation_results = {}\n            \n            # Ensure tables exist\n            self.db_manager.ensure_scoring_tables()\n\n        def validate_score_counts(self) -> bool:\n            \"\"\"Validate minimum number of scores generated\"\"\"\n            query = f\"\"\"\n            SELECT COUNT(*) as SCORE_COUNT\n            FROM {TABLE_NAMES['SCORES']}\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'count_scores',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            score_count = result['SCORE_COUNT'].iloc[0]\n            is_valid = score_count >= VALIDATION_THRESHOLDS['min_score_count']\n            \n            self.validation_results['score_count'] = {\n                'status': 'PASS' if is_valid else 'FAIL',\n                'actual': score_count,\n                'threshold': VALIDATION_THRESHOLDS['min_score_count']\n            }\n            \n            return is_valid\n\n        def validate_null_values(self) -> bool:\n            \"\"\"Validate percentage of null values\"\"\"\n            query = f\"\"\"\n            SELECT \n                COUNT(*) as TOTAL_RECORDS,\n                COUNT(*) - COUNT(AFFINITY_SCORE) as NULL_SCORES,\n                COUNT(*) - COUNT(AFFINITY_GRADE) as NULL_GRADES\n            FROM {TABLE_NAMES['SCORES']}\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'check_nulls',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            row = result.iloc[0]\n            null_pct = max(\n                row['NULL_SCORES'] / row['TOTAL_RECORDS'],\n                row['NULL_GRADES'] / row['TOTAL_RECORDS']\n            )\n            \n            is_valid = null_pct <= VALIDATION_THRESHOLDS['max_null_percentage']\n            \n            self.validation_results['null_values'] = {\n                'status': 'PASS' if is_valid else 'FAIL',\n                'actual': null_pct,\n                'threshold': VALIDATION_THRESHOLDS['max_null_percentage']\n            }\n            \n            return is_valid\n\n        def validate_grade_distribution(self) -> bool:\n            \"\"\"Validate grade distribution is within expected bounds\"\"\"\n            query = f\"\"\"\n            SELECT \n                AFFINITY_GRADE,\n                COUNT(*) / SUM(COUNT(*)) OVER () as grade_pct\n            FROM {TABLE_NAMES['SCORES']}\n            GROUP BY AFFINITY_GRADE\n            ORDER BY AFFINITY_GRADE\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'grade_distribution',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            grade_distribution = dict(zip(result['AFFINITY_GRADE'], result['GRADE_PCT']))\n            all_valid = True\n            \n            self.validation_results['grade_distribution'] = {}\n            \n            for grade, (min_pct, max_pct) in VALIDATION_THRESHOLDS['grade_distribution_bounds'].items():\n                actual_pct = grade_distribution.get(grade, 0)\n                is_valid = min_pct <= actual_pct <= max_pct\n                \n                self.validation_results['grade_distribution'][grade] = {\n                    'status': 'PASS' if is_valid else 'FAIL',\n                    'actual': actual_pct,\n                    'bounds': (min_pct, max_pct)\n                }\n                \n                all_valid &= is_valid\n            \n            return all_valid\n\n        def validate_metadata(self) -> bool:\n            \"\"\"Validate metadata was properly saved\"\"\"\n            query = f\"\"\"\n            SELECT *\n            FROM {TABLE_NAMES['METADATA']}\n            WHERE SCORING_RUN_ID = '{self.scoring_run_id}'\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'check_metadata',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            is_valid = not result.empty\n            \n            self.validation_results['metadata'] = {\n                'status': 'PASS' if is_valid else 'FAIL',\n                'found': not result.empty\n            }\n            \n            return is_valid\n\n        def run_all_validations(self) -> bool:\n            \"\"\"Run all validations and return overall status\"\"\"\n            monitor.start('run_validations')\n            try:\n                validations = [\n                    ('Score Counts', self.validate_score_counts()),\n                    ('Null Values', self.validate_null_values()),\n#                    ('Grade Distribution', self.validate_grade_distribution()),\n                    ('Metadata', self.validate_metadata())\n                ]\n                \n                all_passed = all(result for _, result in validations)\n                \n                # Print validation results\n                print(\"\\nValidation Results:\")\n                print(\"=\" * 50)\n                \n                for name, result in validations:\n                    status = \"✓\" if result else \"✗\"\n                    print(f\"{status} {name}\")\n                \n                if 'score_count' in self.validation_results:\n                    print(f\"\\nScore Count: {self.validation_results['score_count']['actual']:,} \"\n                          f\"(min: {self.validation_results['score_count']['threshold']:,})\")\n                \n                if 'null_values' in self.validation_results:\n                    print(f\"Null Percentage: {self.validation_results['null_values']['actual']:.2%} \"\n                          f\"(max: {self.validation_results['null_values']['threshold']:.2%})\")\n                \n                return all_passed\n            \n            finally:\n                monitor.end()\n\n    # Test validation\n    if __name__ == \"__main__\":\n        try:\n            config = EnvironmentConfig()\n            session = get_active_session()\n            config.set_session(session)\n            \n            # Get latest scoring run\n            query = f\"\"\"\n            SELECT SCORING_RUN_ID\n            FROM {TABLE_NAMES['METADATA']}\n            ORDER BY SCORING_START DESC\n            LIMIT 1\n            \"\"\"\n            \n            result = ScoringDatabaseManager(config).execute_with_retry(\n                'get_latest_run',\n                lambda: session.sql(query).to_pandas())\n            \n            if not result.empty:\n                scoring_run_id = result['SCORING_RUN_ID'].iloc[0]\n                validator = ScoreValidator(scoring_run_id, config)\n                all_passed = validator.run_all_validations()\n                \n                print(f\"\\nOverall Validation Status: {'PASS' if all_passed else 'FAIL'}\")\n            else:\n                print(\"No scoring runs found to validate\")\n            \n        except Exception as e:\n            print(f\"Error in validation: {str(e)}\")\n            traceback.print_exc()\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('score_validation')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01d8b1d4-a1c3-4e31-90ec-433eabdbe38c",
   "metadata": {
    "name": "empty",
    "collapsed": false
   },
   "source": ""
  }
 ]
}
