{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "73r42amgm5ufi4hykymy",
   "authorId": "33929106622",
   "authorName": "TSY@UFL.EDU",
   "authorEmail": "tsy@ufl.edu",
   "sessionId": "7bdf8b81-35d9-4a2c-84b8-6158fdfc51ce",
   "lastEditTime": 1751307745976
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "96f19f42-7d6f-4449-ba45-8c01bef4e691",
   "metadata": {
    "language": "python",
    "name": "dependency_checker"
   },
   "outputs": [],
   "source": "# Cell: dependency_checker\n\"\"\"\nDependencies: None (root cell)\nProvides: Dependency checking functionality\n\"\"\"\nimport logging\nfrom typing import Dict, Set, Any\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass DependencyChecker:\n    def __init__(self):\n        self.loaded_cells = set()\n        self.load_times = {}\n        self.dependency_map = {\n           'dependency_checker': set(),  # No dependencies\n            'notebook_monitor': {'dependency_checker'},\n            'imports': {'dependency_checker', 'notebook_monitor'},\n            'global_constants': {'imports'},\n            'environment_config': {'global_constants', 'imports'},\n            'snowflake_utils': {'environment_config', 'global_constants', 'imports'},\n            'model_scoring': {'snowflake_utils', 'global_constants'},\n            'score_validation': {'model_scoring', 'global_constants'},\n            'scoring_report': {'score_validation', 'global_constants'}\n        }\n   \n    def register_cell(self, cell_name: str):\n        \"\"\"Register a cell as loaded\"\"\"\n        self.loaded_cells.add(cell_name)\n        self.load_times[cell_name] = datetime.now()\n        logger.info(f\"Registered cell: {cell_name}\")\n        print(f\"Registered cell: {cell_name}\")\n\n    def check_dependencies(self, cell_name: str) -> bool:\n        \"\"\"Check if all dependencies for a cell are loaded\"\"\"\n        if cell_name not in self.dependency_map:\n            logger.warning(f\"Unknown cell: {cell_name}\")\n            return False\n            \n        missing = self.dependency_map[cell_name] - self.loaded_cells\n        if missing:\n            logger.error(f\"Missing dependencies for {cell_name}: {missing}\")\n            return False\n        return True\n\n    def show_dependency_tree(self):\n        \"\"\"Display the dependency tree in a readable format\"\"\"\n        print(\"\\nDependency Tree:\")\n        print(\"=\" * 50)\n        for cell in self.dependency_map:\n            deps = self.dependency_map[cell]\n            status = \"✓\" if self.check_dependencies(cell) else \"✗\"\n            loaded = \"✓\" if cell in self.loaded_cells else \"✗\"\n            print(f\"{status} {cell} [{loaded}]\")\n            if deps:\n                for dep in deps:\n                    dep_loaded = \"✓\" if dep in self.loaded_cells else \"✗\"\n                    print(f\"  ├── {dep_loaded} {dep}\")\n            print(\"  │\")\n\n# Initialize the dependency checker\ndep_checker = DependencyChecker()\ndep_checker.register_cell('dependency_checker')\n\n# Show initial status\nlogger.info(\"Dependency checker initialized\")\ndep_checker.show_dependency_tree()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78c81bad-9ff7-4b71-ac0d-d52f62092857",
   "metadata": {
    "language": "python",
    "name": "notebook_monitor"
   },
   "outputs": [],
   "source": "# Cell: notebook_monitor\n\"\"\"\nDependencies: dependency_checker\nProvides: Enhanced monitoring with memory usage and operation counts\n\"\"\"\nimport time\nimport pandas as pd\nimport psutil\nimport os\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Any\n\nclass ScoringMonitor:\n    def __init__(self):\n        if not dep_checker.check_dependencies('notebook_monitor'):\n            print(\"Warning: Dependencies not met for notebook monitor\")\n            \n        self.execution_logs = []\n        self.current_cell = None\n        self.start_time = None\n        self.end_time = None\n        self.operation_counts = {}\n        self.scoring_metrics = {}\n        self.context = []\n        pd.set_option('display.max_rows', None)\n        pd.set_option('display.max_columns', None)\n\n    def add_context(self, context: str) -> None:\n        \"\"\"Add context to the current monitoring\"\"\"\n        self.context.append(context)\n        \n    def remove_context(self) -> None:\n        \"\"\"Remove the most recent context\"\"\"\n        if self.context:\n            self.context.pop()\n\n    def _get_memory_usage(self):\n        \"\"\"Get current memory usage in MB\"\"\"\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss / 1024 / 1024\n\n    def start(self, operation_name: str) -> None:\n        \"\"\"Start monitoring an operation\"\"\"\n        if self.context:\n            operation_name = f\"{operation_name}_{'_'.join(self.context)}\"\n            \n        self.current_cell = {\n            'operation': operation_name,\n            'start_time': datetime.now(),\n            'status': 'running',\n            'duration_seconds': 0,\n            'start_memory_mb': self._get_memory_usage(),\n            'records_processed': 0,\n            'error': None\n        }\n        self.start_time = time.time()\n\n    def update_metrics(self, metrics: Dict[str, Any]) -> None:\n        \"\"\"Update scoring metrics\"\"\"\n        if self.current_cell:\n            self.current_cell.update(metrics)\n\n    def end(self) -> None:\n        \"\"\"End monitoring current operation\"\"\"\n        if self.current_cell:\n            self.end_time = time.time()\n            duration = self.end_time - self.start_time\n            end_memory = self._get_memory_usage()\n            \n            self.current_cell.update({\n                'end_time': datetime.now(),\n                'status': 'completed',\n                'duration_seconds': round(duration, 2),\n                'end_memory_mb': end_memory,\n                'memory_change_mb': round(end_memory - self.current_cell['start_memory_mb'], 2)\n            })\n            \n            self.execution_logs.append(self.current_cell.copy())\n            self.current_cell = None\n\n    def show_summary(self) -> pd.DataFrame:\n        \"\"\"Show execution summary\"\"\"\n        if not self.execution_logs:\n            return pd.DataFrame()\n    \n        try:\n            # Create DataFrame from logs\n            df = pd.DataFrame(self.execution_logs)\n            \n            # Format timestamps\n            for col in ['start_time', 'end_time']:\n                if col in df.columns:\n                    df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d %H:%M:%S')\n            \n            # Add scoring specific metrics\n            total_duration = df['duration_seconds'].sum()\n            total_records = df['records_processed'].sum()\n            \n            print(\"\\nScoring Summary:\")\n            print(f\"Total Duration: {total_duration:.2f} seconds\")\n            print(f\"Total Records Processed: {total_records:,}\")\n            if total_duration > 0:\n                print(f\"Average Processing Rate: {total_records/total_duration:.2f} records/second\")\n            \n            return df\n            \n        except Exception as e:\n            print(f\"Error generating summary: {type(e).__name__} - {str(e)}\")\n            print(traceback.format_exc())\n            return pd.DataFrame()\n\n    def log_error(self, error: Exception) -> None:\n        \"\"\"Log an error for the current operation\"\"\"\n        if self.current_cell:\n            self.current_cell.update({\n                'status': 'error',\n                'error': f\"{type(error).__name__}: {str(error)}\",\n                'end_memory_mb': self._get_memory_usage()\n            })\n            self.end()\n\n# Initialize monitor\nmonitor = ScoringMonitor()\ndep_checker.register_cell('notebook_monitor')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8708664-ee4f-4e07-ae1e-3a971252beb6",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "# Cell: imports\nmonitor.start('imports')\ntry:\n    \"\"\"\n    Dependencies: dependency_checker, notebook_monitor\n    Provides: All required package imports for the scoring notebook\n    \"\"\"\n    # Standard libraries\n    import logging\n    from typing import Dict, Set, Any, List, Optional, Tuple\n    from datetime import datetime\n    import time\n    import traceback\n    import json\n    import gc\n    import pickle\n    import base64\n    import copy\n\n    \n    # Snowpark\n    from snowflake.snowpark.functions import col, when\n    import snowflake.snowpark.functions as F\n\n\n     # Add Snowpark Session import\n    from snowflake.snowpark import Session\n\n    def get_active_session() -> Session:\n        return Session.get_active_session()\n    # Models\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.svm import SVC\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import StandardScaler\n    \n    # Scientific computing\n    import pandas as pd\n    import numpy as np\n    \n    # Visualization\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # Configure logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('imports')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ce84ad4-3680-4718-8ec9-630fdb82a207",
   "metadata": {
    "language": "python",
    "name": "global_constants"
   },
   "outputs": [],
   "source": "# Cell: global_constants\nmonitor.start('global_constants')\ntry:\n    \"\"\"\n    Dependencies: imports\n    Provides: Global constants for the scoring notebook\n    \"\"\"\n    # Department and Model Configuration\n    DEPARTMENT = 'UF'\n    MODEL_TYPE = 'MAJOR_GIFT'\n    \n    # Database configuration\n    SCHEMA = \"UFF_MODEL\"\n    SOURCE_TABLE = \"UF_MAJOR_GIFT_VIEWS_COMBO\"\n    \n    # Scoring parameters\n    CHUNK_SIZE = 150000   # Size of processing chunks\n\n    #model affinity scores sample limit\n    LIMIT=1500000\n    \n    # Model Selection Weights\n    MODEL_SELECTION_WEIGHTS = {\n        'PRIMARY': {\n            'F1_MEAN': 0.30,           # Weight for F1 score\n            'ROC_AUC_MEAN': 0.30,       # Weight for ROC-AUC\n            'MCC_MEAN': 0.20,          # Weight for Matthews Correlation Coefficient\n            'BALANCED_ACCURACY_MEAN': 0.20  # Weight for balanced accuracy\n        },\n        'SECONDARY': {\n            'PERFORMANCE': 0.40,        # Weight for overall performance\n            'PRECISION_RECALL_BALANCE': 0.30,  # Weight for P-R balance\n            'STABILITY': 0.30           # Weight for model stability\n        }\n    }\n\n\n    OUTPUT_COLUMNS = {\n        'required': [\n            'ID',                 # Entity identifier\n            'AFFINITY_SCORE',     # Numeric score (0-100)\n            'AFFINITY_GRADE',     # Letter grade\n            'PROBABILITY',        # Raw model probability\n            'SCORED_DATE'         # When score was generated\n        ],\n        'optional': [\n            'MODEL_NAME',         # Which model generated the score\n            'MODEL_VERSION',      # Version of model used\n            'FEATURE_COUNT',      # Number of features used\n            'CONFIDENCE'          # Model's confidence in prediction\n        ]\n    }\n    \n    VALIDATION_THRESHOLDS = {\n        'min_score_count': 1000,          # Minimum number of scores to generate\n        'max_null_percentage': 0.01,      # Maximum allowed percentage of nulls\n        'grade_distribution_bounds': {     # Expected grade distribution ranges\n            'A': (0.001, 0.01),           # 0.1% to 1%\n            'B': (0.01, 0.05),            # 1% to 5%\n            'C': (0.05, 0.15),            # 5% to 15%\n            'D': (0.15, 0.30),            # 15% to 30%\n            'E': (0.40, 0.80)             # 40% to 80%\n        }\n    }\n\n    # Probability thresholds for grades (replaces GRADE_THRESHOLDS)\n    PROBABILITY_THRESHOLDS = {\n        'A': 0.995,  # 99.5%\n        'B': 0.980,  # 98%\n        'C': 0.950,  # 95%\n        'D': 0.850,  # 85%\n        'E': 0.000   # Everything else\n    }\n    \n    # Affinity Grade Thresholds\n    GRADE_THRESHOLDS = {\n        'A': 99.5,  # >= 99.5\n        'B': 98.0,  # >= 98.0 and < 99.5\n        'C': 95.0,  # >= 95.0 and < 98.0\n        'D': 85.0,  # >= 85.0 and < 95.0\n        'E': 0.0    # < 85.0\n    }\n\n            # Define grade ranges explicitly\n    GRADE_RANGES = [\n                ('A', 99.50, float('inf')),\n                ('B', 98.00, 99.49),\n                ('C', 95.00, 97.99),\n                ('D', 85.00, 94.99),\n                ('E', float('-inf'), 85.00)\n            ]\n            \n\n    \n    # Table naming templates\n    def get_table_name(base_name: str, timestamp: Optional[str] = None) -> str:\n        \"\"\"Generate standardized table names\"\"\"\n        name = f\"{DEPARTMENT}_{MODEL_TYPE}_{base_name}\"\n        if timestamp:\n            name = f\"{name}_{timestamp}\"\n        return name\n\n    # Define standard table names\n#    TABLE_NAMES = {\n#        'SOURCE': SOURCE_TABLE,\n#        'MODEL_RESULTS': get_table_name('RESULTS'),    # Model evaluation results\n#        'MODELS': get_table_name('MODELS'),                  # Serialized models\n#        'FEATURES': get_table_name('FEATURES'),              # Feature importance\n#        'SCORES': get_table_name('AFFINITY_SCORES'),        # Scoring results\n#        'METADATA': get_table_name('SCORING_METADATA')      # Scoring metadata\n#    }\n\n    TABLE_NAMES = {\n        'SOURCE': SOURCE_TABLE,\n        'MODEL_RESULTS': get_table_name('MODEL_PERFORMANCE'),    # Changed from 'UF_MAJOR_GIFT_MODEL_PERFORMANCE'\n        'MODELS': get_table_name('MODELS'),\n        'FEATURES': get_table_name('FEATURE_IMPORTANCE'),        # Changed from 'FEATURES'\n        'SCORES': get_table_name('AFFINITY_SCORES'),\n        'METADATA': get_table_name('SCORING_METADATA')\n    }    \n    \n\n        # Schema definitions for tables\n    TABLE_SCHEMAS = {\n        'SCORES': \"\"\"(\n            ID VARCHAR,\n            PROBABILITY FLOAT,\n            AFFINITY_SCORE FLOAT,\n            AFFINITY_GRADE VARCHAR,\n            SCORED_DATE TIMESTAMP_NTZ,\n            CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n        )\"\"\",\n        'METADATA': \"\"\"(\n            SCORING_RUN_ID VARCHAR,\n            MODEL_NAME VARCHAR,\n            FEATURE_SAMPLE VARCHAR,\n            N_FEATURES NUMBER,\n            F1_SCORE FLOAT,\n            ROC_AUC FLOAT,\n            RECORDS_SCORED NUMBER,\n            SCORING_START TIMESTAMP_NTZ,\n            WHERE_CLAUSE VARCHAR,\n            GRADE_DISTRIBUTION VARIANT,\n            FEATURES_USED VARIANT\n        )\"\"\"\n    }\n\n    # Add debug output to verify configuration\n    print(\"\\nConfiguration Settings:\")\n    print(\"=\" * 50)\n    \n    print(\"\\nTable References:\")\n    for key, value in TABLE_NAMES.items():\n        print(f\"{key}: {value}\")\n        \n    print(\"\\nModel Selection Weights:\")\n    print(\"\\nPrimary Weights:\")\n    for metric, weight in MODEL_SELECTION_WEIGHTS['PRIMARY'].items():\n        print(f\"{metric}: {weight:.2f}\")\n    print(\"\\nSecondary Weights:\")\n    for aspect, weight in MODEL_SELECTION_WEIGHTS['SECONDARY'].items():\n        print(f\"{aspect}: {weight:.2f}\")\n        \n    print(\"\\nAffinity Grade Thresholds:\")\n    for grade, threshold in GRADE_THRESHOLDS.items():\n        print(f\"Grade {grade}: >= {threshold}\")\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('global_constants')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f141557-95a0-4679-9e43-55e679f476f8",
   "metadata": {
    "language": "python",
    "name": "environment_config"
   },
   "outputs": [],
   "source": "# Cell: environment_config\nmonitor.start('environment_config')\ntry:\n    class EnvironmentConfig:\n        # Static configurations that don't change between environments\n        SCHEMA = SCHEMA  # From global_constants\n        TABLES = {\n            'SOURCE': TABLE_NAMES['SOURCE'],\n            'MODEL_RESULTS': TABLE_NAMES['MODEL_RESULTS'],  # Using updated table name\n            'MODELS': TABLE_NAMES['MODELS'],\n            'FEATURES': TABLE_NAMES['FEATURES'],\n            'SCORES': TABLE_NAMES['SCORES'],\n            'METADATA': TABLE_NAMES['METADATA']\n        }\n        \n        # Environment configurations\n        ENVIRONMENTS = {\n            'test': {\n                'database': 'PRE_PRODUCTION',\n                'schema': SCHEMA,\n                'warehouse': 'ANALYSIS'\n            },\n            'prod': {\n                'database': 'PRODUCTION',\n                'schema': SCHEMA,\n                'warehouse': 'ANALYSIS'\n            }\n        }\n        \n# Cell: environment_config\n        def __init__(self):\n            self.current_env = 'test'  # default to test environment\n            self.session = None\n            \n            # Validate table configurations\n            required_tables = {'SOURCE', 'MODEL_RESULTS', 'MODELS', 'FEATURES', 'SCORES', 'METADATA'}\n            missing_tables = required_tables - set(self.TABLES.keys())\n            if missing_tables:\n                raise ValueError(f\"Missing required table configurations: {missing_tables}\")        \n        def set_session(self, session):\n            \"\"\"Set the Snowflake session\"\"\"\n            self.session = session\n            self._apply_environment()\n        \n        def _apply_environment(self):\n            \"\"\"Apply environment settings to Snowflake session\"\"\"\n            if not self.session:\n                raise ValueError(\"Session not initialized\")\n                \n            env = self.ENVIRONMENTS[self.current_env]\n            try:\n                self.session.sql(f\"USE DATABASE {env['database']}\").collect()\n                self.session.sql(f\"USE SCHEMA {env['schema']}\").collect()\n                self.session.sql(f\"USE WAREHOUSE {env['warehouse']}\").collect()\n            except Exception as e:\n                print(f\"Error setting environment: {str(e)}\")\n                raise\n\n        def get_schema_db_name(self) -> str:\n            \"\"\"Get database.schema\"\"\"\n            env = self.ENVIRONMENTS[self.current_env]\n            return f\"{env['database']}.{env['schema']}\"\n            \n        def show_environment_status(self):\n            \"\"\"Display current environment settings\"\"\"\n            if not self.session:\n                raise ValueError(\"Session not initialized\")\n                \n            border = \"=\" * 60\n            print(border)\n            print(f\"{'ENVIRONMENT STATUS':^60}\")\n            print(border)\n            print(f\"ENVIRONMENT: {self.current_env.upper()}\")\n            \n            try:\n                current_settings = self.session.sql(\"\"\"\n                SELECT \n                    CURRENT_DATABASE() as database,\n                    CURRENT_SCHEMA() as schema,\n                    CURRENT_WAREHOUSE() as warehouse\n                \"\"\").collect()\n                \n                print(f\"\"\"\n                DATABASE:  {current_settings[0]['DATABASE']}\n                SCHEMA:    {current_settings[0]['SCHEMA']}\n                WAREHOUSE: {current_settings[0]['WAREHOUSE']}\n                \"\"\")\n                print(border)\n            except Exception as e:\n                print(f\"Error getting current settings: {str(e)}\")\n                raise\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('environment_config')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bf4bd49-01f3-44b6-b25a-ac370f445b41",
   "metadata": {
    "language": "python",
    "name": "snowflake_utils"
   },
   "outputs": [],
   "source": "# Cell: snowflake_utils\nmonitor.start('snowflake_utils')\ntry:\n    class ScoringDatabaseManager:\n        def __init__(self, config: EnvironmentConfig):\n            self.config = config\n            self.session = config.session\n            self.max_retries = 3\n            self.retry_delay = 5  # seconds\n\n        def execute_with_retry(self, operation_name: str, operation, *args, **kwargs):\n            \"\"\"Execute operation with retry logic\"\"\"\n            monitor.start(f'execute_{operation_name}')\n            try:\n                for attempt in range(self.max_retries):\n                    try:\n                        result = operation(*args, **kwargs)\n                        return result\n                    except Exception as e:\n                        if attempt < self.max_retries - 1:\n                            print(f\"Error in {operation_name}, retrying... (Attempt {attempt + 1}/{self.max_retries})\")\n                            time.sleep(self.retry_delay * (attempt + 1))\n                            self.session = get_active_session()\n                            self.config._apply_environment()\n                        else:\n                            raise\n            finally:\n                monitor.end()\n\n        def load_data_for_scoring(self, features: List[str], \n                                 where_clause: Optional[str] = None, \n                                 limit: Optional[int] = None) -> pd.DataFrame:\n            \"\"\"Load only needed features for scoring\"\"\"\n            monitor.start('load_scoring_data')\n            try:\n                # Create feature list maintaining order\n                        # Always include DONOR_ID with features\n                all_features = ['DONOR_ID'] + [f for f in features if f != 'DONOR_ID']\n                feature_list = ', '.join([f'\"{f}\"' for f in all_features])\n                \n                query = f\"\"\"\n                SELECT {feature_list}\n                FROM {self.config.get_schema_db_name()}.{TABLE_NAMES['SOURCE']}\n                \"\"\"\n                if where_clause:\n                    query += f\" WHERE {where_clause}\"\n                if limit:\n                    query += f\" LIMIT {limit}\"\n                \n                print(f\"Loading data for scoring...\")\n                print(f\"Features being selected  (in order):\")\n                for i, feat in enumerate(all_features):\n                    print(f\"{i+1}. {feat}\")\n                    \n                result = self.execute_with_retry(\n                    'load_data',\n                    lambda: self.session.sql(query).to_pandas()\n                )\n                \n                # Verify column order matches request, but preserve DONOR_ID\n#                if 'DONOR_ID' in result.columns:\n#                    donor_ids = result['DONOR_ID'].copy()\n                    # Reorder only the feature columns\n#                    result = result[features]\n                \n                    # Add DONOR_ID back\n #                   result.insert(0, 'DONOR_ID', donor_ids)\n #               else:\n #                   raise ValueError(\"DONOR_ID not found in loaded data\")\n            \n\t\t\t\t\n               \n                print(f\"\\nLoaded {len(result)} records with features in correct order\")\n                print(\"Available columns:\", result.columns.tolist())\n                return result\n                \n            finally:\n                monitor.end()\n                                  \n        def save_scores(self, df: pd.DataFrame, timestamp: str=None) -> None:\n            \"\"\"Save scoring results\"\"\"\n            monitor.start('save_scores')\n            try:\n                # Add validation for table name\n                if 'SCORES' not in TABLE_NAMES:\n                    raise ValueError(\"SCORES table name not found in TABLE_NAMES\")\n                    \n                # Ensure required columns are present\n                if 'required' not in OUTPUT_COLUMNS:\n                    raise ValueError(\"Required columns not defined in OUTPUT_COLUMNS\")\n                    \n                # Ensure required columns are present\n                missing_cols = set(OUTPUT_COLUMNS['required']) - set(df.columns)\n                if missing_cols:\n                    raise ValueError(f\"Missing required columns: {missing_cols}\")\n                \n                # Save to Snowflake\n                table_name = TABLE_NAMES['SCORES']\n                full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                \n                snowpark_df = self.session.create_dataframe(df)\n                snowpark_df.write.save_as_table(full_name, mode='overwrite')\n                \n                print(f\"overwritten scores to: {full_name}\")\n                \n            finally:\n                monitor.end()\n\n        def save_scoring_metadata(self, metadata: Dict[str, Any]) -> None:\n            \"\"\"Save metadata about scoring run\"\"\"\n            monitor.start('save_metadata')\n            try:\n                metadata_df = pd.DataFrame([metadata])\n                table_name = TABLE_NAMES['METADATA']\n                full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                \n                snowpark_df = self.session.create_dataframe(metadata_df)\n                snowpark_df.write.save_as_table(full_name, mode='append')\n                \n                print(f\"Saved metadata to: {full_name}\")\n                \n            finally:\n                monitor.end()\n\n        # In snowflake_utils.py of scoring notebook\n        def ensure_scoring_tables(self) -> None:\n            \"\"\"Ensure scoring-specific tables exist\"\"\"\n            schemas = {\n                'SCORES': f\"\"\"\n                    CREATE TABLE IF NOT EXISTS {TABLE_NAMES['SCORES']} (\n                        ID VARCHAR,\n                        PROBABILITY FLOAT,\n                        AFFINITY_SCORE FLOAT,\n                        AFFINITY_GRADE VARCHAR,\n                        SCORED_DATE TIMESTAMP_NTZ\n                    )\n                \"\"\",\n                'METADATA': f\"\"\"\n                    CREATE TABLE IF NOT EXISTS {TABLE_NAMES['METADATA']} (\n                        SCORING_RUN_ID VARCHAR,\n                        MODEL_NAME VARCHAR,\n                        FEATURE_SAMPLE VARCHAR,\n                        N_FEATURES NUMBER,\n                        F1_SCORE FLOAT,\n                        ROC_AUC FLOAT,\n                        RECORDS_SCORED NUMBER,\n                        SCORING_START TIMESTAMP_NTZ,\n                        WHERE_CLAUSE VARCHAR,\n                        GRADE_DISTRIBUTION VARIANT,\n                        FEATURES_USED VARIANT\n                    )\n                \"\"\"\n            }\n            \n            for table_type, create_sql in schemas.items():\n                table_name = TABLE_NAMES[table_type]\n                full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                \n                # Check if table exists\n                exists_query = f\"\"\"\n                SELECT 1 \n                FROM INFORMATION_SCHEMA.TABLES \n                WHERE TABLE_SCHEMA = '{self.config.SCHEMA}'\n                AND TABLE_NAME = '{table_name}'\n                \"\"\"\n        \n                result = self.execute_with_retry(\n                    'check_table_exists',\n                    lambda: self.session.sql(exists_query).collect()\n                )\n                \n                if not result:\n                    print(f\"Creating table: {table_name}\")\n                    self.execute_with_retry(\n                        'create_table',\n                        lambda: self.session.sql(create_sql).collect()\n                    )\n\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('snowflake_utils')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "model_scoring"
   },
   "source": "# Cell: model_scoring\nmonitor.start('model_scoring')\ntry:\n    class AffinityScorer:\n        def __init__(self, config: EnvironmentConfig):\n            self.config = config\n            self.db_manager = ScoringDatabaseManager(config)\n            self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n            self.model = None\n            self.features = None\n            self.scaler = None\n            self.imputers = None\n            self.model_info = None\n\n            # Ensure required tables exist\n            self.db_manager.ensure_scoring_tables()\n\n        def get_best_model_info(self, target: str = 'COMMIT_MAJOR') -> Dict[str, Any]:\n            print(\"\\nDEBUG: Looking for best model...\")\n            full_table_name = f\"{self.config.get_schema_db_name()}.{TABLE_NAMES['MODEL_RESULTS']}\"\n            print(f\"Checking table: {full_table_name}\")\n            \"\"\"Get the best performing model using multiple metrics\"\"\"\n            print(\"\\nDEBUG: Table Information:\")\n            full_table_name = f\"{self.config.get_schema_db_name()}.{TABLE_NAMES['MODEL_RESULTS']}\"\n            #            print(f\"Table name from TABLE_NAMES: {TABLE_NAMES['MODEL_RESULTS']}\")\n            print(f\"Full qualified table name: {self.config.get_schema_db_name()}.{TABLE_NAMES['MODEL_RESULTS']}\")\n\n\n            print(\"\\nDEBUG: Checking Snowflake columns\")\n           # First, let's see what columns actually exist\n            columns_query = f\"\"\"\n            SELECT *\n            FROM  {self.config.get_schema_db_name()}.{TABLE_NAMES['MODEL_RESULTS']}\n            LIMIT 1\n            \"\"\"\n            try:\n                columns_result = self.db_manager.execute_with_retry(\n                 'check_columns',\n                  lambda: self.db_manager.session.sql(columns_query).collect()\n                )\n                if columns_result:\n    #            print(\"Available columns:\", columns_result[0].keys())\n                # Convert first row to dictionary to get column names   \n                   print(\"Available columns:\", list(columns_result[0].asDict().keys())) #.columns))\n                else:\n                  print(\"No data found in table\")\n\n            except Exception as e:\n                print(f\"Error checking columns: {str(e)}\") \n    \n#            if columns_result:\n#               print(\"Available columns:\", columns_result[0].keys())\n#         First, get all models and their rankings for debugging\n            debug_query = f\"\"\"\n            WITH MODEL_METRICS AS (\n                SELECT \n                    MODEL,\n                    FEATURE_SAMPLE,\n                    N_FEATURES,\n                    F1_MEAN,\n                    ROC_AUC_MEAN,\n                    ACCURACY_MEAN,\n                    PRECISION_MEAN,\n                    RECALL_MEAN,\n                    (F1_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['F1_MEAN']} +\n                     ROC_AUC_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['ROC_AUC_MEAN']} +\n                     ACCURACY_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['BALANCED_ACCURACY_MEAN']}) \n                    AS PRIMARY_SCORE,\n                    ROW_NUMBER() OVER (\n                        PARTITION BY FEATURE_SAMPLE \n                        ORDER BY F1_MEAN DESC\n                    ) as RANK_IN_GROUP\n                FROM    {full_table_name} \n                WHERE TARGET = '{target}'\n            )\n            SELECT *\n            FROM MODEL_METRICS\n            ORDER BY FEATURE_SAMPLE, RANK_IN_GROUP;\n            \"\"\"\n            \n            print(\"\\nDEBUG: All Models and Rankings\")\n            print(\"=\" * 80)\n            debug_result = self.db_manager.execute_with_retry(\n                'debug_models',\n                lambda: self.db_manager.session.sql(debug_query).to_pandas()\n            )\n            \n            if not debug_result.empty:\n                # Show all models grouped by feature sample\n                for feature_sample in debug_result['FEATURE_SAMPLE'].unique():\n                    sample_data = debug_result[debug_result['FEATURE_SAMPLE'] == feature_sample]\n                    print(f\"\\nFeature Sample: {feature_sample}\")\n                    print(\"-\" * 40)\n                    for _, row in sample_data.iterrows():\n                        print(f\"Model: {row['MODEL']}\")\n                        print(f\"  Rank: {row['RANK_IN_GROUP']}\")\n                        print(f\"  F1: {row['F1_MEAN']:.4f}\")\n                        print(f\"  ROC-AUC: {row['ROC_AUC_MEAN']:.4f}\")\n                        print(f\"  Primary Score: {row['PRIMARY_SCORE']:.4f}\")\n            else:\n                print(\"No models found in debug query\")\n        \n            # Now get the best model\n            query = f\"\"\"\n            WITH MODEL_METRICS AS (\n                SELECT \n                    MODEL,\n                    FEATURE_SAMPLE,\n                    N_FEATURES,\n                    F1_MEAN,\n                    ROC_AUC_MEAN,\n                    ACCURACY_MEAN,\n                    PRECISION_MEAN,\n                    RECALL_MEAN,\n\n                    -- Calculate composite scores using configured weights\n                    (F1_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['F1_MEAN']} +\n                     ROC_AUC_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['ROC_AUC_MEAN']} +\n\n                     ACCURACY_MEAN * {MODEL_SELECTION_WEIGHTS['PRIMARY']['BALANCED_ACCURACY_MEAN']}) \n                    AS PRIMARY_SCORE,\n                    ROW_NUMBER() OVER (\n                        PARTITION BY FEATURE_SAMPLE \n                        ORDER BY F1_MEAN DESC\n                    ) as rank_in_group\n                FROM {full_table_name}\n                WHERE TARGET = '{target}'\n            )\n            SELECT \n                MODEL,\n                FEATURE_SAMPLE,\n                N_FEATURES,\n                F1_MEAN,\n                ROC_AUC_MEAN,\n                ACCURACY_MEAN as BALANCED_ACCURACY_MEAN,\n                PRECISION_MEAN,\n                RECALL_MEAN,\n--                MCC_MEAN,\n                PRIMARY_SCORE\n            FROM MODEL_METRICS\n            WHERE rank_in_group = 1\n            ORDER BY PRIMARY_SCORE DESC\n            LIMIT 1;\n            \"\"\"\n    \n            # Changed this part to use db_manager's session\n            result = self.db_manager.execute_with_retry(\n                'get_best_model',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            if result.empty:\n                raise ValueError(\"No model results found\")\n            \n            self.model_info = result.iloc[0].to_dict()\n\n            # Debug output for selected model\n            print(\"\\nDEBUG: Selected Best Model\")\n            print(\"=\" * 80)\n            print(f\"Model Name: {self.model_info['MODEL']}\")\n            print(f\"Feature Sample: {self.model_info['FEATURE_SAMPLE']}\")\n            print(f\"Number of Features: {self.model_info['N_FEATURES']}\")\n            print(\"\\nPerformance Metrics:\")\n            print(f\"F1 Score: {self.model_info['F1_MEAN']:.4f}\")\n            print(f\"ROC-AUC: {self.model_info['ROC_AUC_MEAN']:.4f}\")\n            print(f\"Balanced Accuracy: {self.model_info['BALANCED_ACCURACY_MEAN']:.4f}\")\n                 \n            return self.model_info        \n\n\n\n        def load_model_components(self) -> None:\n            \"\"\"Load model and its components\"\"\"\n            query = f\"\"\"\n            SELECT MODEL_OBJECT, SELECTED_FEATURES, METRICS, SCALER, IMPUTERS\n            FROM {self.config.get_schema_db_name()}.{TABLE_NAMES['MODELS']} \n            WHERE MODEL = '{self.model_info['MODEL']}'\n            AND FEATURE_SAMPLE = '{self.model_info['FEATURE_SAMPLE']}'\n            ORDER BY CREATED_AT DESC\n            LIMIT 1;\n            \"\"\"\n            \n            print(\"\\nLoading best model components...\")\n#            print(f\"Model: {self.model_info['MODEL']}\")\n            print(f\"Feature Sample: {self.model_info['FEATURE_SAMPLE']}\")\n            print(f\"Looking for model with name: {self.model_info['MODEL']}\")\n\n            \n            result = self.db_manager.execute_with_retry(\n                'load_model',\n                lambda: self.db_manager.session.sql(query).collect()\n            )\n            \n            if not result:\n                raise ValueError(\"Could not load model components\")\n            \n            row = result[0]\n            \n            # Load base components\n            self.model = pickle.loads(base64.b64decode(row['MODEL_OBJECT']))\n            self.features = pickle.loads(base64.b64decode(row['SELECTED_FEATURES']))\n            original_scaler = pickle.loads(base64.b64decode(row['SCALER']))\n            original_imputers = pickle.loads(base64.b64decode(row['IMPUTERS']))\n            \n            # Fix scaler first\n            print(\"\\nProcessing scaler:\")\n            print(\"Original scaler features:\", original_scaler.feature_names_in_.tolist())\n            print(f\"Number of original scaler features: {len(original_scaler.feature_names_in_)}\")\n            print(\"Features we need:\", self.features)\n            print(f\"Number of features we need: {len(self.features)}\")\n        \n            # Create new scaler with only the features we need\n            self.scaler = StandardScaler()\n            # Create dummy data with correct features\n            dummy_data = np.zeros((2, len(self.features)))\n            if hasattr(original_scaler, 'mean_'):\n                for i, feature in enumerate(self.features):\n                    if feature in original_scaler.feature_names_in_:\n                        idx = list(original_scaler.feature_names_in_).index(feature)\n                        dummy_data[0, i] = original_scaler.mean_[idx]\n                        dummy_data[1, i] = original_scaler.mean_[idx] + original_scaler.scale_[idx]\n        \n            # Fit new scaler with correct features\n            self.scaler.fit(pd.DataFrame(dummy_data, columns=self.features))\n        \n            print(\"New scaler configuration:\")\n            print(f\"Features: {self.scaler.feature_names_in_.tolist()}\")\n            print(f\"Number of features: {len(self.scaler.feature_names_in_)}\")\n            \n            # Process imputers\n            self.imputers = {}\n            for strategy, imputer in original_imputers.items():\n                print(f\"\\nProcessing {strategy} imputer:\")\n                print(\"Original features:\", imputer.feature_names_in_.tolist())\n                \n                # Get intersection of imputer features and our needed features\n                valid_features = [f for f in imputer.feature_names_in_ if f in self.features]\n                if valid_features:\n                    print(f\"Features to keep: {valid_features}\")\n                    \n                    # Create fresh imputer with same strategy\n                    if hasattr(imputer, 'strategy'):\n                        new_imputer = SimpleImputer(strategy=imputer.strategy)\n                    else:\n                        new_imputer = SimpleImputer(strategy='constant', fill_value=0)\n                    \n                    # Get values for these features from original imputer\n                    indices = [list(imputer.feature_names_in_).index(f) for f in valid_features]\n                    feature_values = imputer.statistics_[indices]\n                    \n                    # Create dummy data to fit new imputer\n                    dummy_data = np.zeros((2, len(valid_features)))\n                    dummy_data[0] = feature_values  # Set first row to our statistics\n                    \n                    # Fit new imputer with correct features\n                    new_imputer.fit(pd.DataFrame(dummy_data, columns=valid_features))\n                    \n                    print(f\"New imputer features: {new_imputer.feature_names_in_.tolist()}\")\n                    print(f\"New statistics shape: {new_imputer.statistics_.shape}\")\n                    \n                    self.imputers[strategy] = new_imputer\n            \n            print(\"\\nFinal component verification:\")\n            print(f\"Total features to use: {len(self.features)}\")\n            for strategy, imputer in self.imputers.items():\n                print(f\"\\n{strategy} imputer:\")\n                print(f\"Features: {imputer.feature_names_in_.tolist()}\")\n                print(f\"Statistics shape: {imputer.statistics_.shape}\")\n        \n        def calculate_affinity_score(self, probabilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n            \"\"\"Convert probabilities to scores and grades\"\"\"\n            print(\"\\nProbability Analysis:\")\n            print(f\"Min probability: {probabilities.min():.3f}\")\n            print(f\"Max probability: {probabilities.max():.3f}\")\n            print(f\"Mean probability: {probabilities.mean():.3f}\")\n            \n            # Round probabilities to 3 decimal places first\n            probabilities = np.round(probabilities, 3)\n            \n            # Convert to scores (0-100) and round to 3 decimal places\n            scores = np.round(probabilities * 100, 1)\n#            scores = np.array(scores).flatten()\n            \n            # Initialize grades array\n            grades = np.full(len(scores), 'E', dtype='object')\n            \n            # Define grade ranges explicitly\n            grade_ranges = GRADE_RANGES \n#                [\n#                ('A', 99.500, float('inf')),\n#                ('B', 98.000, 99.500),\n#                ('C', 95.000, 98.000),\n#                ('D', 85.000, 95.000),\n#                ('E', float('-inf'), 85.000)             ]\n            \n            print(\"\\nGrade Assignment Process:\")\n            for grade, lower_bound, upper_bound  in grade_ranges:\n                if grade == 'E':\n                    mask = (scores < upper_bound )\n                elif grade == 'A':\n                    mask = (scores >= lower_bound)\n                else:\n                    mask = (scores >= lower_bound) & (scores < upper_bound )\n                \n                grades[mask] = grade\n                count = np.sum(mask)\n                print(f\"\\nGrade {grade} ({lower_bound:.3f} to {upper_bound :.3f}):\")\n                print(f\"Found {count} records ({count/len(grades):.2%})\")\n                \n                if count > 0:\n                    sample_idx = np.where(mask)[0][:5]\n                    print(\"Sample records:\")\n                    for idx in sample_idx:\n                        print(f\"  Score: {scores[idx]:.3f}, Probability: {probabilities[idx]:.3f}\")\n      # Validation - ensure no mismatches between scores and grades          \n            print(\"\\nValidation Check:\")\n            for grade, lower_bound, upper_bound  in grade_ranges:\n                if grade == 'E':\n                    expected_mask = (scores < upper_bound )\n                elif grade == 'A':\n                    expected_mask = (scores >= lower_bound)\n                else:\n                    expected_mask = (scores >= lower_bound) & (scores < upper_bound )\n                    \n                actual_mask = (grades == grade)\n                mismatches = np.sum(expected_mask != actual_mask)\n                if mismatches > 0:\n                    print(f\"WARNING: Found {mismatches} mismatches for grade {grade}\")\n                    # Show example mismatches\n                    mismatch_idx = np.where(expected_mask != actual_mask)[0][:5]\n                    print(\"Sample mismatches:\")\n                    for idx in mismatch_idx:\n                        print(f\"  Score: {scores[idx]:.1f}, Expected: {grade}, Got: {grades[idx]}\")\n                else:\n                    print(f\"✓ Grade {grade}: No mismatches\")\n            \n                # Verify probability to score conversion is consistent\n                print(\"\\nValidation - Probability to Score Conversion:\")\n                for i in range(5):  # Show sample conversions\n                    idx = np.random.randint(0, len(probabilities))\n                    expected = probabilities[idx] * 100\n                    actual = scores[idx]\n                    print(f\"Probability: {probabilities[idx]:.3f} → Expected: {expected:.3f} → Actual Score: {actual:.1f}\")\n\n\n            \n            return scores, grades\n            \n\n            \n        def prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:\n            \"\"\"Prepare data for scoring using saved feature order\"\"\"\n            monitor.start('prepare_data')\n            try:\n               # Verify DONOR_ID is present\n                if 'DONOR_ID' not in df.columns:\n                    raise ValueError(\"DONOR_ID not found in input data\")\n\n                # Save DONOR_ID before feature selection\n                donor_ids = df['DONOR_ID'].copy()\n                print(f\"Preserved {len(donor_ids)} DONOR_IDs\")\n                \n                # Select features in correct order\n                feature_df = df[self.features].copy()\n                \n                # Data validation before imputation\n                print(\"\\nPre-imputation validation:\")\n                null_counts = feature_df.isnull().sum()\n                if null_counts.any():\n                    print(\"Columns with nulls:\")\n                    print(null_counts[null_counts > 0])\n                \n                # Apply imputation for each strategy\n                for strategy, imputer in self.imputers.items():\n                    print(f\"\\nProcessing {strategy} imputation:\")\n                    # Create DataFrame with only the features this imputer handles\n                    features_to_impute = imputer.feature_names_in_\n                    print(f\"Imputer features: {features_to_impute.tolist()}\")\n                    \n                    if len(features_to_impute) > 0:\n                        impute_df = feature_df[features_to_impute]\n                        print(f\"Data shape for imputation: {impute_df.shape}\")\n                        \n                        # Verify features match imputer expectations\n                        if impute_df.shape[1] != len(features_to_impute):\n                            raise ValueError(f\"Feature count mismatch: got {impute_df.shape[1]}, expected {len(features_to_impute)}\")\n                        \n                        # Apply imputation\n                        feature_df[features_to_impute] = imputer.transform(impute_df)\n                \n                # Verify no nulls after imputation\n                post_null_counts = feature_df.isnull().sum()\n                if post_null_counts.any():\n                    print(\"\\nWARNING: Nulls remain after imputation:\")\n                    print(post_null_counts[post_null_counts > 0])\n        \n                # Apply scaling\n                feature_df = pd.DataFrame(\n                    self.scaler.transform(feature_df),\n                    columns=self.features,\n                    index=feature_df.index\n                )\n                \n                # Add DONOR_ID back\n                feature_df['DONOR_ID'] = donor_ids\n                \n                return feature_df\n            finally:\n                monitor.end()\n        \n                \n        def score_dataset(self, where_clause: Optional[str] = None, \n                         save_results: bool = True,\n                         limit: Optional[int] = None) -> pd.DataFrame:\n            \"\"\"Score the dataset\"\"\"\n            monitor.start('score_dataset')\n            try:\n                # First get best model and its features\n                print(\"Loading model components...\")\n                self.get_best_model_info()\n                self.load_model_components()\n                \n                # Now load only the needed features\n                print(\"Loading data...\")\n                df = self.db_manager.load_data_for_scoring(\n                    features=self.features, \n                    where_clause=where_clause,\n                    limit=limit\n                )\n                \n                print(f\"Total records to process: {len(df)}\")\n                \n                # Process entire dataset at once since we're already limiting\n                try:\n                    # Prepare features\n                    feature_df = self.prepare_data(df)\n                    \n                    # Remove DONOR_ID before prediction\n                    donor_ids = feature_df['DONOR_ID']\n                    prediction_features = feature_df.drop('DONOR_ID', axis=1)\n                    \n                    # Debug feature values before prediction\n                    print(\"\\nFeature Statistics Before Prediction:\")\n                    print(prediction_features.describe())\n                    print(\"\\nFeature columns in order:\", prediction_features.columns.tolist())\n                    \n                    # Debug model info\n                    print(\"\\nModel Information:\")\n                    print(f\"Model type: {type(self.model).__name__}\")\n                    if hasattr(self.model, 'feature_names_in_'):\n                        print(f\"Model expected features: {self.model.feature_names_in_.tolist()}\")\n                    \n                    # Make predictions\n                    probabilities = self.model.predict_proba(prediction_features)\n                    print(\"\\nRaw Prediction Output:\")\n                    print(f\"Shape: {probabilities.shape}\")\n                    print(f\"Class probabilities min: {probabilities.min():.3f}\")\n                    print(f\"Class probabilities max: {probabilities.max():.3f}\")\n                    \n                    # Take positive class probabilities and round immediately\n                    probabilities = np.round(probabilities[:, 1], 3)  # Round to 3 decimal places\n                    print(\"\\nVerification of probability rounding:\")\n                    print(f\"Sample of raw vs rounded probabilities:\")\n                    sample_indices = np.random.choice(len(probabilities), min(5, len(probabilities)), replace=False)\n                    for idx in sample_indices:\n                        orig = probabilities[idx]  # This is already rounded\n                        score = orig * 100\n                        print(f\"Probability: {orig:.3f} -> Score: {score:.1f}\")\n        \n                    print(\"\\nUnique probability values:\")\n                    unique_probs = np.unique(probabilities)\n                    print(\"First few:\", unique_probs[:10])\n                    print(\"Last few:\", unique_probs[-10:])\n                    \n                    scores, grades = self.calculate_affinity_score(probabilities)\n                    \n                    # Create results\n                    results_df = pd.DataFrame({\n                        'ID': donor_ids,\n                        'PROBABILITY': np.round(probabilities, 3),\n                        'AFFINITY_SCORE': np.round(scores, 1),\n                        'AFFINITY_GRADE': grades,\n                        'SCORED_DATE': datetime.now()\n                    })\n                    \n                    print(f\"\\nFinal Results Shape: {results_df.shape}\")\n                    print(\"\\nGrade Distribution:\")\n                    print(results_df['AFFINITY_GRADE'].value_counts())\n                    \n                    if save_results:\n                        print(f\"\\nSaving results...\")\n                        self.db_manager.save_scores(results_df)\n                        \n                        # Save metadata\n                        metadata = {\n                            'SCORING_RUN_ID': self.timestamp,\n                            'MODEL': self.model_info['MODEL'],\n                            'FEATURE_SAMPLE': self.model_info['FEATURE_SAMPLE'],\n                            'N_FEATURES': self.model_info['N_FEATURES'],\n                            'F1_SCORE': self.model_info['F1_MEAN'],\n                            'ROC_AUC': self.model_info['ROC_AUC_MEAN'],\n                            'RECORDS_SCORED': len(results_df),\n                            'SCORING_START': datetime.now(),\n                            'WHERE_CLAUSE': where_clause,\n                            'GRADE_DISTRIBUTION': results_df['AFFINITY_GRADE'].value_counts().to_dict(),\n                            'FEATURES_USED': self.features\n                        }\n                        \n                        self.db_manager.save_scoring_metadata(metadata)\n                        print(\"Results and metadata saved successfully\")\n                    else:\n                        print(\"\\nResults not saved (save_results=False)\")\n                    \n                    return results_df\n                    \n                except Exception as e:\n                    print(f\"Error during scoring: {str(e)}\")\n                    raise\n                    \n            finally:\n                monitor.end()\n                          \n    # Test scoring\n    if __name__ == \"__main__\":\n        try:\n            config = EnvironmentConfig()\n            session = get_active_session()\n            config.set_session(session)\n            \n            # Initialize scorer\n            scorer = AffinityScorer(config)\n\n            # Define exclusion where clause\n            exclusion_where_clause = \"\"\"\n            DONOR_ID NOT IN (\n                SELECT DISTINCT C.\"ucinn_ascendv2__Donor_ID__c\"\n                FROM PRE_PRODUCTION.ASCEND.\"Contact\" C\n                WHERE C.\"IsDeleted\" = FALSE\n                AND (\n                    C.\"uff_Is_Lump_Sum_Donor__c\" = TRUE\n                    OR C.\"ucinn_ascendv2__First_and_Last_Name_Formula__c\" LIKE ANY ('%Cash Donations%', '%Anonymous Donor%', 'DONOR%')\n                    OR C.\"ucinn_ascendv2__Contact_Type__c\" LIKE ANY ('%Estate Rep%', '%External Contact%', 'Student')\n                    OR C.\"ucinn_ascendv2__Primary_Contact_Type__c\" = 'Student'\n                    OR C.\"ucinn_ascendv2__Is_Deceased__c\" = TRUE \n                    OR C.\"uff_UF_Disqualified__c\" = TRUE\n                )\n            )\n            \"\"\"\n        \n            # Score with exclusion criteria\n            # Score a test sample\n            test_where =\" 1=1\" \n            # change where clause to random if you want a sample\n            # \"RANDOM() < 0.01\"  # 1% sample for testing\n            results = scorer.score_dataset(\n                where_clause=test_where,\n                save_results=True, # Set to False to test without saving\n#                limit=LIMIT # 750000  # Explicitly set limit\n\n            )\n            \n            # Show distribution\n            print(\"\\nScore Distribution:\")\n            print(results['AFFINITY_GRADE'].value_counts(normalize=True))\n            \n        except Exception as e:\n            print(f\"Error in scoring: {str(e)}\")\n            traceback.print_exc()\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('model_scoring')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "896fec52-c5ef-42f2-b1f7-8a37389fe5dd",
   "metadata": {
    "language": "python",
    "name": "scoring_report"
   },
   "outputs": [],
   "source": " # Cell: scoring_report\nmonitor.start('scoring_report')\ntry:\n    class ScoringReporter:\n        def __init__(self, scoring_run_id: str, config: EnvironmentConfig):\n            self.scoring_run_id = scoring_run_id\n            self.db_manager = ScoringDatabaseManager(config)\n            self.metadata = None\n            self.load_metadata()\n\n        def load_metadata(self) -> None:\n            \"\"\"Load metadata for scoring run\"\"\"\n            query = f\"\"\"\n            SELECT *\n            FROM {TABLE_NAMES['METADATA']}\n            WHERE SCORING_RUN_ID = '{self.scoring_run_id}'\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'load_metadata',\n                lambda: self.db_manager.session.sql(query).collect()\n            ).to_pandas()\n            \n            if result.empty:\n                raise ValueError(f\"No metadata found for run {self.scoring_run_id}\")\n            \n            self.metadata = result.iloc[0].to_dict()\n\n        def generate_distribution_analysis(self) -> pd.DataFrame:\n            \"\"\"Analyze score and grade distribution\"\"\"\n            query = f\"\"\"\n            SELECT \n                AFFINITY_GRADE,\n                COUNT(*) as COUNT,\n                COUNT(*) / SUM(COUNT(*)) OVER () as PERCENTAGE,\n                MIN(AFFINITY_SCORE) as MIN_SCORE,\n                MAX(AFFINITY_SCORE) as MAX_SCORE,\n                AVG(AFFINITY_SCORE) as AVG_SCORE,\n                PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY AFFINITY_SCORE) as Q1,\n                PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY AFFINITY_SCORE) as MEDIAN,\n                PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY AFFINITY_SCORE) as Q3\n            FROM {TABLE_NAMES['SCORES']}_{self.scoring_run_id}\n            GROUP BY AFFINITY_GRADE\n            ORDER BY AFFINITY_GRADE\n            \"\"\"\n            return self.db_manager.execute_with_retry(\n                'distribution_analysis',\n                lambda: self.db_manager.session.sql(query).collect()\n            ).to_pandas()\n\n        def generate_feature_importance_summary(self) -> pd.DataFrame:\n            \"\"\"Summarize feature importance for the model used\"\"\"\n            query = f\"\"\"\n            SELECT \n                FEATURE_NAME,\n                IMPORTANCE,\n                RANK() OVER (ORDER BY IMPORTANCE DESC) as IMPORTANCE_RANK\n            FROM {TABLE_NAMES['FEATURES']}\n            WHERE MODEL_NAME = '{self.metadata['MODEL_NAME']}'\n            AND FEATURE_SAMPLE = '{self.metadata['FEATURE_SAMPLE']}'\n            ORDER BY IMPORTANCE DESC\n            LIMIT 20\n            \"\"\"\n            return self.db_manager.execute_with_retry(\n                'feature_importance',\n                lambda: self.db_manager.session.sql(query).collect()\n            ).to_pandas()\n\n        def plot_distribution(self, dist_data: pd.DataFrame) -> None:\n            \"\"\"Plot score distribution\"\"\"\n            plt.figure(figsize=(15, 10))\n            \n            # Create subplots\n            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n            \n            # Plot 1: Grade Distribution\n            sns.barplot(\n                data=dist_data,\n                x='AFFINITY_GRADE',\n                y='PERCENTAGE',\n                ax=ax1\n            )\n            ax1.set_title('Grade Distribution')\n            ax1.set_ylabel('Percentage')\n            \n            # Add percentage labels\n            for i, v in enumerate(dist_data['PERCENTAGE']):\n                ax1.text(i, v, f'{v:.1%}', ha='center', va='bottom')\n            \n            # Plot 2: Box Plot of Scores by Grade\n            sns.boxplot(\n                data=dist_data,\n                x='AFFINITY_GRADE',\n                y='AVG_SCORE',\n                ax=ax2\n            )\n            ax2.set_title('Score Distribution by Grade')\n            ax2.set_ylabel('Score')\n            \n            plt.tight_layout()\n            plt.show()\n\n        def generate_report(self) -> None:\n            \"\"\"Generate comprehensive scoring report\"\"\"\n            monitor.start('generate_report')\n            try:\n                print(\"\\nAffinity Score Report\")\n                print(\"=\" * 50)\n                \n                # Metadata\n                print(\"\\nScoring Run Information:\")\n                print(f\"Run ID: {self.scoring_run_id}\")\n                print(f\"Model: {self.metadata['MODEL_NAME']}\")\n                print(f\"Feature Sample: {self.metadata['FEATURE_SAMPLE']}\")\n                print(f\"Features Used: {self.metadata['N_FEATURES']}\")\n                print(f\"Records Scored: {self.metadata['RECORDS_SCORED']:,}\")\n                print(f\"Model Performance (F1): {self.metadata['F1_SCORE']:.4f}\")\n                print(f\"Model Performance (ROC-AUC): {self.metadata['ROC_AUC']:.4f}\")\n                \n                # Distribution Analysis\n                print(\"\\nScore Distribution Analysis:\")\n                dist_data = self.generate_distribution_analysis()\n                print(\"\\nGrade Distribution:\")\n                print(\"-\" * 80)\n                print(dist_data[['AFFINITY_GRADE', 'COUNT', 'PERCENTAGE', \n                               'MIN_SCORE', 'MAX_SCORE', 'AVG_SCORE']].to_string(index=False))\n                \n                # Plot distributions\n                self.plot_distribution(dist_data)\n                \n                # Feature Importance\n                print(\"\\nTop Feature Importance:\")\n                print(\"-\" * 80)\n                feat_imp = self.generate_feature_importance_summary()\n                print(feat_imp[['FEATURE_NAME', 'IMPORTANCE', \n                              'IMPORTANCE_RANK']].to_string(index=False))\n                \n                # Plot feature importance\n                plt.figure(figsize=(12, 6))\n                sns.barplot(\n                    data=feat_imp.head(10),\n                    x='IMPORTANCE',\n                    y='FEATURE_NAME'\n                )\n                plt.title('Top 10 Feature Importance')\n                plt.tight_layout()\n                plt.show()\n                \n            finally:\n                monitor.end()\n\n    # Test reporting\n#    if __name__ == \"__main__\":\n# \n#        try:\n#            config = EnvironmentConfig()\n#            session = get_active_session()\n#            config.set_session(session)\n#            \n#            # Initialize scorer\n#            scorer = AffinityScorer(config)\n#            \n#            # Score a test sample\n#            test_where = \"RANDOM() < 0.01\"  # 1% sample for testing\n#            results = scorer.score_dataset(\n#                where_clause=test_where,\n#                save_results=False,  # Set to False to test without saving\n#                limit=150000  # Add limit for testing\n#            )\n#            \n#            # Show distribution\n#            print(\"\\nScore Distribution:\")\n#            print(results['AFFINITY_GRADE'].value_counts(normalize=True))\n#            \n#        except Exception as e:\n#            print(f\"Error in scoring: {str(e)}\")\n#            traceback.print_exc()\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('scoring_report')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "663b1021-a90a-41d1-8078-408306425352",
   "metadata": {
    "language": "python",
    "name": "score_validation"
   },
   "outputs": [],
   "source": " # Cell: score_validation\nmonitor.start('score_validation')\ntry:\n    class ScoreValidator:\n        def __init__(self, scoring_run_id: str, config: EnvironmentConfig):\n            self.scoring_run_id = scoring_run_id\n            self.db_manager = ScoringDatabaseManager(config)\n            self.validation_results = {}\n            \n            # Ensure tables exist\n            self.db_manager.ensure_scoring_tables()\n\n        def validate_score_counts(self) -> bool:\n            \"\"\"Validate minimum number of scores generated\"\"\"\n            query = f\"\"\"\n            SELECT COUNT(*) as SCORE_COUNT\n            FROM {TABLE_NAMES['SCORES']}\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'count_scores',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            score_count = result['SCORE_COUNT'].iloc[0]\n            is_valid = score_count >= VALIDATION_THRESHOLDS['min_score_count']\n            \n            self.validation_results['score_count'] = {\n                'status': 'PASS' if is_valid else 'FAIL',\n                'actual': score_count,\n                'threshold': VALIDATION_THRESHOLDS['min_score_count']\n            }\n            \n            return is_valid\n\n        def validate_null_values(self) -> bool:\n            \"\"\"Validate percentage of null values\"\"\"\n            query = f\"\"\"\n            SELECT \n                COUNT(*) as TOTAL_RECORDS,\n                COUNT(*) - COUNT(AFFINITY_SCORE) as NULL_SCORES,\n                COUNT(*) - COUNT(AFFINITY_GRADE) as NULL_GRADES\n            FROM {TABLE_NAMES['SCORES']}\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'check_nulls',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            row = result.iloc[0]\n            null_pct = max(\n                row['NULL_SCORES'] / row['TOTAL_RECORDS'],\n                row['NULL_GRADES'] / row['TOTAL_RECORDS']\n            )\n            \n            is_valid = null_pct <= VALIDATION_THRESHOLDS['max_null_percentage']\n            \n            self.validation_results['null_values'] = {\n                'status': 'PASS' if is_valid else 'FAIL',\n                'actual': null_pct,\n                'threshold': VALIDATION_THRESHOLDS['max_null_percentage']\n            }\n            \n            return is_valid\n\n        def validate_grade_distribution(self) -> bool:\n            \"\"\"Validate grade distribution is within expected bounds\"\"\"\n            query = f\"\"\"\n            SELECT \n                AFFINITY_GRADE,\n                COUNT(*) / SUM(COUNT(*)) OVER () as grade_pct\n            FROM {TABLE_NAMES['SCORES']}\n            GROUP BY AFFINITY_GRADE\n            ORDER BY AFFINITY_GRADE\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'grade_distribution',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            grade_distribution = dict(zip(result['AFFINITY_GRADE'], result['GRADE_PCT']))\n            all_valid = True\n            \n            self.validation_results['grade_distribution'] = {}\n            \n            for grade, (min_pct, max_pct) in VALIDATION_THRESHOLDS['grade_distribution_bounds'].items():\n                actual_pct = grade_distribution.get(grade, 0)\n                is_valid = min_pct <= actual_pct <= max_pct\n                \n                self.validation_results['grade_distribution'][grade] = {\n                    'status': 'PASS' if is_valid else 'FAIL',\n                    'actual': actual_pct,\n                    'bounds': (min_pct, max_pct)\n                }\n                \n                all_valid &= is_valid\n            \n            return all_valid\n\n        def validate_metadata(self) -> bool:\n            \"\"\"Validate metadata was properly saved\"\"\"\n            query = f\"\"\"\n            SELECT *\n            FROM {TABLE_NAMES['METADATA']}\n            WHERE SCORING_RUN_ID = '{self.scoring_run_id}'\n            \"\"\"\n            result = self.db_manager.execute_with_retry(\n                'check_metadata',\n                lambda: self.db_manager.session.sql(query).to_pandas()\n            )\n            \n            is_valid = not result.empty\n            \n            self.validation_results['metadata'] = {\n                'status': 'PASS' if is_valid else 'FAIL',\n                'found': not result.empty\n            }\n            \n            return is_valid\n\n        def run_all_validations(self) -> bool:\n            \"\"\"Run all validations and return overall status\"\"\"\n            monitor.start('run_validations')\n            try:\n                validations = [\n                    ('Score Counts', self.validate_score_counts()),\n                    ('Null Values', self.validate_null_values()),\n#                    ('Grade Distribution', self.validate_grade_distribution()),\n                    ('Metadata', self.validate_metadata())\n                ]\n                \n                all_passed = all(result for _, result in validations)\n                \n                # Print validation results\n                print(\"\\nValidation Results:\")\n                print(\"=\" * 50)\n                \n                for name, result in validations:\n                    status = \"✓\" if result else \"✗\"\n                    print(f\"{status} {name}\")\n                \n                if 'score_count' in self.validation_results:\n                    print(f\"\\nScore Count: {self.validation_results['score_count']['actual']:,} \"\n                          f\"(min: {self.validation_results['score_count']['threshold']:,})\")\n                \n                if 'null_values' in self.validation_results:\n                    print(f\"Null Percentage: {self.validation_results['null_values']['actual']:.2%} \"\n                          f\"(max: {self.validation_results['null_values']['threshold']:.2%})\")\n                \n                return all_passed\n            \n            finally:\n                monitor.end()\n\n    # Test validation\n    if __name__ == \"__main__\":\n        try:\n            config = EnvironmentConfig()\n            session = get_active_session()\n            config.set_session(session)\n            \n            # Get latest scoring run\n            query = f\"\"\"\n            SELECT SCORING_RUN_ID\n            FROM {TABLE_NAMES['METADATA']}\n            ORDER BY SCORING_START DESC\n            LIMIT 1\n            \"\"\"\n            \n            result = ScoringDatabaseManager(config).execute_with_retry(\n                'get_latest_run',\n                lambda: session.sql(query).to_pandas())\n            \n            if not result.empty:\n                scoring_run_id = result['SCORING_RUN_ID'].iloc[0]\n                validator = ScoreValidator(scoring_run_id, config)\n                all_passed = validator.run_all_validations()\n                \n                print(f\"\\nOverall Validation Status: {'PASS' if all_passed else 'FAIL'}\")\n            else:\n                print(\"No scoring runs found to validate\")\n            \n        except Exception as e:\n            print(f\"Error in validation: {str(e)}\")\n            traceback.print_exc()\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('score_validation')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01d8b1d4-a1c3-4e31-90ec-433eabdbe38c",
   "metadata": {
    "name": "empty",
    "collapsed": false
   },
   "source": ""
  }
 ]
}