{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "fycq7tmojtnflnkwrag7",
   "authorId": "33929106622",
   "authorName": "TSY@UFL.EDU",
   "authorEmail": "tsy@ufl.edu",
   "sessionId": "588c288e-b131-4331-921d-39e460f27084",
   "lastEditTime": 1751996558050
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5efe79-cd23-40ed-ab97-b48f6725bf36",
   "metadata": {
    "name": "mermaid",
    "collapsed": false
   },
   "source": "## Proposed Improvements  \nCurrent approach (potentially circular)  \nsnowflake_utils -> depends on -> step1  \nstep1 -> depends on -> snowflake_utils\n\n# More modular approach\nsnowflake_utils -> depends on -> core_utilities\nstep1 -> depends on -> core_utilities\n\n--------------------------------------\n\nprogramtic change for testing vs production\nUsing a configuration file (YAML/JSON)  \nSetting up environment variables  \nCreating a config class that can be initialized with different values for testing vs. production  \n\n---------------------------------------------------\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "4ad96de0-f689-4ca9-ac20-c4d3adc49cfd",
   "metadata": {
    "language": "python",
    "name": "dependency_checker"
   },
   "outputs": [],
   "source": "# Cell: dependency_checker\n\"\"\"\nDependencies: None (root cell)\nProvides: Dependency checking functionality\n\"\"\"\nimport logging\nfrom typing import Dict, Set, Any, List\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass DependencyChecker:\n    def __init__(self):\n        self.loaded_cells = set()\n        self.load_times = {}\n        # In dependency_checker cell, update the dependency_map:\n        self.dependency_map = {\n            'dependency_checker': set(),  # No dependencies\n            'notebook_monitor': {'dependency_checker'},\n            'imports': {'dependency_checker', 'notebook_monitor'},\n            'global_constants': {'imports'},\n            'step1': {'global_constants', 'imports', 'notebook_monitor'},\n            'snowflake_utils': {'imports', 'step1', 'notebook_monitor'},\n            'data_prep_normalization': {'imports', 'snowflake_utils', 'global_constants', 'notebook_monitor'},\n            'feature_engineering': {'imports', 'data_prep_normalization', 'global_constants', 'notebook_monitor'},\n            'model_training': {'imports', 'feature_engineering', 'global_constants', 'notebook_monitor'},\n            'storage_utils': {'imports', 'snowflake_utils', 'global_constants', 'notebook_monitor'},\n#            'final_evaluation_reporting': {'imports', 'model_training', 'global_constants', 'notebook_monitor', 'storage_utils'},\n             'model_evaluator_class': {'imports', 'model_training', 'global_constants', 'notebook_monitor'},\n#            'model_evaluator_methods': {'model_evaluator_class', 'storage_utils', 'notebook_monitor'},\n            'model_evaluation_test': {'model_evaluator_class', 'notebook_monitor'}, # {'model_evaluator_methods', 'notebook_monitor'},\n            'monitor_output': {'notebook_monitor', 'model_evaluation_test'}\n        }\n\n    def verify_execution_order(self, order: List[str]) -> bool:\n        \"\"\"Verify if a given execution order is valid\"\"\"\n        executed = set()\n        print(\"\\nVerifying execution order:\")\n        print(\"=\" * 50)\n        \n        for cell in order:\n            # Check if cell exists in dependency map\n            if cell not in self.dependency_map:\n                print(f\"❌ Unknown cell: {cell}\")\n                return False\n            \n            # Check if all dependencies are executed\n            missing_deps = self.dependency_map[cell] - executed\n            if missing_deps:\n                print(f\"❌ {cell}: Missing dependencies: {missing_deps}\")\n                return False\n            \n            print(f\"✓ {cell}: All dependencies met\")\n            executed.add(cell)\n        \n        return True\n    \n    def show_cell_dependencies(self):\n        \"\"\"Show dependencies for each cell\"\"\"\n        print(\"\\nCell Dependencies:\")\n        print(\"=\" * 50)\n        for cell in self.dependency_map:\n            deps = self.dependency_map[cell]\n            print(f\"\\n{cell}:\")\n            if deps:\n                for dep in sorted(deps):\n                    print(f\"  ├── {dep}\")\n            else:\n                print(\"  └── No dependencies\")\n    \n   \n    def register_cell(self, cell_name: str):\n        \"\"\"Register a cell as loaded\"\"\"\n        self.loaded_cells.add(cell_name)\n        self.load_times[cell_name] = datetime.now()\n        logger.info(f\"Registered cell: {cell_name}\")\n        print(f\"Registered cell: {cell_name}\")  # Additional console output\n\n    def check_dependencies(self, cell_name: str) -> bool:\n        \"\"\"Check if all dependencies for a cell are loaded\"\"\"\n        if cell_name not in self.dependency_map:\n            logger.warning(f\"Unknown cell: {cell_name}\")\n            return False\n            \n        missing = self.dependency_map[cell_name] - self.loaded_cells\n        if missing:\n            logger.error(f\"Missing dependencies for {cell_name}: {missing}\")\n            return False\n        return True\n\n    def get_cell_status(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get status of all cells and their dependencies\"\"\"\n        status = {\n            cell: {\n                'loaded': cell in self.loaded_cells,\n                'dependencies_met': self.check_dependencies(cell),\n                'missing_dependencies': list(self.dependency_map[cell] - self.loaded_cells),\n                'load_time': self.load_times.get(cell, None)\n            }\n            for cell in self.dependency_map\n        }\n        \n        # Log the overall status\n        logger.info(\"\\nCell Status Summary:\")\n        for cell, info in status.items():\n            load_status = '✓' if info['loaded'] else '✗'\n            dep_status = '✓' if info['dependencies_met'] else '✗'\n            load_time = info['load_time'].strftime('%H:%M:%S') if info['load_time'] else 'Not loaded'\n            logger.info(f\"{cell}: {load_status} Loaded ({load_time}), Dependencies: {dep_status}\")\n        \n        return status\n\n    def show_dependency_tree(self):\n        \"\"\"Display the dependency tree in a readable format\"\"\"\n        print(\"\\nDependency Tree:\")\n        print(\"=\" * 50)\n        for cell in self.dependency_map:\n            deps = self.dependency_map[cell]\n            status = \"✓\" if self.check_dependencies(cell) else \"✗\"\n            loaded = \"✓\" if cell in self.loaded_cells else \"✗\"\n            print(f\"{status} {cell} [{loaded}]\")\n            if deps:\n                for dep in deps:\n                    dep_loaded = \"✓\" if dep in self.loaded_cells else \"✗\"\n                    print(f\"  ├── {dep_loaded} {dep}\")\n            print(\"  │\")\n\n    def validate_execution_order(self):\n        \"\"\"Validate that cells were executed in the correct order\"\"\"\n        print(\"\\nExecution Order Validation:\")\n        print(\"=\" * 50)\n        \n        # Check if any cells were loaded before their dependencies\n        for cell in self.loaded_cells:\n            if cell in self.load_times:\n                cell_time = self.load_times[cell]\n                deps = self.dependency_map.get(cell, set())\n                \n                for dep in deps:\n                    if dep in self.load_times:\n                        dep_time = self.load_times[dep]\n                        if dep_time > cell_time:\n                            print(f\"Warning: {cell} was loaded before its dependency {dep}\")\n# Initialize the dependency checker\ndep_checker = DependencyChecker()\ndep_checker.register_cell('dependency_checker')\n\n\n# At the bottom of dependency_checker cell, add:\nexecution_order = [\n    'dependency_checker',\n    'notebook_monitor',\n    'imports',\n    'global_constants',\n    'step1',\n    'snowflake_utils',\n    'data_prep_normalization',\n    'feature_engineering',\n    'model_training',\n    'storage_utils',\n    'model_evaluator_class',\n#    'model_evaluator_methods',\n    'model_evaluation_test',\n    'monitor_output'\n]\n\nprint(\"\\nVerifying proposed execution order...\")\ndep_checker.verify_execution_order(execution_order)\nprint(\"\\nShowing all cell dependencies...\")\ndep_checker.show_cell_dependencies()\n\n\n# Show initial status\nlogger.info(\"Dependency checker initialized\")\ndep_checker.show_dependency_tree()\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d10895e6-fe63-4ef8-99fb-04984d13c0a6",
   "metadata": {
    "language": "python",
    "name": "notebook_monitor"
   },
   "outputs": [],
   "source": "# Cell: notebook_monitor\n\"\"\"\nDependencies: dependency_checker\nProvides: Enhanced monitoring with memory usage and operation counts\n\"\"\"\nimport time\nimport pandas as pd\nimport psutil\nimport os\nfrom datetime import datetime\nfrom typing import Optional, Dict, List, Any\n\nclass CellMonitor:\n    def __init__(self):\n        if not dep_checker.check_dependencies('notebook_monitor'):\n            print(\"Warning: Dependencies not met for notebook monitor\")\n            \n        self.execution_logs = []\n        self.current_cell = None\n        self.start_time = None\n        self.end_time = None\n        self.operation_counts = {}\n        self.memory_snapshots = {}\n        self.context = []  # Add context list\n        pd.set_option('display.max_rows', None)\n        pd.set_option('display.max_columns', None)\n\n    def add_context(self, context: str) -> None:\n        \"\"\"Add context to the current monitoring\"\"\"\n        self.context.append(context)\n        \n    def remove_context(self) -> None:\n        \"\"\"Remove the most recent context\"\"\"\n        if self.context:\n            self.context.pop()\n\n    def _get_memory_usage(self):\n        \"\"\"Get current memory usage in MB\"\"\"\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss / 1024 / 1024\n\n    def start(self, cell_name: str) -> None:\n        \"\"\"Start monitoring a cell execution\"\"\"\n        # Add context if it exists\n        if self.context:\n            cell_name = f\"{cell_name}_{'_'.join(self.context)}\"\n            \n        self.current_cell = {\n            'cell_name': cell_name,\n            'start_time': datetime.now(),\n            'status': 'running',\n            'duration_seconds': 0,\n            'start_memory_mb': self._get_memory_usage(),\n            'peak_memory_mb': 0,\n            'memory_change_mb': 0,\n            'operation_count': 0,\n            'operations': {},\n            'error': None\n        }\n        self.start_time = time.time()\n        self.operation_counts[cell_name] = {}\n\n    def end(self) -> None:\n        \"\"\"End monitoring current cell execution\"\"\"\n        if self.current_cell:\n            self.end_time = time.time()\n            duration = self.end_time - self.start_time\n            end_memory = self._get_memory_usage()\n            \n            self.current_cell.update({\n                'end_time': datetime.now(),\n                'status': 'completed',\n                'duration_seconds': round(duration, 2),\n                'end_memory_mb': end_memory,\n                'memory_change_mb': round(end_memory - self.current_cell['start_memory_mb'], 2)\n            })\n            \n            self.execution_logs.append(self.current_cell.copy())\n            self.current_cell = None\n\n    def show_summary(self) -> pd.DataFrame:\n        \"\"\"Show execution summary\"\"\"\n        print(f\"DEBUG: Number of execution logs: {len(self.execution_logs)}\")\n        \n        if not self.execution_logs:\n            return pd.DataFrame()\n    \n        try:\n            # Create DataFrame from logs\n            df = pd.DataFrame(self.execution_logs)\n            \n            # Format timestamps\n            if 'start_time' in df.columns:\n                df['start_time'] = pd.to_datetime(df['start_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n            if 'end_time' in df.columns:\n                df['end_time'] = pd.to_datetime(df['end_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n            \n            return df\n            \n        except Exception as e:\n            print(f\"Error generating summary: {type(e).__name__} - {str(e)}\")\n            print(traceback.format_exc())\n            return pd.DataFrame()\n\n    def log_error(self, error: Exception) -> None:\n        \"\"\"Log an error for the current cell\"\"\"\n        if self.current_cell:\n            self.current_cell.update({\n                'status': 'error',\n                'error': f\"{type(error).__name__}: {str(error)}\",\n                'end_memory_mb': self._get_memory_usage()\n            })\n            self.end()\n\n# Initialize monitor\nmonitor = CellMonitor()\ndep_checker.register_cell('notebook_monitor')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d182aa46-bdab-4352-8d5e-a3418f649332",
   "metadata": {
    "language": "python",
    "name": "imports",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Cell: imports\nmonitor.start('imports')\ntry:\n    \"\"\"\n    Dependencies: dependency_checker, notebook_monitor\n    Provides: All required package imports for the notebook\n    \"\"\"\n    # Standard libraries\n    import logging\n    from typing import Dict, Set, Any, List, Optional, Tuple\n    from datetime import datetime\n    import time\n    import traceback\n    import json\n    import gc\n    import pickle\n    import base64   # Add this import\n    import joblib\n    import io\n    \n    # Snowpark\n    from snowflake.snowpark.functions import col, when\n    import snowflake.snowpark.functions as F\n    \n    # Scientific computing\n    import pandas as pd\n    import numpy as np\n    \n    # Scikit-learn\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.feature_selection import SelectFromModel\n    from sklearn.feature_selection import SelectKBest, f_classif  # Add this line with the other sklearn imports\n    from sklearn.model_selection import train_test_split, StratifiedKFold\n    from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                               f1_score, roc_auc_score, balanced_accuracy_score,matthews_corrcoef, \n                                precision_recall_curve, auc)  \n    \n    # Models\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.svm import SVC\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.pipeline import make_pipeline\n    from sklearn.model_selection import validation_curve, cross_val_score\n    from sklearn.metrics import classification_report\n\n\n    \n    # Visualization\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    # Configure logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('imports')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f86318e0-bbdc-4098-a7cb-189b7b45b23e",
   "metadata": {
    "language": "python",
    "name": "global_constants",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Cell: global_constants\nmonitor.start('global_constants')\ntry:\n    \"\"\"\n    Dependencies: imports\n    Provides: Global constants for the entire notebook\n    \"\"\"\n\n    # Run configuration\n    RUN_MODE = 'test'  # Options: 'TEST', 'FULL'\n\n    # Department and Model Configuration for prefix to keep track of which unit and what is being modeled\n    DEPARTMENT = 'UF'\n    MODEL_TYPE = 'MAJOR_GIFT'\n \n \n    # Database configuration\n    SCHEMA = \"UFF_MODEL\"\n    SOURCE_TABLE = \"UF_MAJOR_GIFT_VIEWS_COMBO\"\n\n    \n    # Data processing parameters\n    SAMPLE_SIZE = 250000 #75000 47500  - ok 125000 -maybe 300000(too big)  # Number of records to process\n    CHUNK_SIZE =25000 # 25000 10000   # Size of processing chunks\n        # Feature sampling for FULL mode\n    # These will be used to determine how many variables to include\n    FEATURE_SAMPLES = {\n        'third': 1/3,    # Use 1/3 of available features\n        'half': 1/2,     # Use 1/2 of available features\n        'full': 1.0      # Use all features\n    }\n\n  # model names\n    MODEL_NAMES = ['LogisticRegression', 'RandomForest', 'SVC', 'DecisionTree', 'MLPClassifier']\n\n    # Test mode sample size\n#    TEST_SAMPLE_SIZE = 1000\n \n    # Model parameters\n    N_SPLITS = 5        # Number of cross-validation folds\n    RANDOM_SEED = 42    # For reproducibility\n    \n    # Target columns\n    TARGET_COLS = ['COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n\n\n        # Oversampling configuration\n    ENABLE_OVERSAMPLING = False\n    OVERSAMPLING_STRATEGY = 'auto'  # Options: 'auto', 'minority', specific ratio\n    \n    # Table naming templates\n    def get_table_name(base_name: str, timestamp: Optional[str] = None) -> str:\n        \"\"\"Generate standardized table names\"\"\"\n        # Avoid double prefixing\n        if base_name.startswith(f\"{DEPARTMENT}_{MODEL_TYPE}_\"):\n            name = base_name\n        else:\n            name = f\"{DEPARTMENT}_{MODEL_TYPE}_{base_name}\"\n        if timestamp:\n            name = f\"{name}_{timestamp}\"\n        return name    \n        # Define standard table names\n#    TABLE_NAMES = {\n#        'results': get_table_name('RESULTS'),\n#        'features': get_table_name('FEATURES'),\n#        'models': get_table_name('MODELS')\n#    }\n\n    \n# Should be:\n    TABLE_NAMES = {\n        'results': get_table_name('MODEL_PERFORMANCE'),    # This becomes UF_MAJOR_GIFT_MODEL_PERFORMANCE\n        'features': get_table_name('FEATURE_IMPORTANCE'),  # This becomes UF_MAJOR_GIFT_FEATURE_IMPORTANCE\n        'models': get_table_name('MODELS') ,               # This becomes UF_MAJOR_GIFT_MODELS\n        'imputed': get_table_name('IMPUTED')  # This becomes UF_MAJOR_GIFT_IMPUTED\n    }\n# Updated code - just to ensure consistency\n#{\n#       'model_performance': get_table_name('MODEL_PERFORMANCE'),  # This becomes UF_MAJOR_GIFT_MODEL_PERFORMANCE\n#       'feature_importance': get_table_name('FEATURE_IMPORTANCE'),  # This becomes UF_MAJOR_GIFT_FEATURE_IMPORTANCE\n#       'models': get_table_name('MODELS'),  # This becomes UF_MAJOR_GIFT_MODELS\n#       'imputed': get_table_name('IMPUTED')  # This becomes UF_MAJOR_GIFT_IMPUTED\n#    }\n\n    \n    # Function to get imputed table name with timestamp\n#    def get_imputed_table_name(timestamp: Optional[str] = None) -> str:\n#        \"\"\"Get imputed table name with optional timestamp\"\"\"\n#        return get_table_name('IMPUTED', timestamp)\n#        #'imputed':  get_table_name('IMPUTED', timestamp=None)\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('global_constants')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "step1",
    "codeCollapsed": false
   },
   "source": "# Cell: step1\nmonitor.start('step1')\ntry:\n    class EnvironmentConfig:\n        # Static configurations that don't change between environments\n        SCHEMA = SCHEMA # Use global constant\n        TABLES = {\n            'major_gift': SOURCE_TABLE, # Use global constant\n#            'imputed': 'UF_MAJOR_GIFT_VIEWS_IMPUTED'\n#             'imputed':TABLE_NAMES['imputed']# ()\n             'imputed': get_table_name('IMPUTED')  # Use the function instead of TABLE_NAMES reference\n\n        }\n        \n        # Environment configurations\n        ENVIRONMENTS = {\n            'test': {\n                'database': 'PRE_PRODUCTION',\n                'schema': SCHEMA,\n                'warehouse': 'ANALYSIS'\n            },\n            'prod': {\n                'database': 'PRODUCTION',\n                'schema': SCHEMA,\n                'warehouse': 'ANALYSIS'\n            }\n        }\n        \n        def __init__(self):\n            self.current_env = 'test'  # default to test environment\n            self.session = None\n        \n        def set_session(self, session):\n            \"\"\"Set the Snowflake session\"\"\"\n            self.session = session\n            self._apply_environment()\n        \n        def switch_environment(self, env_name):\n            \"\"\"Switch between environments\"\"\"\n            if not self.session:\n                raise ValueError(\"Session not initialized. Call set_session first.\")\n                \n            if env_name not in self.ENVIRONMENTS:\n                raise ValueError(f\"Invalid environment. Use one of: {list(self.ENVIRONMENTS.keys())}\")\n            \n            self.current_env = env_name\n            self._apply_environment()\n            self.show_environment_status()\n        \n        def _apply_environment(self):\n            \"\"\"Apply environment settings to Snowflake session\"\"\"\n            if not self.session:\n                raise ValueError(\"Session not initialized\")\n                \n            env = self.ENVIRONMENTS[self.current_env]\n            try:\n                self.session.sql(f\"USE DATABASE {env['database']}\").collect()\n                self.session.sql(f\"USE SCHEMA {env['schema']}\").collect()\n                self.session.sql(f\"USE WAREHOUSE {env['warehouse']}\").collect()\n            except Exception as e:\n                print(f\"Error setting environment: {str(e)}\")\n                raise\n\n        def get_full_table_name(self, table_key: str) -> str:\n            \"\"\"Get fully qualified table name: database.schema.table\"\"\"\n            env = self.ENVIRONMENTS[self.current_env]\n            table_name = self.TABLES.get(table_key)\n            if not table_name:\n                raise ValueError(f\"Invalid table key. Use one of: {list(self.TABLES.keys())}\")\n            return f\"{env['database']}.{env['schema']}.{table_name}\"\n        \n        def get_schema_db_name(self) -> str:\n            \"\"\"Get database.schema\"\"\"\n            env = self.ENVIRONMENTS[self.current_env]\n            return f\"{env['database']}.{env['schema']}\"\n        \n        def get_temp_table_name(self, base_name=None, timestamp=None):\n            \"\"\"\n            Generate a temporary table name with timestamp\n            Args:\n                base_name: Optional base name (defaults to 'imputed' table)\n                timestamp: Optional timestamp (will generate current timestamp if None)\n            \"\"\"\n            from datetime import datetime\n            \n            if base_name is None:\n                base_name = self.TABLES['imputed']\n                \n            if timestamp is None:\n                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                \n            temp_name = f\"{base_name}_{timestamp}\"\n            return {\n                'table_name': temp_name,\n                'full_name': f\"{self.get_schema_db_name()}.{temp_name}\",\n                'timestamp': timestamp\n                 }\n        \n        def show_environment_status(self):\n            \"\"\"Display current environment settings\"\"\"\n            if not self.session:\n                raise ValueError(\"Session not initialized\")\n                \n            border = \"=\" * 60\n            print(border)\n            print(f\"{'ENVIRONMENT STATUS':^60}\")\n            print(border)\n            print(f\"ENVIRONMENT: {self.current_env.upper()}\")\n            \n            try:\n                current_settings = self.session.sql(\"\"\"\n                SELECT \n                    CURRENT_DATABASE() as database,\n                    CURRENT_SCHEMA() as schema,\n                    CURRENT_WAREHOUSE() as warehouse\n                \"\"\").collect()\n                \n                print(f\"\"\"\n                DATABASE:  {current_settings[0]['DATABASE']}\n                SCHEMA:    {current_settings[0]['SCHEMA']}\n                WAREHOUSE: {current_settings[0]['WAREHOUSE']}\n                \"\"\")\n                print(border)\n            except Exception as e:\n                print(f\"Error getting current settings: {str(e)}\")\n                raise\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('step1')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f1c78a5c-d21b-44b3-8e61-d02b8412d05a",
   "metadata": {
    "language": "python",
    "name": "snowflake_utils"
   },
   "outputs": [],
   "source": "# Cell: snowflake_utils\nmonitor.start('snowflake_utils')\ntry:\n    class SnowflakeManager:\n        def __init__(self, config: EnvironmentConfig):\n            self.config = config\n            self.session = config.session\n            self.max_retries = 3\n            self.retry_delay = 5  # seconds\n            self._error_counts = {}\n\n        def execute_with_retry(self, operation_name: str, operation, *args, **kwargs):\n            monitor.start(f'execute_{operation_name}')\n            try:\n                for attempt in range(self.max_retries):\n                    try:\n                        result = operation(*args, **kwargs)\n                        return result\n                    except Exception as e:\n                        self._track_error(operation_name, e)\n                        if attempt < self.max_retries - 1:\n                            print(f\"Error in {operation_name}, retrying... (Attempt {attempt + 1}/{self.max_retries})\")\n                            time.sleep(self.retry_delay * (attempt + 1))\n                            self.session = get_active_session()\n                            self.config._apply_environment()\n                        else:\n                            raise\n            except Exception as e:\n                print(f\"Error in {operation_name}: {type(e).__name__} - {str(e)}\")\n                print(traceback.format_exc())\n                raise\n            finally:\n                monitor.end()\n\n        def ensure_table_exists(self, table_type: str) -> None:\n            \"\"\"Ensure required tables exist with correct schema\"\"\"\n            monitor.start('ensure_table_exists')\n            try:\n                schemas = {\n                    'results': \"\"\"  \n                        CREATE TABLE IF NOT EXISTS {table_name} (\n                            TARGET VARCHAR,\n                            MODEL VARCHAR,\n                            FEATURE_SAMPLE VARCHAR,\n                            N_FEATURES NUMBER,\n                            FEATURE_RATIO FLOAT,\n                            SAMPLE_SIZE NUMBER,\n                            ACCURACY_MEAN FLOAT,\n                            ACCURACY_STD FLOAT,\n                            PRECISION_MEAN FLOAT,\n                            PRECISION_STD FLOAT,\n                            RECALL_MEAN FLOAT,\n                            RECALL_STD FLOAT,\n                            F1_MEAN FLOAT,\n                            F1_STD FLOAT,\n                            ROC_AUC_MEAN FLOAT,\n                            ROC_AUC_STD FLOAT,\n                            TIMESTAMP TIMESTAMP_NTZ\n                        )\n                    \"\"\",\n                    'models': \"\"\" \n                        CREATE TABLE IF NOT EXISTS {table_name} (\n                            MODEL VARCHAR,\n                            TARGET VARCHAR,\n                            FEATURE_SAMPLE VARCHAR,\n                            N_FEATURES NUMBER,\n                            MODEL_OBJECT VARCHAR,  -- Serialized model\n                            SELECTED_FEATURES VARCHAR,  -- Serialized feature list\n                            METRICS VARCHAR,  -- Serialized metrics\n                            SCALER VARCHAR,  -- Serialized scaler\n                            IMPUTERS VARCHAR,  -- Serialized imputers\n                            CREATED_AT TIMESTAMP_NTZ\n                        )\n                    \"\"\",\n                    'features': \"\"\" \n                        CREATE TABLE IF NOT EXISTS {table_name} (\n--                           MODEL VARCHAR, \n                            FEATURE_NAME VARCHAR,\n                            IMPORTANCE FLOAT,\n                            TARGET VARCHAR,\n                            SAMPLE_TYPE VARCHAR,\n                            CREATED_AT TIMESTAMP_NTZ\n                        )\n                    \"\"\"\n                }\n                \n                if table_type in schemas:\n                    table_name = TABLE_NAMES[table_type]\n                    full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                    \n                    # Check if table exists\n                    exists_query = f\"\"\"\n                    SELECT 1 \n                    FROM INFORMATION_SCHEMA.TABLES \n                    WHERE TABLE_SCHEMA = '{self.config.SCHEMA}'\n                    AND TABLE_NAME = '{table_name}'\n                    \"\"\"\n                    \n                    result = self.execute_with_retry(\n                        'check_table_exists',\n                        lambda: self.session.sql(exists_query).collect()\n                    )\n                    \n                    if not result:\n                        create_query = schemas[table_type].format(table_name=full_name)\n                        self.execute_with_retry(\n                            'create_table',\n                            lambda: self.session.sql(create_query).collect()\n                        )\n                        print(f\"Created table: {full_name}\")\n                \n            finally:\n                monitor.end()\n\n        def load_table(self, table_key: str, sample_percent: Optional[float] = None,\n                      columns: Optional[list] = None, where_clause: Optional[str] = None) -> pd.DataFrame:\n            monitor.start('load_table')\n            try:\n                full_table_name = self.config.get_full_table_name(table_key)\n                cols_str = '*' if not columns else ', '.join(columns)\n                query = f'SELECT {cols_str} FROM {full_table_name}'\n                \n                if where_clause:\n                    query += f' WHERE {where_clause}'\n                if sample_percent is not None:\n                    query += f' SAMPLE({sample_percent})'\n\n                snowpark_df = self.execute_with_retry('load_table', \n                                                    lambda: self.session.sql(query))\n                return snowpark_df.to_pandas()\n            except Exception as e:\n                print(f\"Error loading table {table_key}: {type(e).__name__} - {str(e)}\")\n                print(traceback.format_exc())\n                raise\n            finally:\n                monitor.end()\n\n        def save_results(self, df: pd.DataFrame, base_name: str, \n                        mode: str = 'overwrite',\n                        timestamp: Optional[str] = None,\n                        partition_by: Optional[List[str]] = None, \n                        cluster_by: Optional[List[str]] = None) -> Dict[str, str]:\n            \"\"\"Save results to Snowflake\"\"\"\n            monitor.start('save_results')\n            try:\n                # Determine table type and ensure it exists\n                if base_name == 'MODEL_RESULTS':\n                    table_type = 'results'\n                    self.ensure_table_exists(table_type)\n                    table_name = TABLE_NAMES[table_type]\n                    mode = 'append'\n                elif base_name == 'IMPUTED':\n                    table_type = 'imputed'\n#                    self.ensure_table_exists(table_type)\n#                    if timestamp table_name = get_table_name('IMPUTED', timestamp)\n#                    else table_name = get_table_name('IMPUTED')\n#                    table_name = get_imputed_table_name(timestamp)\n                    table_name = get_table_name('IMPUTED', timestamp)\n                    mode = 'overwrite'\n                elif base_name == 'MODELS':\n                    table_type = 'models'\n                    self.ensure_table_exists(table_type)\n                    table_name = TABLE_NAMES[table_type]\n                    mode = 'append'\n                elif base_name == 'FEATURES':\n                    table_type = 'features'\n                    self.ensure_table_exists(table_type)\n                    table_name = TABLE_NAMES[table_type]\n                    mode = 'append'\n                elif base_name == 'results' or base_name == 'features' or base_name == 'models':\n                    # Handle direct table_type references\n                    table_type = base_name\n                    self.ensure_table_exists(table_type)\n                    table_name = TABLE_NAMES[table_type]\n                    mode = 'append'\n                    \n                else:\n                                        # If not a predefined table, use the naming function\n                    table_name = get_table_name(base_name)\n                \n                full_name = f\"{self.config.get_schema_db_name()}.{table_name}\"\n                print(f\"Saving to {full_name} with mode: {mode}\")\n                \n                snowpark_df = self.session.create_dataframe(df)\n                writer = snowpark_df.write\n\n                if partition_by:\n                    writer = writer.partition_by(partition_by)\n                if cluster_by:\n                    writer = writer.cluster_by(cluster_by)\n\n                writer.save_as_table(full_name, mode=mode)\n                \n                return {\n                    'table_name': table_name,\n                    'full_name': full_name,\n                    'timestamp': timestamp\n                }\n            \n            except Exception as e:\n                print(f\"Error saving results: {type(e).__name__} - {str(e)}\")\n                print(traceback.format_exc())\n                raise\n            finally:\n                monitor.end()\n\n        def verify_tables(self) -> pd.DataFrame:\n            \"\"\"Verify all required tables exist with correct structure\"\"\"\n            monitor.start('verify_tables')\n            try:\n                queries = {\n                    'existence': f\"\"\"\n                        SELECT TABLE_NAME, TABLE_SCHEMA\n                        FROM INFORMATION_SCHEMA.TABLES \n                        WHERE TABLE_SCHEMA = '{self.config.SCHEMA}'\n                        AND TABLE_NAME LIKE '{DEPARTMENT}_{MODEL_TYPE}_%'\n                    \"\"\",\n                    'structure': f\"\"\"\n                        SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE, IS_NULLABLE\n                        FROM INFORMATION_SCHEMA.COLUMNS\n                        WHERE TABLE_SCHEMA = '{self.config.SCHEMA}'\n                        AND TABLE_NAME LIKE '{DEPARTMENT}_{MODEL_TYPE}_%'\n                        ORDER BY TABLE_NAME, ORDINAL_POSITION\n                    \"\"\"\n                }\n                \n                results = {}\n                for query_name, query in queries.items():\n                     # Execute query and convert result to DataFrame\n                    snowpark_df = self.execute_with_retry(\n                        f'verify_{query_name}',\n                        lambda: self.session.sql(query)\n                    ) \n                    results[query_name] = snowpark_df.to_pandas()\n                \n                return results\n                \n            finally:\n                monitor.end()\n\n        def _track_error(self, operation: str, error: Exception) -> None:\n            error_key = f\"{operation}:{type(error).__name__}\"\n            self._error_counts[error_key] = self._error_counts.get(error_key, 0) + 1\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('snowflake_utils')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "180319ca-fbe1-4173-aa2c-5c16b4ed2cb2",
   "metadata": {
    "language": "python",
    "name": "data_prep_normalization"
   },
   "outputs": [],
   "source": "# Cell: data_prep_normalization\nmonitor.start('data_prep_normalization')\ntry:\n    def load_and_prepare_data(config: EnvironmentConfig, \n                             sample_size: int = SAMPLE_SIZE,\n                             random_seed: int = RANDOM_SEED) -> pd.DataFrame:\n\n#        # Add feature selection to reduce overfitting\n#        if len(self.features) > 50:  # If too many features\n#            from sklearn.feature_selection import SelectKBest, f_classif\n#            selector = SelectKBest(score_func=f_classif, k=min(30, len(self.features)))\n#            feature_df_selected = selector.fit_transform(feature_df, target_for_selection)\n#            print(f\"Reduced features from {len(self.features)} to {feature_df_selected.shape[1]}\")\n        \n\n        \n        \"\"\"\n        Load specific sample size using Snowpark's capabilities with dynamic type handling\n        \"\"\"\n        monitor.start('load_and_prepare_data')\n        try:\n            sf_manager = SnowflakeManager(config)\n            \n            # Load data from Snowflake\n            query = f\"\"\"\n            SELECT *\n            FROM {sf_manager.config.get_full_table_name('major_gift')}\n            ORDER BY RANDOM({random_seed})\n            LIMIT {sample_size}\n            \"\"\"\n            \n            print(f\"Loading {sample_size} records...\")\n            snow_df = sf_manager.execute_with_retry(\n                'load_data',\n                lambda: sf_manager.session.sql(query)\n            )\n            \n            print(\"Converting to pandas...\")\n            df = snow_df.to_pandas()\n            \n            # Identify column types using pandas\n            print(\"\\nAnalyzing column data types:\")\n            numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n            categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n            \n            print(f\"Found {len(numeric_columns)} numeric columns\")\n            print(f\"Found {len(categorical_columns)} categorical columns\")\n            \n            # Process potential Y/N columns in categorical columns\n            for col in categorical_columns:\n                # Check for Y/N pattern in the column\n                unique_vals = set(str(x).upper() for x in df[col].dropna().unique() if str(x).strip())\n                is_yn = unique_vals and all(val in ['Y', 'N', 'YES', 'NO', 'TRUE', 'FALSE'] for val in unique_vals)\n                \n                if is_yn:\n                    print(f\"Converting Y/N column: {col}\")\n                    # Convert Y/N values to 1/0\n                    yn_mapping = {\n                        'Y': 1, 'YES': 1, 'TRUE': 1, 'y': 1, 'yes': 1, 'true': 1,\n                        'N': 0, 'NO': 0, 'FALSE': 0, 'n': 0, 'no': 0, 'false': 0\n                    }\n                    \n                    # Apply mapping\n                    df[col] = df[col].map(lambda x: yn_mapping.get(str(x).upper(), None) \n                                         if pd.notna(x) and str(x).strip() else None)\n                    \n            # After conversion, recheck column types\n            print(\"\\nUpdated column data types:\")\n            numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n            categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n            \n            print(f\"Now have {len(numeric_columns)} numeric columns\")\n            print(f\"Now have {len(categorical_columns)} categorical columns\")\n            \n            return df\n            \n        except Exception as e:\n            print(f\"Error in data loading: {str(e)}\")\n            print(traceback.format_exc())\n            raise\n        finally:\n            monitor.end()\n\n    def determine_imputation_strategy(df: pd.DataFrame) -> Dict[str, List[str]]:\n        \"\"\"\n        Determine appropriate imputation strategy for each column based on its characteristics\n        \"\"\"\n        column_groups = {\n            'mean': [],    # For financial and continuous data\n            'median': [],  # For skewed distributions\n            'constant': [] # For sparse or categorical-derived numerics\n        }\n        \n        # DEBUG_START - Column Analysis\n        print(\"\\nAnalyzing column characteristics:\")\n        print(\"=\" * 50)\n        # DEBUG_END\n        \n        for col in df.columns:\n            # Get non-null values for analysis\n            non_null = df[col].dropna()\n            null_pct = df[col].isnull().mean()\n            \n            # Handle completely null columns\n            if null_pct == 1.0:\n                column_groups['constant'].append(col)\n                continue\n                \n            # For columns with some data\n            if len(non_null) > 0:\n                unique_pct = len(non_null.unique()) / len(non_null)\n                try:\n                    skewness = non_null.skew()\n                except:\n                    skewness = 0  # Default for non-numeric or error cases\n                \n                # DEBUG_START - Individual Column Stats\n                print(f\"\\nColumn: {col}\")\n                print(f\"Null %: {null_pct:.2%}\")\n                print(f\"Unique %: {unique_pct:.2%}\")\n                print(f\"Skewness: {skewness:.2f}\")\n                # DEBUG_END\n                \n                # Binary/Boolean derived columns go to constant\n                if len(non_null.unique()) <= 2:\n                    column_groups['constant'].append(col)\n                # High nulls or few uniques go to constant\n                elif null_pct > 0.9 or unique_pct < 0.01:\n                    column_groups['constant'].append(col)\n                # Highly skewed data goes to median\n                elif abs(skewness) > 2:\n                    column_groups['median'].append(col)\n                # Everything else goes to mean\n                else:\n                    column_groups['mean'].append(col)\n            else:\n                column_groups['constant'].append(col)\n\n        # VALIDATION_START - Column Assignment\n        print(\"\\nValidating column assignments:\")\n        print(\"=\" * 50)\n        all_assigned = set([col for group in column_groups.values() for col in group])\n        all_columns = set(df.columns)\n        unassigned = all_columns - all_assigned\n        if unassigned:\n            print(f\"Warning: Adding unassigned columns to constant imputation: {unassigned}\")\n            column_groups['constant'].extend(unassigned)\n        # VALIDATION_END\n\n        return column_groups\n\n    def impute_and_normalize(df: pd.DataFrame, \n                            config: EnvironmentConfig, \n                            chunk_size: int = CHUNK_SIZE,\n                            save_imputed: bool = True,\n                            timestamp: Optional[str] = None) -> Tuple[pd.DataFrame, StandardScaler, \n                                                             Dict[str, SimpleImputer],\n                                                             Dict[str, List[str]]]:\n        \"\"\"\n        Impute missing values and normalize data in chunks\n        Returns:\n            - Normalized DataFrame\n            - Fitted StandardScaler\n            - Dictionary of fitted imputers\n            - Dictionary of imputation strategies\n        \"\"\"\n        monitor.start('impute_and_normalize')\n        try:\n            # Get numeric columns\n            numeric_df = df.copy()\n            \n            # DEBUG_START - Initial Data Stats\n            print(f\"\\nInitial shape: {numeric_df.shape}\")\n            print(\"\\nMissing value summary:\")\n            print(numeric_df.isnull().sum().sort_values(ascending=False))\n            # DEBUG_END\n            \n            # Determine imputation strategy\n            column_groups = determine_imputation_strategy(numeric_df)\n            \n            # Define imputation strategies\n            imputers = {\n                'mean': SimpleImputer(strategy='mean'),\n                'median': SimpleImputer(strategy='median'),\n                'constant': SimpleImputer(strategy='constant',  fill_value=0) #, keep_empty_feature=True) # Add when sklearn >= 1.8\n            }\n\n            # Initialize scaler\n            scaler = StandardScaler()\n\n            # Fit imputers and scaler on full dataset first\n            for strategy, cols in column_groups.items():\n                if cols:  # Only if we have columns for this strategy\n                    imputers[strategy].fit(numeric_df[cols])\n            \n            # Process in chunks\n            results = []\n            for start_idx in range(0, len(numeric_df), chunk_size):\n                chunk = numeric_df.iloc[start_idx:start_idx + chunk_size].copy()\n                \n                # Impute\n                for strategy, cols in column_groups.items():\n                    if cols:\n                        chunk[cols] = imputers[strategy].transform(chunk[cols])\n                \n                results.append(chunk)\n                print(f\"Processed chunk {len(results)}: rows {start_idx} to {start_idx + len(chunk)}\")\n                \n                gc.collect()\n\n            # Combine all imputed results\n            imputed_df = pd.concat(results, ignore_index=True)\n            \n            # Save imputed data if requested\n            if save_imputed:\n                sf_manager = SnowflakeManager(config)\n                save_info = sf_manager.save_results(\n                    imputed_df, \n                    base_name='IMPUTED',\n                    mode='overwrite',\n                    timestamp=timestamp  # Pass timestamp here\n\n                )\n                print(f\"\\nSaved imputed data to: {save_info['full_name']}\")\n\n            # Now normalize the imputed data\n            scaler.fit(imputed_df)\n            \n            # Normalize in chunks\n            normalized_results = []\n            for start_idx in range(0, len(imputed_df), chunk_size):\n                chunk = imputed_df.iloc[start_idx:start_idx + chunk_size].copy()\n                chunk_normalized = pd.DataFrame(\n                    scaler.transform(chunk),\n                    columns=chunk.columns,\n                    index=chunk.index\n                )\n                normalized_results.append(chunk_normalized)\n                \n                gc.collect()\n\n            final_df = pd.concat(normalized_results, ignore_index=True)\n            \n            # VALIDATION_START - Final Validation\n            print(\"\\nFinal validation:\")\n            print(f\"Final shape: {final_df.shape}\")\n            final_nulls = final_df.isnull().sum().sum()\n            print(f\"Total null values remaining: {final_nulls}\")\n            # VALIDATION_END\n\n            return final_df, scaler, imputers, column_groups\n\n        except Exception as e:\n            print(f\"Error in imputation and normalization: {str(e)}\")\n            print(traceback.format_exc())\n            raise\n        finally:\n            monitor.end()\n\n    # Test the data preparation\n    if __name__ == \"__main__\":\n        config = EnvironmentConfig()\n        session = get_active_session()\n        config.set_session(session)\n        \n        # Test with small sample first\n        print(\"Testing data preparation...\")\n        df = load_and_prepare_data(config, sample_size=SAMPLE_SIZE)\n        print(f\"Initial shape: {df.shape}\")\n        \n        # Prepare numeric data (will add this function in next section)\n        numeric_df = df.select_dtypes(include=['int64', 'float64'])\n        print(f\"Numeric shape: {numeric_df.shape}\")\n        \n        # Test imputation and normalization\n        normalized_df, scaler, imputers, strategies = impute_and_normalize(\n            numeric_df, \n            config, \n            chunk_size=CHUNK_SIZE,\n            save_imputed=False\n        )\n        print(f\"Normalized shape: {normalized_df.shape}\")\n        \n        # Check normalization results\n        print(\"\\nNormalization check (mean should be ~0, std should be ~1):\")\n        print(normalized_df.describe())\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('data_prep_normalization')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37658dce-2c48-46cf-b6cb-9e1c1d402710",
   "metadata": {
    "language": "python",
    "name": "feature_engineering"
   },
   "outputs": [],
   "source": "# Cell: feature_engineering\nmonitor.start('feature_engineering')\ntry:\n    def prepare_numeric_data(df: pd.DataFrame,\n                             target_cols: List[str] = ['COMMIT_MAJOR', 'INFLATION_MAJOR_COMMIT']\n                             ) -> Tuple[pd.DataFrame, Dict[str, pd.Series]]:\n        \"\"\"\n        Prepare numeric features and targets, including encoding categorical features\n        \"\"\"\n        monitor.start('prepare_numeric_data')\n        try:\n            # Save targets with binary conversion\n            targets = {}\n            for col in target_cols:\n                if col in df.columns:\n                    # Convert 'MAJOR'/'NOT' to 1/0\n                    targets[col] = (df[col] == 'MAJOR').astype(int)\n                    # DEBUG_START - Target Distribution\n                    print(f\"\\nTarget {col} distribution:\")\n                    print(targets[col].value_counts(normalize=True))\n                    # DEBUG_END\n            # First identify numeric and categorical columns\n            numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n            categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n            print(f\"\\nFound {len(numeric_columns)} numeric columns and {len(categorical_columns)} categorical columns\")\n            # Create a DataFrame with numeric columns\n            numeric_df = df[numeric_columns].copy()\n            # Handle categorical columns with one-hot encoding\n            if len(categorical_columns) > 0:\n                print(f\"Encoding {len(categorical_columns)} categorical columns\")\n                # For each categorical column, encode it and add to numeric_df\n                for col in categorical_columns:\n                    # Skip target columns\n                    if col in target_cols:\n                        continue\n                    # Skip columns with too many unique values (threshold can be adjusted)\n                    unique_count = df[col].nunique()\n                    if unique_count > 20:  # Skip if more than 20 unique values\n                        print(f\"Skipping column {col} with {unique_count} unique values (too many for one-hot encoding)\")\n                        continue\n                    # Clean column names by removing spaces and apostrophes\n#                    clean_col = col.replace(' ', '').replace(\"'\", \"\")\n                    # Clean the values in the column as well\n                    df[col] = df[col].astype(str).apply(lambda x: x.replace(' ', '').replace(\"'\", \"\")).replace(\"-\", \"\")\n                    # Apply one-hot encoding\n                    try:\n                        # Get dummies without dropping first category to preserve all information\n                        dummies = pd.get_dummies(df[col], prefix=col, dummy_na=False, drop_first=False)\n                        dummies.columns = [col_name.upper().replace(' ', '_').replace('-', '_').replace('.', '_') \n                                            for col_name in dummies.columns]\n                        # Remove columns with all zeros (useless predictors)\n                        dummies = dummies.loc[:, dummies.sum() > 0]\n                        print(f\"  - Encoded {col}: added {dummies.shape[1]} dummy columns\")\n                        # Add to numeric DataFrame\n                        numeric_df = pd.concat([numeric_df, dummies], axis=1)\n                    except Exception as e:\n                        print(f\"  - Error encoding column {col}: {str(e)}\")\n            # Remove target columns if they exist in numeric_df\n            numeric_df = numeric_df.drop(columns=target_cols, errors='ignore')\n            # Remove specific columns if they exist\n            columns_to_remove = ['TOTAL_COMMIT_VALUE', 'INFLATION_TOT_COMMIT']\n            existing_columns = [col for col in columns_to_remove if col in numeric_df.columns]\n            if existing_columns:\n                print(\"\\nRemoving columns:\")\n                for col in existing_columns:\n                    print(f\"- {col}\")\n                numeric_df = numeric_df.drop(columns=existing_columns)\n            # Handle Y/N columns already converted to 1/0 (these should already be numeric)\n            bool_cols = df.select_dtypes(include=['bool']).columns\n            for col in bool_cols:\n                if col not in numeric_df.columns:  # Only if not already included\n                    numeric_df[col] = df[col].astype(float)\n            print(f\"Final shape after preparation: {numeric_df.shape}\")\n            print(f\"Sample feature names: {list(numeric_df.columns[:5])}\")\n            return numeric_df, targets\n        finally:\n            monitor.end()\n            \n    class FeatureImportanceAnalyzer:\n        def __init__(self, n_jobs: int = -1):\n            self.feature_importances = {}\n            self.feature_names = None  # Add this to store feature names\n            self.n_jobs = n_jobs\n            self.importance_estimator = RandomForestClassifier(\n                n_estimators=100,\n                random_state=42,\n                n_jobs=n_jobs,\n                class_weight='balanced'\n            )\n        def calculate_importance(self, X: pd.DataFrame, y: pd.Series, target_name: str) -> Dict[str, float]:\n            \"\"\"Calculate feature importance for a target\"\"\"\n            monitor.start(f'feature_importance_{target_name}')\n            try:\n                # Store feature names and their order\n                self.feature_names = X.columns.tolist()\n                # Fit random forest for importance calculation\n                self.importance_estimator.fit(X, y)\n                # Get feature importance\n                importance = self.importance_estimator.feature_importances_\n                importance_dict = dict(zip(self.feature_names, importance))\n                # Sort by importance\n                importance_dict = {\n                    k: v for k, v in sorted(\n                        importance_dict.items(),\n                        key=lambda item: item[1],\n                        reverse=True\n                    )\n                }\n                # Store for this target\n                self.feature_importances[target_name] = importance_dict\n                return importance_dict\n            finally:\n                monitor.end()\n                \n        def select_features(self, df: pd.DataFrame, importance_dict: Dict[str, float],\n                            sample_ratio: float, k: Optional[int] = None) -> pd.DataFrame:\n            \"\"\"\n            Select top features based on importance and ratio\n            Args:\n                df: Input DataFrame\n                importance_dict: Dictionary of feature importances\n                sample_ratio: Ratio of features to select\n                k: Optional specific number of features to select\n            Returns:\n                DataFrame with selected features\n            \"\"\"\n            # Sort features by importance\n            sorted_features = sorted(importance_dict.items(),\n                                     key=lambda x: x[1],\n                                     reverse=True)\n            # Calculate number of features if k not provided\n            if k is None:\n                k = max(1, min(\n                    int(len(sorted_features) * sample_ratio),\n                    len(sorted_features)\n                ))\n            else:\n                k = min(k, len(sorted_features))  # Don't select more than available\n            # Select top k features\n            selected_features = [f[0] for f in sorted_features[:k]]\n            print(f\"Selected {len(selected_features)} features out of {len(sorted_features)}\")\n            # Verify features exist in DataFrame\n            missing_features = [f for f in selected_features if f not in df.columns]\n            if missing_features:\n                print(\"\\nWARNING: Some selected features not in DataFrame:\")\n                print(missing_features)\n            # Return copy of DataFrame with selected features\n            result_df = df[selected_features].copy()\n            return result_df\n            \n        def plot_importance(self, target_name: str, top_n: int = 20):\n            \"\"\"Plot feature importance\"\"\"\n            monitor.start(f'plot_importance_{target_name}')\n            try:\n                if target_name not in self.feature_importances:\n                    raise ValueError(f\"No importance calculated for {target_name}\")\n                importance_dict = self.feature_importances[target_name]\n                # Get top N features\n                features = list(importance_dict.keys())[:top_n]\n                importance = list(importance_dict.values())[:top_n]\n                # Create plot\n                plt.figure(figsize=(12, 8))\n                sns.barplot(x=importance, y=features)\n                plt.title(f'Top {top_n} Feature Importance for {target_name}')\n                plt.xlabel('Importance')\n                plt.ylabel('Feature')\n                plt.tight_layout()\n                return plt.gcf()\n            finally:\n                monitor.end()\n                \n    # Test feature engineering\n    if __name__ == \"__main__\":\n        config = EnvironmentConfig()\n        session = get_active_session()\n        config.set_session(session)\n        # Load sample data\n        print(\"Testing feature engineering...\")\n        df = load_and_prepare_data(config, sample_size=SAMPLE_SIZE)\n        # Prepare numeric data\n        numeric_df, targets = prepare_numeric_data(df)\n        normalized_df, scaler, imputers, strategies = impute_and_normalize(\n            numeric_df,\n            config,\n            save_imputed=False\n        )\n        # Test feature importance and selection\n        analyzer = FeatureImportanceAnalyzer()\n        # Test for each target and feature sample size\n        for target_name, target_values in targets.items():\n            print(f\"\\n{'='*80}\")\n            print(f\"Testing features for {target_name}\")\n            print(f\"{'='*80}\")\n            # Calculate importance\n            importance_dict = analyzer.calculate_importance(\n                normalized_df,\n                target_values,\n                target_name\n            )\n            # Test different feature sample ratios\n            for sample_name, ratio in FEATURE_SAMPLES.items():\n                print(f\"\\n{'-'*40}\")\n                print(f\"Testing {sample_name} feature sample\")\n                print(f\"{'-'*40}\")\n                # Calculate features for this ratio\n                total_features = normalized_df.shape[1]\n                n_features = max(1, min(\n                    int(total_features * ratio),\n                    total_features\n                ))\n                # Select features\n                sampled_df = analyzer.select_features(\n                    normalized_df,\n                    importance_dict,\n                    ratio,\n                    k=n_features\n                )\n                # Verify results\n                print(f\"\\nVerification:\")\n                print(f\"Expected {n_features} features\")\n                print(f\"Got {len(sampled_df.columns)} features\")\n                print(f\"Feature ratio: {len(sampled_df.columns)/total_features:.1%}\")\n            # Plot importance\n            analyzer.plot_importance(target_name)\n            plt.show()\nfinally:\n    monitor.end()\ndep_checker.register_cell('feature_engineering')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c22d4407-c80d-444a-aeb8-9a9456e0570d",
   "metadata": {
    "language": "python",
    "name": "model_training"
   },
   "outputs": [],
   "source": "class ModelTrainer:\n    def __init__(self, n_splits: int = N_SPLITS, chunk_size: int = CHUNK_SIZE):\n        self.n_splits = n_splits\n        self.chunk_size = chunk_size\n        self.models = [\n            ## consider C =1 Less regularization , max_iter=5000 ,or  solver='lbfgs'  # lbfgs often converges faster than saga must # Removed n_jobs=-1\n            (MODEL_NAMES[0], LogisticRegression(\n                random_state=42, solver='lbfgs', max_iter=5000,\n                penalty='l2', C=0.1,  class_weight='balanced')) #n_jobs=-1,\n            ,(MODEL_NAMES[1], RandomForestClassifier(\n                random_state=42, n_jobs=-1, class_weight='balanced',\n                n_estimators=100, max_depth=None))\n##             ,(MODEL_NAMES[2], SVC(\n##                 random_state=42, probability=True, class_weight='balanced',\n##                 kernel='rbf'))\n            ,(MODEL_NAMES[3], DecisionTreeClassifier(\n                random_state=42, class_weight='balanced',\n                max_depth=None))\n\n# Note: sklearn's MLPClassifier doesn't have dropout\n# Consider switching to MLPRegressor or using TensorFlow/PyTorch\n            \n##            ,(MODEL_NAMES[4], MLPClassifier(\n##                random_state=42,\n##                hidden_layer_sizes=(50,),  # Reduce complexity - smaller network OR hidden_layer_sizes=(50, 25) - Tapering layers\n##                max_iter=3000,  # consider reducing iterations\n##                early_stopping=True,\n##                validation_fraction=0.2,  # Increase validation set for better early stopping 0.25,  # Larger validation set\n##                n_iter_no_change=50,  # Stop earlier when no improvement 15 Stop sooner higher more patience\n##                learning_rate_init=0.01 ,  # Slightly higher learning rate changed from  0.01\n##                learning_rate='adaptive' ,   # Key addition!\n##                alpha=0.01,  # STRONG regularization (key for overfitting) # Try values: 0.001, 0.01, 0.1, 1.0\n##                solver='adam',\n#                beta_1=0.9,\n#                beta_2=0.999,\n##                tol=1e-3,  # Less strict tolerance\n#                verbose=True  # Monitor training progress\n##                    ))\n        ]\n        self.cv = StratifiedKFold(\n            n_splits=n_splits, shuffle=True, random_state=42\n        )\n\n    def train_and_evaluate(self, X: pd.DataFrame, y: pd.Series,\n                           feature_sample: str = 'full') -> dict:\n        \"\"\"Train and evaluate models with cross-validation\"\"\"\n        monitor.start('train_and_evaluate')\n        try:\n            # Feature selection: select top 20 features by f_classif for example\n#            X_selected, selected_features = self.select_features(X, y, k = min(20, X.shape[1])) # k=20) #capped 20 features\n            X_selected = X\n            selected_features = X.columns.tolist()\n            X_selected = pd.DataFrame(X_selected, columns=selected_features, index=X.index)\n\n            results = {name: {\n                'CV_SCORES': [],\n                'FEATURE_IMPORTANCE': None,\n                'MODEL': None,\n                'AVG_METRICS': None,\n                'BEST_THRESHOLD': None,\n                'FEATURE_SAMPLE': feature_sample,\n                'FEATURES': selected_features\n            } for name, _ in self.models}\n\n            # Cross-validation splits\n            for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_selected, y)):\n                X_train = X_selected.iloc[train_idx][selected_features].copy()\n                X_val = X_selected.iloc[val_idx][selected_features].copy()\n                y_train = y.iloc[train_idx]\n                y_val = y.iloc[val_idx]\n\n                for name, model in self.models:\n                    try:\n                        if name == 'MLPClassifier' and feature_sample != 'third':\n                            print(f\"Skipping MLPClassifier for {feature_sample}\")\n                            continue\n                        if name == 'MLPClassifier':# and getattr(model, 'early_stopping', False):\n                            # Use fit instead of partial_fit for early stopping support\n                            model.fit(X_train, y_train)\n                        elif hasattr(model, 'partial_fit'):\n                            # For models supporting partial_fit and no early stopping, train in chunks\n                            classes = np.unique(y)\n                            for start_idx in range(0, len(X_train), self.chunk_size):\n                                end_idx = min(start_idx + self.chunk_size, len(X_train))\n                                X_chunk = X_train.iloc[start_idx:end_idx]\n                                y_chunk = y_train.iloc[start_idx:end_idx]\n                                model.partial_fit(X_chunk, y_chunk, classes=classes)\n                        else:\n                            model.fit(X_train, y_train)\n\n                        y_pred = model.predict(X_val)\n                        y_prob = model.predict_proba(X_val)[:, 1]\n\n                        metrics = self.calculate_metrics(y_val, y_pred, y_prob)\n                        results[name]['CV_SCORES'].append(metrics)\n\n                        # Find optimal threshold for binary classification based on F1 score\n                        thresholds = np.linspace(0, 1, 100)\n                        f1_scores = [f1_score(y_val, y_prob >= t) for t in thresholds]\n                        best_idx = np.argmax(f1_scores)\n                        results[name]['BEST_THRESHOLD'] = thresholds[best_idx]\n\n                        # Store feature importances for tree models\n                        if hasattr(model, 'feature_importances_'):\n                            if results[name]['FEATURE_IMPORTANCE'] is None:\n                                results[name]['FEATURE_IMPORTANCE'] = {}\n                            importance_dict = dict(zip(selected_features,\n                                                       model.feature_importances_))\n                            results[name]['FEATURE_IMPORTANCE'] = importance_dict\n\n                        results[name]['MODEL'] = model\n\n                    except Exception as e:\n                        print(f\"Error training {name} in fold {fold + 1}: {e}\")\n                        results[name]['CV_SCORES'].append({'ERROR': str(e)})\n\n            # Aggregate average metrics across folds\n            for name in results:\n                cv_scores = results[name]['CV_SCORES']\n                if cv_scores and all('ERROR' not in score for score in cv_scores):\n                    avg_metrics = {}\n                    for metric in cv_scores[0].keys():\n                        vals = [score[metric] for score in cv_scores]\n                        avg_metrics[metric] = {\n                            'MEAN': np.mean(vals),\n                            'STD': np.std(vals),\n                            'CI_LOW': np.percentile(vals, 2.5),\n                            'CI_HIGH': np.percentile(vals, 97.5)\n                        }\n                    results[name]['AVG_METRICS'] = avg_metrics\n\n            return results\n        finally:\n            monitor.end()\n\n    def predict_with_aligned_features(self, model, X: pd.DataFrame, selected_features: list) -> np.ndarray:\n        \"\"\"\n        Predict using the model ensuring X columns are aligned with selected_features.\n\n        Parameters:\n            model: Trained model\n            X: Input DataFrame for prediction\n            selected_features: List of features used to train the model\n\n        Returns:\n            Predicted class labels as np.ndarray\n        \"\"\"\n        missing_features = [f for f in selected_features if f not in X.columns]\n        if missing_features:\n            raise ValueError(f\"Missing features for prediction: {missing_features}\")\n\n        X_aligned = X.loc[:, selected_features]\n        return model.predict(X_aligned)\n\n    def predict_proba_with_aligned_features(self, model, X: pd.DataFrame, selected_features: list) -> np.ndarray:\n        \"\"\"\n        Predict class probabilities with feature alignment.\n\n        Parameters:\n            model: Trained model\n            X: Input DataFrame for prediction\n            selected_features: List of features used to train the model\n\n        Returns:\n            Predicted class probabilities as np.ndarray\n        \"\"\"\n        missing_features = [f for f in selected_features if f not in X.columns]\n        if missing_features:\n            raise ValueError(f\"Missing features for prediction: {missing_features}\")\n\n        X_aligned = X.loc[:, selected_features]\n        return model.predict_proba(X_aligned)\n\n    def select_features(self, X: pd.DataFrame, y: pd.Series, k: int = 20) -> tuple:\n        \"\"\"\n        Select top k features based on ANOVA F-value (f_classif).\n\n        Returns:\n            Tuple containing:\n                - DataFrame of selected features,\n                - List of selected feature names\n        \"\"\"\n        selector = SelectKBest(score_func=f_classif, k=k)\n        X_selected = selector.fit_transform(X, y)\n        selected_features = X.columns[selector.get_support()].tolist()\n        X_selected_df = pd.DataFrame(X_selected, columns=selected_features, index=X.index)\n        return X_selected_df, selected_features\n\n    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray,\n                          y_prob: np.ndarray) -> dict:\n        \"\"\"\n        Calculate common classification metrics.\n\n        Returns:\n            Dictionary containing accuracy, balanced accuracy, precision,\n            recall, F1, ROC AUC, Matthews correlation coefficient, and PR AUC.\n        \"\"\"\n        metrics = {}\n        metrics['ACCURACY'] = accuracy_score(y_true, y_pred)\n        metrics['BALANCED_ACCURACY'] = balanced_accuracy_score(y_true, y_pred)\n        metrics['PRECISION'] = precision_score(y_true, y_pred, zero_division=0)\n        metrics['RECALL'] = recall_score(y_true, y_pred, zero_division=0)\n        metrics['F1'] = f1_score(y_true, y_pred, zero_division=0)\n        metrics['ROC_AUC'] = roc_auc_score(y_true, y_prob)\n        metrics['MCC'] = matthews_corrcoef(y_true, y_pred)\n\n        precision, recall, _ = precision_recall_curve(y_true, y_prob)\n        metrics['PR_AUC'] = auc(recall, precision)\n\n        # Optionally, add class distribution\n        metrics['POS_CLASS_RATIO'] = np.mean(y_true == 1)\n\n        return metrics\n\n    def plot_cv_results(self, results: dict, show_bar_plots: bool = False,\n                        show_box_plots: bool = True, target_name: str = None):\n        \"\"\"Plot cross-validation results comparing models across feature sets\"\"\"\n        monitor.start('plot_cv_results')\n        try:\n            if not results:\n                print(\"No results to plot\")\n                return None\n            # Define metrics to plot\n            metrics = ['ACCURACY', 'BALANCED_ACCURACY', 'PRECISION', 'RECALL',\n                       'F1', 'ROC_AUC', 'PR_AUC', 'MCC']\n    \n            if show_box_plots:\n                # Prepare data for box plots\n                plot_data = []\n                model_names = list(results.keys())\n                print(f\"\\nProcessing plots for target: {target_name}\")\n                print(f\"Number of models: {len(model_names)}\")\n    \n                # Debug the results structure\n                print(\"\\nResults structure:\")\n                for model_name, model_results in results.items():\n                    print(f\"\\nModel: {model_name}\")\n                    print(f\"Type: {type(model_results)}\")\n                    if isinstance(model_results, dict):\n                        print(f\"Keys: {model_results.keys()}\")\n    \n                # Process results for plotting\n                for model_name, model_results in results.items():\n                    print(f\"\\nProcessing model: {model_name}\")\n                    if isinstance(model_results, dict) and 'CV_SCORES' in model_results:\n                        feature_set = model_results.get('FEATURE_SAMPLE', 'unknown')\n                        print(f\"Processing feature set: {feature_set}\")\n                        for score_dict in model_results['CV_SCORES']:\n                            if not isinstance(score_dict, dict) or 'ERROR' in score_dict:\n                                continue\n                            row = {\n                                'Model': model_name,\n                                'Feature Set': feature_set,\n                                **{k: v for k, v in score_dict.items() if k in metrics}\n                            }\n                            plot_data.append(row)\n                    elif isinstance(model_results, list):\n                        for result in model_results:\n                            if isinstance(result, dict) and 'CV_SCORES' in result:\n                                feature_set = result.get('FEATURE_SAMPLE', 'unknown')\n                                print(f\"Processing feature set: {feature_set}\")\n                                for score_dict in result['CV_SCORES']:\n                                    if not isinstance(score_dict, dict) or 'ERROR' in score_dict:\n                                        continue\n                                    row = {\n                                        'Model': model_name,\n                                        'Feature Set': feature_set,\n                                        **{k: v for k, v in score_dict.items() if k in metrics}\n                                    }\n                                    plot_data.append(row)\n                if plot_data:\n                    df = pd.DataFrame(plot_data)\n    \n                    # Debug the plot data\n                    print(\"\\nUnique Feature Sets:\", df['Feature Set'].unique())\n                    print(\"Unique Models:\", df['Model'].unique())\n    \n                    # Ensure correct feature set order\n                    feature_set_order = ['third', 'half', 'full']\n                    df['Feature Set'] = pd.Categorical(df['Feature Set'],\n                                                      categories=feature_set_order,\n                                                      ordered=True)\n    \n                    # Create color palette based on actual models\n                    palette = dict(zip(model_names,\n                                       ['#1f77b4', '#ff7f0e', '#2ca02c',\n                                        '#d62728', '#9467bd'][:len(model_names)]))\n    \n                    # Set figure size based on number of metrics\n                    fig = plt.figure(figsize=(15, 5*len(metrics)))\n    \n                    for i, metric in enumerate(metrics, 1):\n                        if metric in df.columns:\n                            ax = plt.subplot(len(metrics), 1, i)\n                            sns.boxplot(data=df,\n                                        x='Feature Set',\n                                        y=metric,\n                                        hue='Model',\n                                        ax=ax,\n                                        palette=palette)\n                            title = f'Model Performance: {metric}'\n                            if target_name:\n                                title += f' ({target_name})'\n                            ax.set_title(title)\n                            ax.set_xlabel('Feature Set Size')\n                            ax.set_ylabel(metric)\n                            ax.grid(True, linestyle='--', alpha=0.7)\n                            ax.legend(title='Model', loc='upper right')\n    \n                    plt.tight_layout()\n                    return fig\n            return None\n        except Exception as e:\n            print(f\"Error plotting CV results: {str(e)}\")\n            traceback.print_exc()\n            return None\n        finally:\n            monitor.end()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc4619cf-3572-4164-bbd8-a671771fe650",
   "metadata": {
    "language": "python",
    "name": "storage_utils"
   },
   "outputs": [],
   "source": "# Cell: storage_utils\nmonitor.start('storage_utils')\ntry:\n    class ModelStorageManager:\n        def __init__(self, config: EnvironmentConfig):\n            self.config = config\n            self.sf_manager = SnowflakeManager(config)\n\n        #  replace the serialize_object method:\n       \n        def serialize_object(self, obj: Any) -> str:\n            \"\"\"Serialize object using joblib with compression\"\"\"\n            buffer = io.BytesIO()\n            joblib.dump(obj, buffer, compress=3)  # Compression level 3 (good balance)\n            return base64.b64encode(buffer.getvalue()).decode('utf-8')\n        \n        def deserialize_object(self, serialized_str: str) -> Any:\n            \"\"\"Deserialize object using joblib\"\"\"\n            buffer = io.BytesIO(base64.b64decode(serialized_str))\n\n\n        \n        def serialize_objectOLD(self, obj: Any) -> str:\n            \"\"\"Serialize object to base64 string\"\"\"\n            return base64.b64encode(pickle.dumps(obj)).decode('utf-8')\n\n\n        def save_model(self, model: Any, model_name: str, \n                      target: str, feature_sample: str,\n                      selected_features: List[str],\n                      metrics: Dict[str, Any],\n                      scaler: Any = None,\n                      imputers: Dict[str, Any] = None) -> None:\n            \"\"\"Save trained model with metadata\"\"\"\n            monitor.start('save_model')\n            try:\n                # Ensure features are stored in correct order\n                model_data = pd.DataFrame([{\n                    'MODEL': model_name,\n                    'TARGET': target,\n                    'FEATURE_SAMPLE': feature_sample,\n                    'N_FEATURES': len(selected_features),\n                    'MODEL_OBJECT': self.serialize_object(model),\n                    'SELECTED_FEATURES': self.serialize_object(selected_features),  # Exact feature list\n                    'METRICS': self.serialize_object(metrics),\n                    'SCALER': self.serialize_object(scaler),\n                    'IMPUTERS': self.serialize_object(imputers),\n                    'CREATED_AT': datetime.now()\n                }])\n                \n                self.sf_manager.save_results(\n                    model_data, \n                    base_name='models',\n                    mode='append'\n                )\n                \n            finally:\n                monitor.end()\n                \n        def save_imputation_config(self, \n                                 imputers: Dict[str, SimpleImputer],\n                                 strategies: Dict[str, List[str]],\n                                 scaler: StandardScaler,\n                                 feature_stats: Dict[str, Dict[str, float]]) -> None:\n            \"\"\"Save imputation configuration and statistics\"\"\"\n            monitor.start('save_imputation_config')\n            try:\n                imputation_data = pd.DataFrame([{\n                    'IMPUTERS': self.serialize_object(imputers),\n                    'STRATEGIES': self.serialize_object(strategies),\n                    'SCALER': self.serialize_object(scaler),\n                    'FEATURE_STATS': self.serialize_object(feature_stats),\n                    'CREATED_AT': datetime.now()\n                }])\n                \n                self.sf_manager.save_results(\n                    imputation_data,\n                    base_name='IMPUTED',\n                    mode='overwrite'\n                )\n                \n            finally:\n                monitor.end()\n\n        def load_latest_model(self, model_name: str, target: str, \n                            feature_sample: str) -> Tuple[Any, List[str], Dict[str, Any]]:\n            \"\"\"Load latest model and its components\"\"\"\n            monitor.start('load_latest_model')\n            try:\n                query = f\"\"\"\n                SELECT MODEL_OBJECT, SELECTED_FEATURES, METRICS\n                FROM {TABLE_NAMES['models']}\n                WHERE MODEL = '{model_name}'\n                AND TARGET = '{target}'\n                AND FEATURE_SAMPLE = '{feature_sample}'\n                ORDER BY CREATED_AT DESC\n                LIMIT 1\n                \"\"\"\n                \n                result = self.sf_manager.execute_with_retry(\n                    'load_model',\n                    lambda: self.sf_manager.session.sql(query).collect()\n                )\n                \n                if not result:\n                    return None, None, None\n                    \n                row = result[0]\n                model = pickle.loads(base64.b64decode(row['MODEL_OBJECT']))\n                features = pickle.loads(base64.b64decode(row['SELECTED_FEATURES']))\n                metrics = pickle.loads(base64.b64decode(row['METRICS']))\n                \n                return model, features, metrics\n                \n            finally:\n                monitor.end()\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('storage_utils')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9b9852e-0d48-4263-abb7-87f27e6106cd",
   "metadata": {
    "language": "python",
    "name": "model_evaluator_class"
   },
   "outputs": [],
   "source": "# Cell: model_evaluator_class\nmonitor.start('model_evaluator_class')\ntry:\n    print(\"Available classes:\", [name for name in dir() if name.startswith('Model')])\n    \n    class ModelEvaluator:\n        def __init__(self, \n                    sample_size: int = SAMPLE_SIZE, \n                    n_splits: int = N_SPLITS,\n                    chunk_size: int = CHUNK_SIZE):\n            self.sample_size = sample_size\n            self.n_splits = n_splits\n            self.chunk_size = chunk_size\n            self.feature_analyzer = FeatureImportanceAnalyzer()\n            self.trainer = ModelTrainer(n_splits=n_splits, chunk_size=chunk_size)\n            self.results = {}\n\n        def run_full_evaluation(self, config: EnvironmentConfig) -> Dict[str, Any]:\n            \"\"\"Run complete evaluation pipeline\"\"\"\n            monitor.start('run_full_evaluation')\n            try:\n                # Generate timestamp once at the start\n                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                all_results = {}\n                storage_manager = ModelStorageManager(config)\n                \n                # Load and prepare initial data\n                print(\"Loading data...\")\n                df = load_and_prepare_data(config, self.sample_size)\n                numeric_df, targets = prepare_numeric_data(df)\n                \n                # Impute and normalize full dataset\n                print(\"\\nImputing and normalizing...\")\n                normalized_df, scaler, imputers, strategies = impute_and_normalize(\n                    numeric_df, \n                    config,\n                    save_imputed=True,\n                    timestamp=timestamp\n                )\n        \n                # Save imputation configuration\n                storage_manager.save_imputation_config(\n                    imputers=imputers,\n                    strategies=strategies,\n                    scaler=scaler,\n                    feature_stats=None\n                )\n                \n                print(\"\\nStarting feature sample processing...\")\n                all_results = {}  # Initialize results dictionary\n                \n                # Evaluate with different feature samples\n                for sample_name, sample_ratio in FEATURE_SAMPLES.items():\n                    monitor.add_context(sample_name)\n                    try:\n                        print(f\"\\nProcessing {sample_name} feature sample \"\n                              f\"({sample_ratio:.1%} of features)\")\n                        \n                        sample_results = {}\n                        \n                        for target_name, target_values in targets.items():\n                            print(f\"\\nEvaluating {target_name}\")\n                            \n                            # Calculate feature importance\n                            importance_dict = self.feature_analyzer.calculate_importance(\n                                normalized_df.copy(), \n                                target_values,\n                                target_name\n                            )\n\n                            # Calculate feature importance\n                            importance_dict = self.feature_analyzer.calculate_importance(\n                                normalized_df.copy(), \n                                target_values,\n                                target_name\n                            )                            \n \n                            # Save feature importance data\n                            feature_importance_data = []\n                            for feature, importance in importance_dict.items():\n                                    feature_importance_data.append({\n#                                        'MODEL' : model_name,\n                                        'FEATURE_NAME': feature,\n                                        'IMPORTANCE': importance,\n                                        'TARGET': target_name,\n                                        'SAMPLE_TYPE': sample_name,\n                                        'CREATED_AT': datetime.now()\n                                    })\n                            if feature_importance_data:  # Only save if we have data                            \n                             feature_df = pd.DataFrame(feature_importance_data)\n                             storage_manager.sf_manager.save_results(\n                                feature_df,\n                                base_name='features',  # This will use TABLE_NAMES['features']\n                                mode='append'\n                             )\n                             print(f\"Saved feature importance data for {target_name} - {sample_name}\")\n                            \n\n                            \n                            # Calculate number of features to select based on ratio\n                            total_features = normalized_df.shape[1]\n                            n_features = max(1, min(\n                                int(total_features * sample_ratio),  # Requested number\n                                total_features  # Total available\n                            ))\n#                            print(f\"Selecting {n_features} features from {total_features} total\")\n                            print(f\"Available features: {total_features}\")\n                            print(f\"Requested features based on ratio {sample_ratio}: {int(total_features * sample_ratio)}\")\n                            print(f\"Final number of features to select: {n_features}\")\n                             \n                            \n                            # Select features for this sample ratio\n                            sampled_df = self.feature_analyzer.select_features(\n                                normalized_df.copy(),\n                                importance_dict,\n                                sample_ratio,\n#                                k=min(20, n_features)  # Don't request more features than we want for this sample\n                                k=n_features  # Pass calculated number of features\n                            )\n                            print(f\"Selected {len(sampled_df.columns)} features for {sample_name}\")\n                            \n                            # Train and evaluate models\n                            model_results = self.trainer.train_and_evaluate(\n                                X=sampled_df,\n                                y=target_values,\n                                feature_sample=sample_name  # Make sure this is passed\n                            )\n\n                           # Save results to Snowflake for each model\n                            for model_name, model_info in model_results.items():\n                                if 'AVG_METRICS' in model_info and model_info['AVG_METRICS'] is not None:\n                                    metrics = model_info['AVG_METRICS']\n                                    result_row = pd.DataFrame([{\n                                        'TARGET': target_name,\n                                        'MODEL': model_name,\n                                        'FEATURE_SAMPLE': sample_name,\n                                        'N_FEATURES': len(sampled_df.columns),\n                                        'FEATURE_RATIO': sample_ratio,\n                                        'SAMPLE_SIZE': len(df),\n                                        'ACCURACY_MEAN': metrics['ACCURACY']['MEAN'],\n                                        'ACCURACY_STD': metrics['ACCURACY']['STD'],\n                                        'PRECISION_MEAN': metrics['PRECISION']['MEAN'],\n                                        'PRECISION_STD': metrics['PRECISION']['STD'],\n                                        'RECALL_MEAN': metrics['RECALL']['MEAN'],\n                                        'RECALL_STD': metrics['RECALL']['STD'],\n                                        'F1_MEAN': metrics['F1']['MEAN'],\n                                        'F1_STD': metrics['F1']['STD'],\n                                        'ROC_AUC_MEAN': metrics['ROC_AUC']['MEAN'],\n                                        'ROC_AUC_STD': metrics['ROC_AUC']['STD'],\n                                        'TIMESTAMP': datetime.now()\n                                    }])\n                                    \n                                    # Save to Snowflake\n                                    storage_manager.sf_manager.save_results(\n                                        result_row,\n                                        base_name='MODEL_RESULTS',  # Use MODEL_RESULTS instead of 'results'base_name='results', \n                                        mode='append'\n                                    )\n                                    print(f\"Saved results for {model_name} - {target_name} - {sample_name}\")\n                            \n                            # Store results in nested structure\n                            sample_results[target_name] = {\n                                'FEATURE_IMPORTANCE': importance_dict,\n                                'MODEL_PERFORMANCE': model_results,\n                                'SELECTED_FEATURES': sampled_df.columns.tolist(),\n                                'DATA_STATS': {\n                                    'SAMPLE_SIZE': len(df),\n                                    'N_FEATURES': len(sampled_df.columns),\n                                    'FEATURE_RATIO': sample_ratio,\n                                    'CLASS_DISTRIBUTION': target_values.value_counts(normalize=True).to_dict()\n                                }\n                            }\n                            \n                            # Save models with components\n                            for model_name, model_info in model_results.items():\n                                if ('ERROR' not in model_info and \n                                    model_info.get('MODEL') is not None):\n                                    storage_manager.save_model(\n                                        model=model_info['MODEL'],\n                                        model_name=model_name,\n                                        target=target_name,\n                                        feature_sample=sample_name,\n#                                        selected_features=sampled_df.columns.tolist(), \n                                        selected_features=model_info['FEATURES'],  # ✅ Actual features used by model!                                        \n                                        metrics=model_info['AVG_METRICS'],\n                                        scaler=scaler,\n                                        imputers=imputers\n                                    )\n                        \n                        all_results[sample_name] = sample_results\n                        print(f\"Added results for {sample_name}\")\n                        \n                    finally:\n                        monitor.remove_context()\n                \n                print(\"\\nVerifying results structure:\")\n                for sample_name in all_results:\n                    print(f\"- {sample_name}\")\n                    for target_name in all_results[sample_name]:\n                        print(f\"  - {target_name}: {len(all_results[sample_name][target_name]['MODEL_PERFORMANCE'])} models\")\n                \n                self.results = all_results\n                return all_results\n                \n            finally:\n                monitor.end()\n#        def generate_report(self, show_bar_plots: bool = False, show_box_plots: bool = True, \n        def generate_report(self, show_bar_plots: bool = False, show_box_plots: bool = True, \n                           save_plots: bool = False) -> None:\n            \"\"\"Generate comprehensive evaluation report\"\"\"\n            monitor.start('generate_report')\n            try:\n                if not self.results:\n                    print(\"No results to report. Please run full_evaluation first.\")\n                    return\n                \n                print(\"\\nResults structure at start of generate_report:\")\n                for sample_name, sample_results in self.results.items():\n                    print(f\"Found sample: {sample_name}\")\n                    \n                # For each target, combine results across all feature sets\n                for target_name in next(iter(self.results.values())):\n                    print(f\"\\nResults for {target_name}:\")\n                    print(\"-\"*80)\n                    \n                    # Combine model performance results across all feature sets\n                    combined_performance = {}\n                    \n                    # First, initialize combined_performance with empty lists for each model\n                    first_sample = next(iter(self.results.values()))\n                    first_target = first_sample[target_name]\n                    model_names = first_target['MODEL_PERFORMANCE'].keys()\n                    combined_performance = {model_name: [] for model_name in model_names}\n                    \n                    # Then add results from each feature set\n                    for sample_name, sample_results in self.results.items():\n                        print(f\"\\nProcessing sample: {sample_name}\")\n                        target_results = sample_results[target_name]\n                        model_results = target_results['MODEL_PERFORMANCE']\n                        \n                        for model_name, model_info in model_results.items():\n                            print(f\"  Processing model: {model_name}\")\n                            model_info['FEATURE_SAMPLE'] = sample_name  # Ensure feature set is tagged\n                            combined_performance[model_name].append(model_info)\n                    \n                    # Generate plots\n                    print(f\"\\nGenerating plots for {target_name}\")\n                    fig = self.trainer.plot_cv_results(\n                        combined_performance,\n                        show_bar_plots=show_bar_plots,\n                        show_box_plots=show_box_plots,\n                        target_name=target_name\n                    )\n                            \n                    if fig is not None:\n                        plt.figure(fig.number)\n                        plt.show()\n                        if save_plots:\n                            plot_filename = f\"cv_results_{target_name}.png\"\n                            fig.savefig(plot_filename, bbox_inches='tight')\n                            print(f\"Saved plot to {plot_filename}\")\n                        plt.close(fig)\n                    \n                    # Print detailed statistics for each feature set\n                    for sample_name, sample_results in self.results.items():\n                        print(f\"\\n{sample_name.upper()} FEATURE SET:\")\n                        target_results = sample_results[target_name]\n                        stats = target_results['DATA_STATS']\n                        print(f\"Sample Size: {stats['SAMPLE_SIZE']:,}\")\n                        print(f\"Number of Features: {stats['N_FEATURES']}\")\n                        print(f\"Feature Ratio: {stats['FEATURE_RATIO']:.1%}\")\n                \n                plt.close('all')\n                    \n            finally:\n             monitor.end()\n\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('model_evaluator_class')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3012677-f764-40b0-a479-78830d3556e0",
   "metadata": {
    "language": "python",
    "name": "model_evaluation_test"
   },
   "outputs": [],
   "source": "# Cell: model_evaluation_test\nmonitor.start('model_evaluation_test')\ntry:\n    # Test evaluation\n    if __name__ == \"__main__\":\n        try:\n            config = EnvironmentConfig()\n            session = get_active_session()\n            config.set_session(session)\n            \n            evaluator = ModelEvaluator(sample_size=SAMPLE_SIZE)\n            results = evaluator.run_full_evaluation(config)\n            # Debug: Check what feature sets we have\n            print(\"\\nFeature sets in results:\")\n            for sample_name in results.keys():\n                print(f\"- {sample_name}\")\n            evaluator.generate_report(show_bar_plots=False, show_box_plots=True)\n            \n        except Exception as e:\n            print(f\"Error in evaluation: {str(e)}\")\n            traceback.print_exc()\n\nfinally:\n    monitor.end()\ndep_checker.register_cell('model_evaluation_test')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ce35070-dd9b-4b04-b4ca-530a2886279f",
   "metadata": {
    "language": "python",
    "name": "monitor_output",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Cell: monitor_output\n\"\"\"\nDependencies:\n- notebook_monitor\n- final_evaluation_reporting\nProvides: Execution summary and performance metrics\n\"\"\"\ndef show_monitoring_results():\n    try:\n        if not dep_checker.check_dependencies('monitor_output'):\n            raise RuntimeError(\"Dependencies not met for monitor output\")\n\n        # Show execution summary\n        pd.set_option('display.max_rows', None)\n        pd.set_option('display.max_columns', None)\n        pd.set_option('display.width', None)\n        pd.set_option('display.max_colwidth', None)\n        summary_df = monitor.show_summary()\n        \n        def categorize_operation(cell_name):\n            \"\"\"Enhanced categorization that handles context-based names\"\"\"\n            # Extract base operation name and context\n            parts = cell_name.split('_')\n            \n            # Look for sample size contexts\n            size_contexts = {'third', 'half', 'full', 'test'}\n            context = next((p for p in parts if p.lower() in size_contexts), '')\n            \n            # Base categorization\n            if 'train_and_evaluate' in cell_name:\n                base_cat = 'Model Training'\n            elif 'impute' in cell_name:\n                base_cat = 'Data Preparation'\n            elif 'feature_importance' in cell_name:\n                base_cat = 'Feature Analysis'\n            elif 'plot' in cell_name:\n                base_cat = 'Visualization'\n            elif 'load' in cell_name:\n                base_cat = 'Data Loading'\n            else:\n                base_cat = 'Other'\n            \n            # Add context if it exists\n            if context:\n                return f\"{base_cat} ({context})\"\n            return base_cat\n\n        # Define categories and their icons\n        CATEGORY_INDICATORS = {\n            'Model Training': '📊',\n            'Data Preparation': '🔧',\n            'Feature Analysis': '📈',\n            'Visualization': '📉',\n            'Data Loading': '💾',\n            'Other': '📌'\n        }\n        \n        # Define time thresholds\n        TIME_THRESHOLDS = {\n            'critical': 180,  # 3 minutes - 🔴\n            'warning': 60,    # 1 minute - 🟡\n            'normal': 0       # Under 1 minute - 🟢\n        }\n\n        def get_time_indicator(duration):\n            \"\"\"Get time indicator symbol\"\"\"\n            duration = float(duration.strip('s'))\n            if duration >= TIME_THRESHOLDS['critical']:\n                return '🔴'\n            elif duration >= TIME_THRESHOLDS['warning']:\n                return '🟡'\n            return '🟢'\n\n        # Process data\n        performance_df = summary_df.sort_values('duration_seconds', ascending=False)\n        performance_df['category'] = performance_df['cell_name'].apply(categorize_operation)\n        \n        # Calculate percentage of total time\n        total_time = performance_df['duration_seconds'].sum()\n        performance_df['percentage_of_total'] = (\n            performance_df['duration_seconds'] / total_time * 100\n        ).round(2)\n        performance_df['percentage_of_total'] = performance_df['percentage_of_total'].apply(\n            lambda x: f\"{x:>5.1f}%\"\n        )\n        \n        # Format duration\n        performance_df['duration_seconds'] = performance_df['duration_seconds'].apply(\n            lambda x: f\"{x:>6.2f}s\"\n        )\n        \n        # Print legend\n        print(\"\\nLegend:\")\n        print(\"=\" * 80)\n        print(\"Categories:\", \" | \".join(f\"{icon} {cat}\" for cat, icon in CATEGORY_INDICATORS.items()))\n        print(\"Timing:\", \"🔴 >3m | 🟡 >1m | 🟢 <1m\")\n        \n        # Show category summaries\n        print(\"\\nExecution Summary by Category:\")\n        print(\"-\" * 80)\n        category_stats = performance_df.groupby('category').agg({\n            'cell_name': 'count',\n            'duration_seconds': lambda x: f\"{sum(float(i.strip('s')) for i in x):>6.2f}s\"\n        })\n        category_stats.index = [f\"{CATEGORY_INDICATORS.get(cat.split(' ')[0], '📌')} {cat}\" \n                              for cat in category_stats.index]\n        category_stats.columns = ['Operation Count', 'Total Time']\n        print(category_stats.sort_values('Total Time', ascending=False))\n        \n        # Show detailed performance\n        print(\"\\nDetailed Operation Performance:\")\n        print(\"-\" * 120)  # Wider to accommodate longer category names\n        \n        # Format DataFrame for display\n        formatted_df = performance_df.copy()\n        formatted_df['time_alert'] = formatted_df['duration_seconds'].apply(get_time_indicator)\n        formatted_df['category'] = formatted_df['category'].apply(\n            lambda x: f\"{CATEGORY_INDICATORS.get(x.split(' ')[0], '📌')} {x:<25}\"\n        )\n        formatted_df['cell_name'] = formatted_df['cell_name'].apply(lambda x: f\"{x:<40}\")\n        formatted_df['status'] = formatted_df['status'].apply(lambda x: f\"{x:<10}\")\n        \n        display_cols = ['time_alert', 'category', 'cell_name', 'duration_seconds', \n                       'percentage_of_total', 'status']\n        print(formatted_df[display_cols].to_string(index=False))\n        \n        # Performance alerts\n        print(\"\\nPerformance Alerts:\")\n        print(\"-\" * 80)\n        critical_ops = formatted_df[formatted_df['time_alert'] == '🔴']\n        warning_ops = formatted_df[formatted_df['time_alert'] == '🟡']\n        \n        if not critical_ops.empty:\n            print(\"\\n🔴 Critical Operations (>3m):\")\n            for _, row in critical_ops.iterrows():\n                print(f\"   • {row['category']}: {row['cell_name']} ({row['duration_seconds']})\")\n                \n        if not warning_ops.empty:\n            print(\"\\n🟡 Warning Operations (>1m):\")\n            for _, row in warning_ops.iterrows():\n                print(f\"   • {row['category']}: {row['cell_name']} ({row['duration_seconds']})\")\n\n        # Overall success rate\n        success_rate = (performance_df['status'] == 'completed').mean() * 100\n        print(f\"\\nOverall Success Rate: {success_rate:.1f}%\")\n        \n        # Total execution time\n        total_time_sec = sum(float(t.strip('s')) for t in performance_df['duration_seconds'])\n        print(f\"Total Execution Time: {total_time_sec:.2f}s ({total_time_sec/60:.2f}m)\")\n\n        return formatted_df\n        \n    except Exception as e:\n        print(f\"Error in monitoring output: {type(e).__name__} - {str(e)}\")\n        print(traceback.format_exc())\n        return pd.DataFrame()\n\n# Execute monitoring output\nmonitor_df = show_monitoring_results()\ndep_checker.register_cell('monitor_output')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd2df1d9-2f4a-428f-b28a-4b671d1bbee1",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": ""
  }
 ]
}
